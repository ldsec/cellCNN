{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellCnn [1] data generation\n",
    "\n",
    "source: https://github.com/eiriniar/CellCnn\n",
    "\n",
    "\"\"\" Copyright 2016-2017 ETH Zurich, Eirini Arvaniti and Manfred Claassen.\n",
    "\n",
    "This module contains data preprocessing/distribution functions.\n",
    "\n",
    "\"\"\"\n",
    "The code is slightly changed depending on original implementation to make it compatible with decentralized settings\n",
    "\n",
    "In this example, we preprocess and distribute the dataset to characterize healthy donor (HD) to  non-inflammatory neurological disease (NIND) or relapsing–remitting multiple sclerosis (RRMS) (set cg parameter) [2].\n",
    "\n",
    "The dataset comprises mass cytometry measurements of 35 markers, for PBMC samples of 100 donors with varying conditions for HD/RRMS/NIND. \n",
    "\n",
    "To run this example, \n",
    "\n",
    "    - download the public dataset at http://flowrepository.org/experiments/2166/,\n",
    "    - uncompress it and place \"discovery_cohort\" in the data/cellCNN/ folder\n",
    "\n",
    "Data distribution: We fix the test set and the training dataset is then generated by distributing different donors for each institution depending on number of institutions.\n",
    "\n",
    "[1] E. Arvaniti and M. Claassen. Sensitive detection of rare disease-associated cell subsets via representation learning.Nat Commun, 8:1–10, 2017\n",
    "\n",
    "[2]  E. Galli, F. J. Hartmann, B. Schreiner, F. Ingelfinger, E. Arvaniti, M. Diebold, D. Mrdjen, F. van der Meer, C. Krieg, F. A. Nimer, N. S. R. Sanderson, C. Stadelmann, M. Khademi, F. Piehl, M. Claassen, T. Derfuss, T. P. Olsson, and B. Becher. GM-CSF and CXCR4 define a t helper cell signature in multiple sclerosis. Nature medicine, 25:1290 – 1300, 201\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, errno, glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cellCNN_utils  \n",
    "from cellCNN_utils import loadFCS, ftrans, mkdir_p, get_items, generate_data, generate_normalized_data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "d = Path().resolve()\n",
    "sys.path.append(d)\n",
    "%pylab inline\n",
    "# define input and output directories\n",
    "FCS_DATA_PATH = os.path.join(d, 'FlowRepository')\n",
    "# select the relevant markers for further analysis\n",
    "data_fcs = loadFCS(glob.glob(FCS_DATA_PATH + '/discovery_cohort.fcs')[0], transform=None, auto_comp=False)\n",
    "print(data_fcs.channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------SET PARAMS HERE -----------------------#\n",
    "\n",
    "cg = 'RRMS' #Control group, set to NIND or RRMS depending on experimental setting\n",
    "ncells = 2000 #num cells per multi-cell input or multi-cell test data\n",
    "ncell_test = 5000 #num cells for phenotype prediction\n",
    "ntrain_all = 120 # number of multi-cell inputs for the train data (in total)\n",
    "ntest_all = 40 # number of multi-cell inputs for the test data\n",
    "nfilter = 8 #number of filters for the training\n",
    "per_sample = True\n",
    "per_sample_train = 5\n",
    "per_sample_test= 8\n",
    "#-----------------------------------------------------------------------#\n",
    "\n",
    "markers=['CCR2', 'CCR4', 'CCR6', 'CCR7', 'CXCR4', 'CXCR5', 'CD103', 'CD14', 'CD20', \n",
    "        'CD25', 'CD27', 'CD28', 'CD3', 'CD4', 'CD45RA', 'CD45RO', 'CD56', 'CD57', 'CD69', 'CD8', \n",
    "        'TCRgd', 'PD.1', 'GM.CSF', 'IFN.g', 'IL.10', 'IL.13', 'IL.17A', 'IL.2', 'IL.21', 'IL.22', 'IL.3',\n",
    "        'IL.4', 'IL.6', 'IL.9', 'TNF.a']\n",
    "len(markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gate_source is 35.index\n",
    "#gate_source is ind=1 in excell, label is ind=4\n",
    "#match gate_source from table NINDC=0, RRMS=1\n",
    "metadata= pd.read_excel(FCS_DATA_PATH+'/meta_data_discovery_cohort.xlsx')\n",
    "metadata=metadata.to_numpy()\n",
    "gate_source = metadata[:,1]\n",
    "labelsTemp = metadata[:,4]\n",
    "data = []\n",
    "sample_labels =[]\n",
    "for i in range(99):\n",
    "    cur_gs = gate_source[i]\n",
    "    cur_lab = labelsTemp[i]\n",
    "    patient_sample = []\n",
    "    if cur_lab == 'HD':\n",
    "        gs_ind = np.where(data_fcs.events[:,35]==cur_gs)\n",
    "        for j in gs_ind[0]:\n",
    "            patient_sample.append(data_fcs.events[j,0:35])\n",
    "        sample_labels.append(0)\n",
    "    elif cur_lab == cg:\n",
    "        gs_ind = np.where(data_fcs.events[:,35]==cur_gs)\n",
    "        for j in gs_ind[0]:\n",
    "            patient_sample.append(data_fcs.events[j,0:35])\n",
    "        sample_labels.append(1)\n",
    "    if len(patient_sample)>0:\n",
    "        data.append(np.asarray(patient_sample))\n",
    "sample_labels=np.asarray(sample_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we randomly split the samples in training/test sets.\n",
    "\n",
    "def train_test_split(train_idx1=[], train_idx2=[], test=True):\n",
    "    # set random seed for reproducible results and monte-carlo repetitions only for training!\n",
    "#     np.random.seed(4)\n",
    "    \n",
    "    # split the fcs files into training, validation and test set (note that secure-protocols do not use validation sets)\n",
    "    group1 = np.where(sample_labels == 0)[0]\n",
    "    group2 = np.where(sample_labels == 1)[0]\n",
    "    l1, l2 = len(group1), len(group2)\n",
    "    ntrain_per_class = 24\n",
    "    ntest_group1 = l1 - ntrain_per_class\n",
    "    ntest_group2 = l2 - ntrain_per_class\n",
    "\n",
    "    # get the sample indices\n",
    "    train_idx1 = list(np.random.choice(group1, size=ntrain_per_class, replace=False))\n",
    "    test_idx1 = [i for i in group1 if i not in train_idx1]\n",
    "    train_idx2 = list(np.random.choice(group2, size=ntrain_per_class, replace=False))\n",
    "    test_idx2 = [i for i in group2 if i not in train_idx2]\n",
    "\n",
    "    print(\"test indices\")\n",
    "    test_indices = [test_idx1,test_idx2]\n",
    "    print(test_indices) \n",
    "    print(\"train indices\")\n",
    "    print(train_idx1)\n",
    "    print(train_idx2)\n",
    "\n",
    "    train_indices = [train_idx1,train_idx2]\n",
    "\n",
    "    # load the training samples\n",
    "    group1_list, group2_list = [], []\n",
    "    for idx in train_idx1:\n",
    "        x = data[idx][:]\n",
    "        group1_list.append(x)\n",
    "\n",
    "    for idx in train_idx2:\n",
    "        x = data[idx][:]\n",
    "        group2_list.append(x)\n",
    "\n",
    "    # load the test samples\n",
    "    t_group1_list, t_group2_list = [], []\n",
    "    test_phenotypes = []\n",
    "    for idx in test_idx1:\n",
    "        x = data[idx][:]\n",
    "        t_group1_list.append(x)\n",
    "        test_phenotypes.append(0)\n",
    "\n",
    "    for idx in test_idx2:\n",
    "        x = data[idx][:]\n",
    "        t_group2_list.append(x)\n",
    "        test_phenotypes.append(1)\n",
    "\n",
    "    # finally prepare training data\n",
    "    cut = int(1 * len(group1_list))\n",
    "    train_samples = group1_list[:cut] + group2_list[:cut]\n",
    "    train_phenotypes = [0] * len(group1_list[:cut]) + [1] * len(group2_list[:cut])\n",
    "    valid_samples = group1_list[cut:] + group2_list[cut:]\n",
    "    valid_phenotypes = [0] * len(group1_list[cut:]) + [1] * len(group2_list[cut:])\n",
    "    test_samples = t_group1_list + t_group2_list\n",
    "    print(test_phenotypes)\n",
    "    return train_samples,train_phenotypes,test_samples,test_phenotypes, test_indices,train_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate original data (with transform)\n",
    "In the following, \n",
    "- We generate training data with $ncell$ cells and $nsubset$ samples per patient (if per_sample is set to True), and $ncell$ cells and $nsubset$ samples per phenotype otherwise\n",
    "- We generate the test data for $ncell$ cells per patient/phenotype depending on the setting\n",
    "\n",
    "Processed data is placed under Flow/ folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#debug, for changes introduced to cellCNN_utils\n",
    "# import importlib\n",
    "# importlib.reload(cellCNN_utils)\n",
    "\n",
    "train_samples, train_phenotypes, test_samples, test_phenotypes, test_indices, train_indices = train_test_split()\n",
    "# generate ntrain_all (ntrain_all/2 per phenotype) training samples for centralized test!\n",
    "\n",
    "scaler,x_tr,y_tr = generate_data(train_samples, train_phenotypes, 'Flow/', generate_valid_set=False, \n",
    "                                               ncell=ncells, nsubset=int(ntrain_all/2), scale=True, \n",
    "                                               per_sample=False, verbose=0, saveFile=True,subset_selection = 'random')\n",
    "\n",
    "\n",
    "scaler,x_test,y_test = generate_data(test_samples, test_phenotypes, 'Flow/', generate_valid_set=False, \n",
    "                                               ncell=ncells, nsubset=per_sample_test, scale=True, \n",
    "                                               per_sample=per_sample, verbose=0, saveFile=True,\n",
    "                                               subset_selection = 'random', generateAsTest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate another test set 'per-individual' in test indices using maximum number of cells to use for phenotype prediction, called X_test_all\n",
    "The below script generates and prints the max number of cells for phenotype prediction for the current example which then will be used as a parameter in the golang protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "#generate also the test set on full min-ncell per sample:\n",
    "def generate_for_pheno_prediction(new_samples,phenotypes,scaler):\n",
    "        ncell_per_sample = np.min([x.shape[0] for x in new_samples])\n",
    "        print(f\"Predictions based on multi-cell inputs containing {ncell_per_sample} cells.\")\n",
    "        nmark = len(new_samples[0][1])\n",
    "        # scale the new samples if we did that for the training samples\n",
    "        if scaler is not None:\n",
    "            new_samples = [scaler.transform(x) for x in new_samples]\n",
    "        new_samples = [shuffle(x)[:ncell_per_sample].reshape(1, ncell_per_sample, nmark)\n",
    "                           for x in new_samples]\n",
    "        data_test = np.vstack(new_samples)\n",
    "        mkdir_p('Flow/X_test_all/')\n",
    "        for i in range(len(data_test)):\n",
    "            np.savetxt('Flow/' + 'X_test_all/' + str(i) +'.txt', (transpose(data_test[i])))\n",
    "        np.savetxt('Flow/' + 'y_test_all.txt', phenotypes)\n",
    "        return data_test,phenotypes,ncell_per_sample\n",
    "    \n",
    "data_test,phenotypes,ncell_per_sample = generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "print(shape(data_test))\n",
    "print(phenotypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate  data split between $nhosts$ parties\n",
    "In the following, \n",
    "- We generate training data with $ncells$ cells per sample/patient and $nsubset$ samples per patient/phenotype, per party\n",
    "- Example below distributes the train indices per donor for 4 parties\n",
    "\n",
    "Processed data is placed under splitFlow/host_i for party-i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(cellCNN_utils)\n",
    "\n",
    "# Here we randomly split the samples for n hosts\n",
    "nhosts=3\n",
    "\n",
    "test_idx1 = test_indices[0]\n",
    "test_idx2 = test_indices[1]\n",
    "print(train_indices[0])\n",
    "train_idx1 = train_indices[0]\n",
    "train_idx2 = train_indices[1]\n",
    "\n",
    "\n",
    "print(\"Test set indices:\")\n",
    "print(test_idx1)\n",
    "print(test_idx2)\n",
    "print(\"Global train set indices:\")\n",
    "\n",
    "#to take the runs on 10 different distributions (for box plot)\n",
    "print(train_idx1)\n",
    "print(train_idx2)\n",
    "\n",
    "#distribute train indices balanced among n hosts:\n",
    "split_idx_1 = []\n",
    "split_idx_2 = []\n",
    "group1_list = np.flip(np.array_split(numpy.array(train_idx1), nhosts))\n",
    "group2_list = numpy.array_split(numpy.array(train_idx2), nhosts)\n",
    "\n",
    "for i in range(nhosts):\n",
    "    split_idx_1.append(group1_list[i].tolist())\n",
    "    split_idx_2.append(group2_list[i].tolist())\n",
    "\n",
    "print(\"Global train splitted among hosts - indices:\")\n",
    "print(split_idx_1)\n",
    "print(split_idx_2)\n",
    "\n",
    "xtr = []\n",
    "ytr=[]\n",
    "for i in range(nhosts):\n",
    "    print(\"\\nHost no.\", i, \":\")\n",
    "    folder_path = 'splitFlow' + str(nhosts) + '/host' + str(i) + '/'\n",
    "    host_idx_1 = split_idx_1[i]\n",
    "    host_idx_2 = split_idx_2[i]\n",
    "    print(\"host_idx_1:\", host_idx_1, \"- host_idx_2:\", host_idx_2)\n",
    "     # load the training samples\n",
    "    host_group1_list, host_group2_list = [], []\n",
    "    train_samples,train_phenotypes = [],[]\n",
    "    for idx in host_idx_1:\n",
    "        x = data[idx][:]\n",
    "        host_group1_list.append(x)\n",
    "\n",
    "    for idx in host_idx_2:\n",
    "        x = data[idx][:]\n",
    "        host_group2_list.append(x)\n",
    "\n",
    "    # finally prepare training and vallidation data\n",
    "    cut = int(1 * len(host_group1_list))\n",
    "    train_samples = host_group1_list[:cut] + host_group2_list[:cut]\n",
    "    train_phenotypes = [0] * len(host_group1_list[:cut]) + [1] * len(host_group2_list[:cut])\n",
    "    print(train_phenotypes)\n",
    "    scaler,x_tr,y_tr = generate_data(train_samples, train_phenotypes, folder_path, scale=True, ncell=ncells, \n",
    "                  nsubset=per_sample_train,per_sample=per_sample, verbose=0,generate_valid_set=False,\n",
    "                                     saveFile=True,scaler=scaler,subset_selection = 'random',oneFile=None)\n",
    "#     xtr.append(x_tr)\n",
    "#     ytr.append(y_tr)\n",
    "# x_tr=np.reshape(xtr, (ntrain_all,35,ncells))\n",
    "# y_tr=np.reshape(ytr, (ntrain_all,1))\n",
    "# print(len(x_tr))\n",
    "# print(len(y_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is for internal reasons to initially tune parameters for the distributed protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulating distribution and encryption to find best\n",
    "#learning rate and momentum to be tried on actual implementation\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, initializers, regularizers, optimizers, callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "def model_pred(prob):\n",
    "    pred = []\n",
    "    for p in prob:\n",
    "        if p[0]>p[1]:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    return pred\n",
    "def sigmoidApprox(x):\n",
    "    degree = 3\n",
    "    interval = 3\n",
    "    if degree == 3:  \n",
    "        if interval == 3:\n",
    "            return 0.5 + 0.6997*K.pow(x/interval,1)-0.2649*K.pow(x/interval,3)\n",
    "        if interval == 5:\n",
    "            return 0.5 + 0.9917*K.pow(x/interval,1)-0.5592*K.pow(x/interval,3)\n",
    "        if interval == 7:\n",
    "            return 0.5 + 1.1511*K.pow(x/interval,1)-0.7517*K.pow(x/interval,3)\n",
    "        if interval == 8:\n",
    "            return 0.5 + 1.2010*K.pow(x/interval,1)-0.8156*K.pow(x/interval,2)\n",
    "        if interval == 12:\n",
    "            return 0.5 + 1.2384*K.pow(x/interval,1)-0.8647*K.pow(x/interval,2)\n",
    "#repeat cellcnn original training on full train data\n",
    "\n",
    "y_tr_n = to_categorical(y_tr)\n",
    "\n",
    "def pool_top_k(x, k):\n",
    "    return tf.reduce_mean(tf.sort(x, axis=1, direction='DESCENDING')[:, :k, :], axis=1)\n",
    "def create_model (k,ncell,nfilter,lr,m):\n",
    "        \n",
    "        data_input = keras.Input(shape=(ncell, 35))\n",
    "        coeff_l1=0\n",
    "        coeff_l2=1e-4\n",
    "        n_classes=2\n",
    "        conv = layers.Conv1D(filters=nfilter,\n",
    "                             kernel_size=1,\n",
    "                             activation='linear',\n",
    "                             kernel_initializer=initializers.GlorotNormal(),\n",
    "                             name='conv1')(data_input)\n",
    "\n",
    "        # the cell grouping part (top-k pooling)\n",
    "        pooled = layers.Lambda(pool_top_k, output_shape=(nfilter,), arguments={'k': k})(conv)\n",
    "        output = layers.Dense(units=n_classes,\n",
    "                                  activation=sigmoidApprox,\n",
    "                                  kernel_initializer=initializers.RandomUniform(),\n",
    "                                  name='output')(pooled)\n",
    "        model = keras.Model(inputs=data_input, outputs=output)\n",
    "\n",
    "        model.compile(optimizer=optimizers.SGD(learning_rate=lr,momentum=m),\n",
    "                          loss='mean_squared_error',\n",
    "                          metrics=['accuracy'])\n",
    "        return model\n",
    "lrs=[0.01,0.02,0.001, 0.005, 0.0001,0.0005]\n",
    "ms=[0.9,0.7]\n",
    "\n",
    "for l in lrs:\n",
    "    for m in ms:\n",
    "        model = create_model(ncells,ncells,nfilter,l,m)\n",
    "        #generate data\n",
    "        x_tr =np.asarray(x_tr)\n",
    "        x_tr_n = x_tr.transpose(0,2,1)\n",
    "        history = model.fit(x_tr_n, y_tr_n,\n",
    "                    batch_size=8,\n",
    "                    epochs=30,\n",
    "                    verbose=2,\n",
    "                    validation_split=0)\n",
    "        #select top ntest_all number of test multi-cells\n",
    "        \n",
    "        x_test = np.asarray(x_test)\n",
    "        x_test_n = x_test.transpose(0,2,1)\n",
    "        x_test_n = x_test_n[0:ntest_all,:]\n",
    "\n",
    "        y_test_n = to_categorical(y_test)\n",
    "        y_test_n = y_test_n[0:ntest_all,:]\n",
    "        y_test = y_test[0:ntest_all]\n",
    "        \n",
    "        print(\"learning rate is,\",l)\n",
    "        print(\"momentum is,\",m)\n",
    "        loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "        #score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "        print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "        y_pred = model.predict(x_test_n)\n",
    "        y_pred = model_pred(y_pred)\n",
    "\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"F-score:\",f1_score(y_test, y_pred))\n",
    "        print(\"precision:\",precision_score(y_test, y_pred))\n",
    "        print(\"recall:\",recall_score(y_test, y_pred)) \n",
    "        #For phenotype predictions on test set using all cells \n",
    "        print(ncell_test)\n",
    "        print(len(data_test[0]))\n",
    "        model2 = create_model(11663, 11663, nfilter,l,m)\n",
    "        weights = model.get_weights()\n",
    "        model2.set_weights(weights)\n",
    "        \n",
    "        data_test = np.array(data_test)\n",
    "        print(len(data_test))\n",
    "        data_test_n = data_test\n",
    "#         .transpose(0,2,1)\n",
    "        phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "        y_pred = model2.predict(data_test_n)\n",
    "        y_pred = model_pred(y_pred)\n",
    "        # print(y_pred)\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "        print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "        print(\"F-score:\",f1_score(phenotypes, y_pred))\n",
    "        print(\"precision:\",precision_score(phenotypes, y_pred))\n",
    "        print(\"recall:\",recall_score(phenotypes, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is to reproduce the CellCnn accuracies with the current generation of data (local, centralized etc) to reproduce the classification metrics with original architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The reproduction of original CellCnn model training without validation test and further analysis part\n",
    "#This part is used for the comparison of accuracy/precision/recall/f-score of CellCnn with secure distributed version\n",
    "#test on 100-cell multi-instances and full test set phenotype prediction\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, initializers, regularizers, optimizers, callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "#repeat cellcnn original training on full train data\n",
    "\n",
    "y_tr_n = to_categorical(y_tr)\n",
    "\n",
    "def pool_top_k(x, k):\n",
    "    return tf.reduce_mean(tf.sort(x, axis=1, direction='DESCENDING')[:, :k, :], axis=1)\n",
    "def create_model (k,ncell,nfilter):\n",
    "        \n",
    "        data_input = keras.Input(shape=(ncell, 35))\n",
    "        coeff_l1=0\n",
    "        coeff_l2=1e-4\n",
    "        n_classes=2\n",
    "        # the filters\n",
    "        conv = layers.Conv1D(filters=nfilter,\n",
    "                             kernel_size=1,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer=initializers.RandomUniform(),\n",
    "                             kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                             name='conv1')(data_input)\n",
    "\n",
    "        # the cell grouping part (top-k pooling)\n",
    "        pooled = layers.Lambda(pool_top_k, output_shape=(nfilter,), arguments={'k': k})(conv)\n",
    "        output = layers.Dense(units=n_classes,\n",
    "                                  activation='softmax',\n",
    "                                  kernel_initializer=initializers.RandomUniform(),\n",
    "                                  kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                                  name='output')(pooled)\n",
    "        model = keras.Model(inputs=data_input, outputs=output)\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "        return model\n",
    "model = create_model(ncells,ncells,nfilter)\n",
    "#generate data\n",
    "\n",
    "x_tr_n = x_tr.transpose(0,2,1)\n",
    "# Fit data to model\n",
    "shuffler = np.random.permutation(len(x_tr_n))\n",
    "x_tr_n = x_tr_n[shuffler]\n",
    "y_tr_n = y_tr_n[shuffler]\n",
    "\n",
    "history = model.fit(x_tr_n, y_tr_n,\n",
    "            batch_size=64,\n",
    "            epochs=30,\n",
    "            verbose=2,\n",
    "            validation_split=0)\n",
    "\n",
    "# list all data in history\n",
    "# print(history.history.keys())\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 100-cell predictions on test set\n",
    "def model_pred(prob):\n",
    "    pred = []\n",
    "    for p in prob:\n",
    "        if p[0]>p[1]:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    return pred\n",
    "\n",
    "x_test_n = x_test.transpose(0,2,1)\n",
    "x_test_n = x_test_n[0:ntest_all,:]\n",
    "\n",
    "y_test_n = to_categorical(y_test)\n",
    "y_test_n = y_test_n[0:ntest_all,:]\n",
    "loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "#score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "print(\"For 100-cell predictions on test set with size\",x_test_n.shape)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "y_pred = model.predict(x_test_n)\n",
    "y_pred = model_pred(y_pred)\n",
    "y_test= y_test[0:ntest_all]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "accmulti += accuracy_score(y_test, y_pred)\n",
    "precmulti += precision_score(y_test, y_pred)\n",
    "recallmulti += recall_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F-score:\",f1_score(y_test, y_pred))\n",
    "print(\"precision:\",precision_score(y_test, y_pred))\n",
    "print(\"recall:\",recall_score(y_test, y_pred)) \n",
    "#For phenotype predictions on test set using all cells \n",
    "\n",
    "model2 = create_model(10173, 10173,nfilter)\n",
    "weights = model.get_weights()\n",
    "model2.set_weights(weights)\n",
    "data_test_n = data_test\n",
    "# .transpose(0,2,1)\n",
    "phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "y_pred = model2.predict(data_test_n)\n",
    "y_pred = model_pred(y_pred)\n",
    "# print(y_pred)\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "print(\"F-score:\",f1_score(phenotypes, y_pred))\n",
    "print(\"precision:\",precision_score(phenotypes, y_pred))\n",
    "print(\"recall:\",recall_score(phenotypes, y_pred)) \n",
    "acccel += accuracy_score(phenotypes, y_pred)\n",
    "preccel += precision_score(phenotypes, y_pred)\n",
    "recallcel += recall_score(phenotypes, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
