{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "['CCR2', 'CCR4', 'CCR6', 'CCR7', 'CXCR4', 'CXCR5', 'CD103', 'CD14', 'CD20', 'CD25', 'CD27', 'CD28', 'CD3', 'CD4', 'CD45RA', 'CD45RO', 'CD56', 'CD57', 'CD69', 'CD8', 'TCRgd', 'PD.1', 'GM.CSF', 'IFN.g', 'IL.10', 'IL.13', 'IL.17A', 'IL.2', 'IL.21', 'IL.22', 'IL.3', 'IL.4', 'IL.6', 'IL.9', 'TNF.a', 'gate_source', 'manual_labels', 'labels', 'cell_id', 'cd4_labels', 'cd8_labels', 'run_0', 'run_1', 'run_2', 'intersection_3_runs']\n"
     ]
    }
   ],
   "source": [
    "import os, sys, errno, glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cellCNN_utils  \n",
    "from cellCNN_utils import loadFCS, ftrans, mkdir_p, get_items, generate_data, generate_normalized_data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "d = Path().resolve()\n",
    "sys.path.append(d)\n",
    "%pylab inline\n",
    "# define input and output directories\n",
    "FCS_DATA_PATH = os.path.join(d, 'FlowRepository')\n",
    "# select the relevant markers for further analysis\n",
    "data_fcs = loadFCS(glob.glob(FCS_DATA_PATH + '/discovery_cohort.fcs')[0], transform=None, auto_comp=False)\n",
    "print(data_fcs.channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------------SET PARAMS HERE -----------------------#\n",
    "\n",
    "cg = 'RRMS' #Control group, set to NIND or RRMS depending on experimental setting\n",
    "ncells = 100 #num cells per multi-cell input or multi-cell test data\n",
    "ncell_test = 5000 #num cells for phenotype prediction\n",
    "ntrain_all = 30000 # number of multi-cell inputs for the train data (in total)\n",
    "ntest_all = 10000 # number of multi-cell inputs for the test data\n",
    "nfilter = 8 #number of filters for the training\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "\n",
    "markers=['CCR2', 'CCR4', 'CCR6', 'CCR7', 'CXCR4', 'CXCR5', 'CD103', 'CD14', 'CD20', \n",
    "        'CD25', 'CD27', 'CD28', 'CD3', 'CD4', 'CD45RA', 'CD45RO', 'CD56', 'CD57', 'CD69', 'CD8', \n",
    "        'TCRgd', 'PD.1', 'GM.CSF', 'IFN.g', 'IL.10', 'IL.13', 'IL.17A', 'IL.2', 'IL.21', 'IL.22', 'IL.3',\n",
    "        'IL.4', 'IL.6', 'IL.9', 'TNF.a']\n",
    "len(markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gate_source is 35.index\n",
    "#gate_source is ind=1 in excell, label is ind=4\n",
    "#match gate_source from table NINDC=0, RRMS=1\n",
    "metadata= pd.read_excel(FCS_DATA_PATH+'/meta_data_discovery_cohort.xlsx')\n",
    "metadata=metadata.to_numpy()\n",
    "gate_source = metadata[:,1]\n",
    "labelsTemp = metadata[:,4]\n",
    "data = []\n",
    "sample_labels =[]\n",
    "for i in range(99):\n",
    "    cur_gs = gate_source[i]\n",
    "    cur_lab = labelsTemp[i]\n",
    "    patient_sample = []\n",
    "    if cur_lab == 'HD':\n",
    "        gs_ind = np.where(data_fcs.events[:,35]==cur_gs)\n",
    "        for j in gs_ind[0]:\n",
    "            patient_sample.append(data_fcs.events[j,0:35])\n",
    "        sample_labels.append(0)\n",
    "    elif cur_lab == cg:\n",
    "        gs_ind = np.where(data_fcs.events[:,35]==cur_gs)\n",
    "        for j in gs_ind[0]:\n",
    "            patient_sample.append(data_fcs.events[j,0:35])\n",
    "        sample_labels.append(1)\n",
    "    if len(patient_sample)>0:\n",
    "        data.append(np.asarray(patient_sample))\n",
    "sample_labels=np.asarray(sample_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we randomly split the samples in training/test sets.\n",
    "\n",
    "def train_test_split(train_idx1=[], train_idx2=[], test=True):\n",
    "    # set random seed for reproducible results and monte-carlo repetitions only for training!\n",
    "#     np.random.seed(4)\n",
    "    \n",
    "    # split the fcs files into training, validation and test set (note that secure-protocols do not use validation sets)\n",
    "    group1 = np.where(sample_labels == 0)[0]\n",
    "    group2 = np.where(sample_labels == 1)[0]\n",
    "    l1, l2 = len(group1), len(group2)\n",
    "    ntrain_per_class = 24\n",
    "    ntest_group1 = l1 - ntrain_per_class\n",
    "    ntest_group2 = l2 - ntrain_per_class\n",
    "\n",
    "    # get the sample indices\n",
    "    train_idx1 = list(np.random.choice(group1, size=ntrain_per_class, replace=False))\n",
    "    test_idx1 = [i for i in group1 if i not in train_idx1]\n",
    "    train_idx2 = list(np.random.choice(group2, size=ntrain_per_class, replace=False))\n",
    "    test_idx2 = [i for i in group2 if i not in train_idx2]\n",
    "\n",
    "    print(\"test indices\")\n",
    "    test_indices = [test_idx1,test_idx2]\n",
    "    print(test_indices) \n",
    "    print(\"train indices\")\n",
    "    print(train_idx1)\n",
    "    print(train_idx2)\n",
    "\n",
    "    train_indices = [train_idx1,train_idx2]\n",
    "\n",
    "    # load the training samples\n",
    "    group1_list, group2_list = [], []\n",
    "    for idx in train_idx1:\n",
    "        x = data[idx][:]\n",
    "        group1_list.append(x)\n",
    "\n",
    "    for idx in train_idx2:\n",
    "        x = data[idx][:]\n",
    "        group2_list.append(x)\n",
    "\n",
    "    # load the test samples\n",
    "    t_group1_list, t_group2_list = [], []\n",
    "    test_phenotypes = []\n",
    "    for idx in test_idx1:\n",
    "        x = data[idx][:]\n",
    "        t_group1_list.append(x)\n",
    "        test_phenotypes.append(0)\n",
    "\n",
    "    for idx in test_idx2:\n",
    "        x = data[idx][:]\n",
    "        t_group2_list.append(x)\n",
    "        test_phenotypes.append(1)\n",
    "\n",
    "    # finally prepare training data\n",
    "    cut = int(1 * len(group1_list))\n",
    "    train_samples = group1_list[:cut] + group2_list[:cut]\n",
    "    train_phenotypes = [0] * len(group1_list[:cut]) + [1] * len(group2_list[:cut])\n",
    "    valid_samples = group1_list[cut:] + group2_list[cut:]\n",
    "    valid_phenotypes = [0] * len(group1_list[cut:]) + [1] * len(group2_list[cut:])\n",
    "    test_samples = t_group1_list + t_group2_list\n",
    "    print(test_phenotypes)\n",
    "    return train_samples,train_phenotypes,test_samples,test_phenotypes, test_indices,train_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test indices\n",
      "[[19, 44, 50, 53, 55], [6, 7, 11, 13, 25, 37, 41]]\n",
      "train indices\n",
      "[57, 58, 43, 45, 0, 10, 49, 26, 21, 17, 15, 54, 36, 18, 35, 48, 20, 30, 27, 31, 52, 51, 56, 1]\n",
      "[8, 9, 2, 4, 40, 28, 5, 34, 39, 22, 47, 42, 16, 23, 59, 33, 12, 29, 32, 14, 24, 3, 38, 46]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#debug, for changes introduced to cellCNN_utils\n",
    "# import importlib\n",
    "# importlib.reload(cellCNN_utils)\n",
    "\n",
    "train_samples, train_phenotypes, test_samples, test_phenotypes, test_indices, train_indices = train_test_split()\n",
    "# generate ntrain_all (ntrain_all/2 per phenotype) training samples for centralized test!\n",
    "\n",
    "# scaler,x_tr,y_tr = generate_data(train_samples, train_phenotypes, 'Flow/', generate_valid_set=False, \n",
    "#                                                ncell=ncells, nsubset=int(ntrain_all/2), scale=True, \n",
    "#                                                per_sample=False, verbose=0, saveFile=False,subset_selection = 'random')\n",
    "\n",
    "\n",
    "scaler,x_test,y_test = generate_data(test_samples, test_phenotypes, 'Flow/', generate_valid_set=False, \n",
    "                                               ncell=ncells, nsubset=5000, scale=True, \n",
    "                                               per_sample=False, verbose=0, saveFile=True,\n",
    "                                               subset_selection = 'random', generateAsTest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions based on multi-cell inputs containing 10173 cells.\n",
      "(12, 10173, 35)\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "#generate also the test set on full min-ncell per sample:\n",
    "def generate_for_pheno_prediction(new_samples,phenotypes,scaler):\n",
    "        ncell_per_sample = np.min([x.shape[0] for x in new_samples])\n",
    "        print(f\"Predictions based on multi-cell inputs containing {ncell_per_sample} cells.\")\n",
    "        nmark = len(new_samples[0][1])\n",
    "        # scale the new samples if we did that for the training samples\n",
    "        if scaler is not None:\n",
    "            new_samples = [scaler.transform(x) for x in new_samples]\n",
    "        new_samples = [shuffle(x)[:ncell_per_sample].reshape(1, ncell_per_sample, nmark)\n",
    "                           for x in new_samples]\n",
    "        data_test = np.vstack(new_samples)\n",
    "        mkdir_p('Flow/X_test_all/')\n",
    "        for i in range(len(data_test)):\n",
    "            np.savetxt('Flow/' + 'X_test_all/' + str(i) +'.txt', (transpose(data_test[i])))\n",
    "        np.savetxt('Flow/' + 'y_test_all.txt', phenotypes)\n",
    "        return data_test,phenotypes,ncell_per_sample\n",
    "    \n",
    "data_test,phenotypes,ncell_per_sample = generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "print(shape(data_test))\n",
    "print(phenotypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set indices:\n",
      "[19, 44, 50, 53, 55]\n",
      "[6, 7, 11, 13, 25, 37, 41]\n",
      "Global train set indices:\n",
      "[54, 27, 20, 15, 48, 52, 18, 31, 45, 17, 30, 49, 21, 51, 1, 10, 43, 57, 26, 56, 35, 0, 36, 58]\n",
      "[28, 40, 8, 38, 5, 29, 42, 39, 47, 23, 24, 4, 14, 22, 46, 16, 12, 33, 32, 59, 34, 9, 2, 3]\n",
      "Global train splitted among hosts - indices:\n",
      "[[58, 36, 0, 35], [56, 26, 57, 43], [10, 1, 51, 21], [49, 30, 17, 45], [31, 18, 52, 48], [15, 20, 27, 54]]\n",
      "[[28, 40, 8, 38], [5, 29, 42, 39], [47, 23, 24, 4], [14, 22, 46, 16], [12, 33, 32, 59], [34, 9, 2, 3]]\n",
      "\n",
      "Host no. 0 :\n",
      "host_idx_1: [58, 36, 0, 35] - host_idx_2: [28, 40, 8, 38]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1]\n",
      "old scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 1 :\n",
      "host_idx_1: [56, 26, 57, 43] - host_idx_2: [5, 29, 42, 39]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1]\n",
      "old scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 2 :\n",
      "host_idx_1: [10, 1, 51, 21] - host_idx_2: [47, 23, 24, 4]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1]\n",
      "old scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 3 :\n",
      "host_idx_1: [49, 30, 17, 45] - host_idx_2: [14, 22, 46, 16]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1]\n",
      "old scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 4 :\n",
      "host_idx_1: [31, 18, 52, 48] - host_idx_2: [12, 33, 32, 59]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1]\n",
      "old scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 5 :\n",
      "host_idx_1: [15, 20, 27, 54] - host_idx_2: [34, 9, 2, 3]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1]\n",
      "old scale\n",
      "Generating multi-cell inputs...\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(cellCNN_utils)\n",
    "\n",
    "# Here we randomly split the samples for n hosts\n",
    "nhosts=6\n",
    "\n",
    "test_idx1 = test_indices[0]\n",
    "test_idx2 = test_indices[1]\n",
    "\n",
    "train_idx1 = train_indices[0]\n",
    "train_idx2 = train_indices[1]\n",
    "\n",
    "train_idx1 = shuffle(train_idx1)\n",
    "train_idx2 = shuffle(train_idx2)\n",
    "\n",
    "print(\"Test set indices:\")\n",
    "print(test_idx1)\n",
    "print(test_idx2)\n",
    "print(\"Global train set indices:\")\n",
    "\n",
    "#to take the runs on 10 different distributions (for box plot)\n",
    "\n",
    "train_idx1 = shuffle(train_idx1)\n",
    "train_idx2 = shuffle(train_idx2)\n",
    "print(train_idx1)\n",
    "print(train_idx2)\n",
    "\n",
    "#distribute train indices balanced among n hosts:\n",
    "split_idx_1 = []\n",
    "split_idx_2 = []\n",
    "group1_list = np.flip(np.array_split(numpy.array(train_idx1), nhosts))\n",
    "group2_list = numpy.array_split(numpy.array(train_idx2), nhosts)\n",
    "\n",
    "for i in range(nhosts):\n",
    "    split_idx_1.append(group1_list[i].tolist())\n",
    "    split_idx_2.append(group2_list[i].tolist())\n",
    "\n",
    "print(\"Global train splitted among hosts - indices:\")\n",
    "print(split_idx_1)\n",
    "print(split_idx_2)\n",
    "\n",
    "xtr = []\n",
    "ytr=[]\n",
    "for i in range(nhosts):\n",
    "    print(\"\\nHost no.\", i, \":\")\n",
    "    folder_path = 'splitFlow' + str(nhosts) + '/host' + str(i) + '/'\n",
    "    host_idx_1 = split_idx_1[i]\n",
    "    host_idx_2 = split_idx_2[i]\n",
    "    print(\"host_idx_1:\", host_idx_1, \"- host_idx_2:\", host_idx_2)\n",
    "     # load the training samples\n",
    "    host_group1_list, host_group2_list = [], []\n",
    "    train_samples,train_phenotypes = [],[]\n",
    "    for idx in host_idx_1:\n",
    "        x = data[idx][:]\n",
    "        host_group1_list.append(x)\n",
    "\n",
    "    for idx in host_idx_2:\n",
    "        x = data[idx][:]\n",
    "        host_group2_list.append(x)\n",
    "\n",
    "    # finally prepare training and vallidation data\n",
    "    cut = int(1 * len(host_group1_list))\n",
    "    train_samples = host_group1_list[:cut] + host_group2_list[:cut]\n",
    "    train_phenotypes = [0] * len(host_group1_list[:cut]) + [1] * len(host_group2_list[:cut])\n",
    "    print(train_phenotypes)\n",
    "    scaler,x_tr,y_tr = generate_data(train_samples, train_phenotypes, folder_path, scale=True, ncell=ncells, \n",
    "                  nsubset=int(ntrain_all/(2*nhosts)),per_sample=False, verbose=0,generate_valid_set=False,\n",
    "                                     saveFile=True,scaler=scaler,subset_selection = 'random',oneFile=None)\n",
    "    xtr.append(x_tr)\n",
    "    ytr.append(y_tr)\n",
    "x_tr=np.reshape(xtr, (ntrain_all,35,ncells))\n",
    "y_tr=np.reshape(ytr, (ntrain_all,1))\n",
    "print(len(x_tr))\n",
    "print(len(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulating distribution and encryption to find best\n",
    "#learning rate and momentum to be tried on actual implementation\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, initializers, regularizers, optimizers, callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "def model_pred(prob):\n",
    "    pred = []\n",
    "    for p in prob:\n",
    "        if p[0]>p[1]:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    return pred\n",
    "def sigmoidApprox(x):\n",
    "    degree = 3\n",
    "    interval = 3\n",
    "    if degree == 3:  \n",
    "        if interval == 3:\n",
    "            return 0.5 + 0.6997*K.pow(x/interval,1)-0.2649*K.pow(x/interval,3)\n",
    "        if interval == 5:\n",
    "            return 0.5 + 0.9917*K.pow(x/interval,1)-0.5592*K.pow(x/interval,3)\n",
    "        if interval == 7:\n",
    "            return 0.5 + 1.1511*K.pow(x/interval,1)-0.7517*K.pow(x/interval,3)\n",
    "        if interval == 8:\n",
    "            return 0.5 + 1.2010*K.pow(x/interval,1)-0.8156*K.pow(x/interval,2)\n",
    "        if interval == 12:\n",
    "            return 0.5 + 1.2384*K.pow(x/interval,1)-0.8647*K.pow(x/interval,2)\n",
    "#repeat cellcnn original training on full train data\n",
    "\n",
    "y_tr_n = to_categorical(y_tr)\n",
    "\n",
    "def pool_top_k(x, k):\n",
    "    return tf.reduce_mean(tf.sort(x, axis=1, direction='DESCENDING')[:, :k, :], axis=1)\n",
    "def create_model (k,ncell,nfilter,lr,m):\n",
    "        \n",
    "        data_input = keras.Input(shape=(ncell, 35))\n",
    "        coeff_l1=0\n",
    "        coeff_l2=1e-4\n",
    "        n_classes=2\n",
    "        conv = layers.Conv1D(filters=nfilter,\n",
    "                             kernel_size=1,\n",
    "                             activation='linear',\n",
    "                             kernel_initializer=initializers.GlorotNormal(),\n",
    "                             name='conv1')(data_input)\n",
    "\n",
    "        # the cell grouping part (top-k pooling)\n",
    "        pooled = layers.Lambda(pool_top_k, output_shape=(nfilter,), arguments={'k': k})(conv)\n",
    "        output = layers.Dense(units=n_classes,\n",
    "                                  activation=sigmoidApprox,\n",
    "                                  kernel_initializer=initializers.RandomUniform(),\n",
    "                                  name='output')(pooled)\n",
    "        model = keras.Model(inputs=data_input, outputs=output)\n",
    "\n",
    "        model.compile(optimizer=optimizers.SGD(learning_rate=lr,momentum=m),\n",
    "                          loss='mean_squared_error',\n",
    "                          metrics=['accuracy'])\n",
    "        return model\n",
    "lrs=[0.01,0.02,0.001, 0.005, 0.0001,0.0005]\n",
    "ms=[0.9,0.7]\n",
    "\n",
    "for l in lrs:\n",
    "    for m in ms:\n",
    "        model = create_model(ncells,ncells,nfilter,l,m)\n",
    "        #generate data\n",
    "        x_tr =np.asarray(x_tr)\n",
    "        x_tr_n = x_tr.transpose(0,2,1)\n",
    "        history = model.fit(x_tr_n, y_tr_n,\n",
    "                    batch_size=8,\n",
    "                    epochs=30,\n",
    "                    verbose=2,\n",
    "                    validation_split=0)\n",
    "        #select top ntest_all number of test multi-cells\n",
    "        \n",
    "        x_test = np.asarray(x_test)\n",
    "        x_test_n = x_test.transpose(0,2,1)\n",
    "        x_test_n = x_test_n[0:ntest_all,:]\n",
    "\n",
    "        y_test_n = to_categorical(y_test)\n",
    "        y_test_n = y_test_n[0:ntest_all,:]\n",
    "        y_test = y_test[0:ntest_all]\n",
    "        \n",
    "        print(\"learning rate is,\",l)\n",
    "        print(\"momentum is,\",m)\n",
    "        loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "        #score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "        print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "        y_pred = model.predict(x_test_n)\n",
    "        y_pred = model_pred(y_pred)\n",
    "\n",
    "\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"F-score:\",f1_score(y_test, y_pred))\n",
    "        print(\"precision:\",precision_score(y_test, y_pred))\n",
    "        print(\"recall:\",recall_score(y_test, y_pred)) \n",
    "        #For phenotype predictions on test set using all cells \n",
    "        print(ncell_test)\n",
    "        print(len(data_test[0]))\n",
    "        model2 = create_model(11663, 11663, nfilter,l,m)\n",
    "        weights = model.get_weights()\n",
    "        model2.set_weights(weights)\n",
    "        \n",
    "        data_test = np.array(data_test)\n",
    "        print(len(data_test))\n",
    "        data_test_n = data_test\n",
    "#         .transpose(0,2,1)\n",
    "        phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "        y_pred = model2.predict(data_test_n)\n",
    "        y_pred = model_pred(y_pred)\n",
    "        # print(y_pred)\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "        print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "        print(\"F-score:\",f1_score(phenotypes, y_pred))\n",
    "        print(\"precision:\",precision_score(phenotypes, y_pred))\n",
    "        print(\"recall:\",recall_score(phenotypes, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The reproduction of original CellCnn model training without validation test and further analysis part\n",
    "#This part is used for the comparison of accuracy/precision/recall/f-score of CellCnn with secure distributed version\n",
    "#test on 100-cell multi-instances and full test set phenotype prediction\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, initializers, regularizers, optimizers, callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "#repeat cellcnn original training on full train data\n",
    "\n",
    "y_tr_n = to_categorical(y_tr)\n",
    "\n",
    "def pool_top_k(x, k):\n",
    "    return tf.reduce_mean(tf.sort(x, axis=1, direction='DESCENDING')[:, :k, :], axis=1)\n",
    "def create_model (k,ncell,nfilter):\n",
    "        \n",
    "        data_input = keras.Input(shape=(ncell, 35))\n",
    "        coeff_l1=0\n",
    "        coeff_l2=1e-4\n",
    "        n_classes=2\n",
    "        # the filters\n",
    "        conv = layers.Conv1D(filters=nfilter,\n",
    "                             kernel_size=1,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer=initializers.RandomUniform(),\n",
    "                             kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                             name='conv1')(data_input)\n",
    "\n",
    "        # the cell grouping part (top-k pooling)\n",
    "        pooled = layers.Lambda(pool_top_k, output_shape=(nfilter,), arguments={'k': k})(conv)\n",
    "        output = layers.Dense(units=n_classes,\n",
    "                                  activation='softmax',\n",
    "                                  kernel_initializer=initializers.RandomUniform(),\n",
    "                                  kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                                  name='output')(pooled)\n",
    "        model = keras.Model(inputs=data_input, outputs=output)\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "        return model\n",
    "model = create_model(ncells,ncells,nfilter)\n",
    "#generate data\n",
    "\n",
    "x_tr_n = x_tr.transpose(0,2,1)\n",
    "# Fit data to model\n",
    "shuffler = np.random.permutation(len(x_tr_n))\n",
    "x_tr_n = x_tr_n[shuffler]\n",
    "y_tr_n = y_tr_n[shuffler]\n",
    "\n",
    "history = model.fit(x_tr_n, y_tr_n,\n",
    "            batch_size=64,\n",
    "            epochs=30,\n",
    "            verbose=2,\n",
    "            validation_split=0)\n",
    "\n",
    "# list all data in history\n",
    "# print(history.history.keys())\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accmulti,precmulti,recallmulti,acccel,preccel,recallcel = 0,0,0,0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 100-cell predictions on test set\n",
    "def model_pred(prob):\n",
    "    pred = []\n",
    "    for p in prob:\n",
    "        if p[0]>p[1]:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    return pred\n",
    "\n",
    "x_test_n = x_test.transpose(0,2,1)\n",
    "x_test_n = x_test_n[0:ntest_all,:]\n",
    "\n",
    "y_test_n = to_categorical(y_test)\n",
    "y_test_n = y_test_n[0:ntest_all,:]\n",
    "loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "#score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "print(\"For 100-cell predictions on test set with size\",x_test_n.shape)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "y_pred = model.predict(x_test_n)\n",
    "y_pred = model_pred(y_pred)\n",
    "y_test= y_test[0:ntest_all]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "accmulti += accuracy_score(y_test, y_pred)\n",
    "precmulti += precision_score(y_test, y_pred)\n",
    "recallmulti += recall_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F-score:\",f1_score(y_test, y_pred))\n",
    "print(\"precision:\",precision_score(y_test, y_pred))\n",
    "print(\"recall:\",recall_score(y_test, y_pred)) \n",
    "#For phenotype predictions on test set using all cells \n",
    "\n",
    "model2 = create_model(10173, 10173,nfilter)\n",
    "weights = model.get_weights()\n",
    "model2.set_weights(weights)\n",
    "data_test_n = data_test\n",
    "# .transpose(0,2,1)\n",
    "phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "y_pred = model2.predict(data_test_n)\n",
    "y_pred = model_pred(y_pred)\n",
    "# print(y_pred)\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "print(\"F-score:\",f1_score(phenotypes, y_pred))\n",
    "print(\"precision:\",precision_score(phenotypes, y_pred))\n",
    "print(\"recall:\",recall_score(phenotypes, y_pred)) \n",
    "acccel += accuracy_score(phenotypes, y_pred)\n",
    "preccel += precision_score(phenotypes, y_pred)\n",
    "recallcel += recall_score(phenotypes, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
