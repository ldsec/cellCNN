{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellCnn [1] data generation\n",
    "\n",
    "source: https://github.com/eiriniar/CellCnn\n",
    "\n",
    "\"\"\" Copyright 2016-2017 ETH Zurich, Eirini Arvaniti and Manfred Claassen.\n",
    "\n",
    "This module contains data preprocessing/distribution functions.\n",
    "\n",
    "\"\"\"\n",
    "The code is slightly changed depending on original implementation to make it compatible with decentralized settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we preprocess and distribute a mass cytometry dataset to characterize Cytomegalovirus (CMV) infection [2].\n",
    "\n",
    "The dataset comprises mass cytometry measurements of 36 markers, including 28 NK cell receptors, for PBMC samples of 20 donors with varying serology for CMV. \n",
    "\n",
    "The example depends on **manually gated NK cell compartment**. To run this example, \n",
    "\n",
    "    - download the [NK cell dataset] at https://imsb.ethz.ch/research/claassen/Software/cellcnn.html,\n",
    "    - uncompress and place it in the data/cellCNN/ folder\n",
    "\n",
    "Data distribution: We fix the test set for all experimental settings, the training dataset is then generated by distribution different donors for each institution depending on number of institutions.\n",
    "\n",
    "[1] E. Arvaniti and M. Claassen. Sensitive detection of rare disease-associated cell subsets via representation learning.Nat Commun, 8:1â€“10, 2017\n",
    "[2] Horowitz, A. et al. Genetic and environmental determinants of human NK cell diversity revealed by mass cytometry. Sci. Transl. Med. 5 (2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install FlowIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['shuffle']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import os, sys, errno, glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cellCNN_utils  \n",
    "from cellCNN_utils import loadFCS, ftrans, mkdir_p, get_items, generate_data, generate_normalized_data\n",
    "\n",
    "from pathlib import Path\n",
    "d = Path().resolve()\n",
    "sys.path.append(d)\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input and output directories\n",
    "FCS_DATA_PATH = os.path.join(d, 'NK_cell_dataset', 'gated_NK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'Cell_length', 'CD3', 'Dead', '(La139)Dd', 'CD27', 'CD19', 'CD4', 'CD8', 'CD57', '2DL1-S1', 'TRAIL', '2DL2-L3-S2', 'CD16', 'CD10', '3DL1-S1', 'CD117', '2DS4', 'ILT2-CD85j', 'NKp46', 'NKG2D', 'NKG2C', '2B4', 'CD33', 'CD11b', 'NKp30', 'CD122', '3DL1', 'NKp44', 'CD127', '2DL1', 'CD94', 'CD34', 'CCR7', '2DL3', 'NKG2A', 'HLA-DR', '2DL4', 'CD56', '2DL5', 'CD25', 'DNA1', 'DNA2']\n",
      "(43,)\n"
     ]
    }
   ],
   "source": [
    "# list of measured markers in the dataset:\n",
    "data_fcs = loadFCS(glob.glob(FCS_DATA_PATH + '/*.fcs')[0], transform=None, auto_comp=False)\n",
    "print(data_fcs.channels)\n",
    "print(shape(data_fcs.channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n"
     ]
    }
   ],
   "source": [
    "# select the relevant markers for data generation, same markers originally used in cellCnn analysis\n",
    "markers = ['CD3', 'CD27', 'CD19', 'CD4', 'CD8', 'CD57', '2DL1-S1', 'TRAIL', '2DL2-L3-S2',\n",
    "           'CD16', 'CD10', '3DL1-S1', 'CD117', '2DS4', 'ILT2-CD85j', 'NKp46', 'NKG2D',\n",
    "           'NKG2C', '2B4', 'CD33', 'CD11b', 'NKp30', 'CD122', '3DL1', 'NKp44', 'CD127', '2DL1',\n",
    "           'CD94', 'CD34', 'CCR7', '2DL3', 'NKG2A', 'HLA-DR', '2DL4', 'CD56', '2DL5', 'CD25']\n",
    "marker_idx = [data_fcs.channels.index(label) for label in markers]\n",
    "print(marker_idx)\n",
    "nmark = len(markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a_001_NK.fcs' 'a_002_NK.fcs' 'a_003_NK.fcs' 'a_004_NK.fcs'\n",
      " 'a_005_NK.fcs' 'a_006_NK.fcs' 'a_007_NK.fcs' 'a_009_NK.fcs'\n",
      " 'a_010_NK.fcs' 'a_011_NK.fcs' 'a_012_NK.fcs' 'a_1a_NK.fcs' 'a_2a_NK.fcs'\n",
      " 'a_2b_NK.fcs' 'a_3a_NK.fcs' 'a_3b_NK.fcs' 'a_4a_NK.fcs' 'a_4b_NK.fcs'\n",
      " 'a_5a_NK.fcs' 'a_5b_NK.fcs']\n"
     ]
    }
   ],
   "source": [
    "# load the sample names and corresponding labels (0: CMV-, 1: CMV+), here from a CSV file\n",
    "# prior CMV infection status is obtained from the original study (Horowitz et al. 2013)\n",
    "csv_file = 'NK_fcs_samples_with_labels.csv'\n",
    "fcs_info = np.array(pd.read_csv(csv_file, sep=','))\n",
    "sample_ids = fcs_info[:, 0]\n",
    "sample_labels = fcs_info[:, 1].astype(int)\n",
    "print(sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we randomly split the samples in training/test sets.\n",
    "\n",
    "def train_test_split(train_idx1=[], train_idx2=[], test=True):\n",
    "    # set random seed for reproducible results\n",
    "    np.random.seed(12345)\n",
    "\n",
    "    # cofactor for arcsinh transformation\n",
    "    cofactor = 5\n",
    "    \n",
    "    # split the fcs files into training, validation and test set (note that secure-protocols do not use validation sets)\n",
    "    group1 = np.where(sample_labels == 0)[0]\n",
    "    group2 = np.where(sample_labels == 1)[0]\n",
    "    l1, l2 = len(group1), len(group2)\n",
    "    ntrain_per_class = 7\n",
    "    ntest_group1 = l1 - ntrain_per_class\n",
    "    ntest_group2 = l2 - ntrain_per_class\n",
    "\n",
    "    # get the sample indices\n",
    "    train_idx1 = list(np.random.choice(group1, size=ntrain_per_class, replace=False))\n",
    "    test_idx1 = [i for i in group1 if i not in train_idx1]\n",
    "    train_idx2 = list(np.random.choice(group2, size=ntrain_per_class, replace=False))\n",
    "    test_idx2 = [i for i in group2 if i not in train_idx2]\n",
    "    \n",
    "    print(\"test indices\")\n",
    "    test_indices = [test_idx1,test_idx2]\n",
    "    print(test_indices)\n",
    "\n",
    "    print(\"train indices\")\n",
    "\n",
    "    \n",
    "    print(train_idx1)\n",
    "    print(train_idx2)\n",
    "    train_indices = [train_idx1,train_idx2]\n",
    "\n",
    "    # load the training samples\n",
    "    group1_list, group2_list = [], []\n",
    "    for idx in train_idx1:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        group1_list.append(x)\n",
    "\n",
    "    for idx in train_idx2:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        group2_list.append(x)\n",
    "\n",
    "    # load the test samples\n",
    "    t_group1_list, t_group2_list = [], []\n",
    "    test_phenotypes = []\n",
    "    for idx in test_idx1:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        t_group1_list.append(x)\n",
    "        test_phenotypes.append(0)\n",
    "\n",
    "    for idx in test_idx2:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        t_group2_list.append(x)\n",
    "        test_phenotypes.append(1)\n",
    "\n",
    "    # finally prepare training data\n",
    "    cut = int(1 * len(group1_list))\n",
    "    train_samples = group1_list[:cut] + group2_list[:cut]\n",
    "    train_phenotypes = [0] * len(group1_list[:cut]) + [1] * len(group2_list[:cut])\n",
    "    valid_samples = group1_list[cut:] + group2_list[cut:]\n",
    "    valid_phenotypes = [0] * len(group1_list[cut:]) + [1] * len(group2_list[cut:])\n",
    "    test_samples = t_group1_list + t_group2_list\n",
    "\n",
    "    return train_samples,train_phenotypes,test_samples,test_phenotypes, test_indices,train_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate original data (with transform)\n",
    "In the following, \n",
    "- We generate training data with $ncell=200$ cells per sample and $nsubset=1000$ samples per class\n",
    "- We generate the test data for $ncell=200$ cells per sample from test indices, called X_test\n",
    "- We generate another test set 'per-individual' in test indices using maximum number of cells to use for phenotype prediction, called X_test_all\n",
    "\n",
    "Processed data is placed under originalNK/ folder\n",
    "\n",
    "The script prints the max number of cells for the current example (i.e., 5652 for this dataset) which then will be used as a parameter in the golang protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test indices\n",
      "[[3, 5, 8, 9], [1, 6]]\n",
      "train indices\n",
      "[12, 2, 7, 11, 15, 14, 13]\n",
      "[16, 0, 4, 10, 18, 19, 17]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "Predictions based on multi-cell inputs containing 5652 cells.\n",
      "(6, 5652, 37)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train_samples, train_phenotypes, test_samples, test_phenotypes, test_indices, train_indices = train_test_split()\n",
    "\n",
    "scaler,x_tr,y_tr,x_test,y_test = generate_data(train_samples, train_phenotypes, 'originalNK/', valid_samples=test_samples, valid_phenotypes=test_phenotypes, \n",
    "                                               ncell=200, nsubset=1000, per_sample=False, verbose=0, saveFile=False)\n",
    "\n",
    "#generate also the test set on full max-ncell per sample:\n",
    "def generate_for_pheno_prediction(new_samples,phenotypes,scaler):\n",
    "        ncell_per_sample = np.min([x.shape[0] for x in new_samples])\n",
    "        print(f\"Predictions based on multi-cell inputs containing {ncell_per_sample} cells.\")\n",
    "        nmark = len(new_samples[0][1])\n",
    "        # z-transform the new samples if we did that for the training samples\n",
    "        if scaler is not None:\n",
    "            new_samples = [scaler.transform(x) for x in new_samples]\n",
    "        new_samples = [shuffle(x)[:ncell_per_sample].reshape(1, ncell_per_sample, nmark)\n",
    "                           for x in new_samples]\n",
    "        data_test = np.vstack(new_samples)\n",
    "#         mkdir_p('originalNK/X_test_all/')\n",
    "#         for i in range(len(data_test)):\n",
    "#             np.savetxt('originalNK/' + 'X_test_all/' + str(i) +'.txt', (transpose(data_test[i])))\n",
    "#         np.savetxt('originalNK/' + 'y_test_all.txt', phenotypes)\n",
    "        return data_test,phenotypes\n",
    "\n",
    "data_test,phenotypes=generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "print(shape(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate normalized data\n",
    "In the following, \n",
    "- We generate normalized training data with $ncell=200$ cells per sample and $nsubset=1000$ samples per class\n",
    "- We generate the test data for $ncell=200$ cells per sample from test indices, called X_test\n",
    "- We generate another test set 'per-individual' in test indices using maximum number of cells to use for phenotype prediction, called X_test_all\n",
    "\n",
    "Processed data is placed under normalizedNK/ folder\n",
    "\n",
    "The script prints the max number of cells for the current example (i.e., 5652 for this dataset) which then will be used as a parameter in the golang protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "train_samples, train_phenotypes, test_samples, test_phenotypes, test_indices, train_indices = train_test_split()\n",
    "generate_normalized_data(train_samples, train_phenotypes, 'normalizedNK/', valid_samples=test_samples, valid_phenotypes=test_phenotypes, ncell=200, nsubset=1000)\n",
    "\n",
    "#generate also the test set on full ncell per sample:\n",
    "def generate_for_pheno_prediction(new_samples,phenotypes):\n",
    "        ncell_per_sample = np.min([x.shape[0] for x in new_samples])\n",
    "        print(f\"Predictions based on multi-cell inputs containing {ncell_per_sample} cells.\")\n",
    "        nmark = len(new_samples[0][1])\n",
    "\n",
    "        new_samples = [shuffle(x)[:ncell_per_sample].reshape(1, ncell_per_sample, nmark)\n",
    "                           for x in new_samples]\n",
    "        data_test = np.vstack(new_samples)\n",
    "        mkdir_p('normalizedNK/X_test_all/')\n",
    "        for i in range(len(data_test)):\n",
    "            np.savetxt('normalizedNK/' + 'X_test_all/' + str(i) +'.txt', normalize(transpose(data_test[i])))\n",
    "        np.savetxt('normalizedNK/' + 'y_test_all.txt', phenotypes)\n",
    "        return data_test\n",
    "\n",
    "data_test=generate_for_pheno_prediction(test_samples,test_phenotypes)\n",
    "print(shape(data_test))\n",
    "print(shape(test_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate  data split between $nhosts$ parties\n",
    "In the following, \n",
    "- We generate training data with $ncell=200$ cells per sample and $nsubset=1000$ samples per class, per party\n",
    "- Example below distributes the train indices per donor for 3 parties\n",
    "\n",
    "Processed data is placed under splitNK/host_i for party-i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set indices:\n",
      "[3, 5, 8, 9]\n",
      "[1, 6]\n",
      "Global train set indices:\n",
      "[12, 2, 7, 11, 15, 14, 13]\n",
      "[16, 0, 4, 10, 18, 19, 17]\n",
      "Global train splitted among hosts - indices:\n",
      "[[15, 14, 13], [12, 2, 7, 11]]\n",
      "[[16, 0, 4, 10], [18, 19, 17]]\n",
      "\n",
      "Host no. 0 :\n",
      "host_idx_1: [15, 14, 13] - host_idx_2: [16, 0, 4, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 1 :\n",
      "host_idx_1: [12, 2, 7, 11] - host_idx_2: [18, 19, 17]\n",
      "[0, 0, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Here we randomly split the samples in training/test sets.\n",
    "nhosts=2\n",
    "cofactor = 5\n",
    "test_idx1 = test_indices[0]\n",
    "test_idx2 = test_indices[1]\n",
    "\n",
    "train_idx1 = train_indices[0]\n",
    "train_idx2 = train_indices[1]\n",
    "\n",
    "\n",
    "print(\"Test set indices:\")\n",
    "print(test_idx1)\n",
    "print(test_idx2)\n",
    "print(\"Global train set indices:\")\n",
    "print(train_idx1)\n",
    "print(train_idx2)\n",
    "\n",
    "#distribute train indices balanced among n hosts:\n",
    "split_idx_1 = []\n",
    "split_idx_2 = []\n",
    "group1_list = np.flip(np.array_split(numpy.array(train_idx1), nhosts))\n",
    "group2_list = numpy.array_split(numpy.array(train_idx2), nhosts)\n",
    "\n",
    "for i in range(nhosts):\n",
    "    split_idx_1.append(group1_list[i].tolist())\n",
    "    split_idx_2.append(group2_list[i].tolist())\n",
    "\n",
    "print(\"Global train splitted among hosts - indices:\")\n",
    "print(split_idx_1)\n",
    "print(split_idx_2)\n",
    "\n",
    "\n",
    "for i in range(nhosts):\n",
    "    print(\"\\nHost no.\", i, \":\")\n",
    "    folder_path = 'splitNK2/host' + str(i) + '/'\n",
    "    host_idx_1 = split_idx_1[i]\n",
    "    host_idx_2 = split_idx_2[i]\n",
    "    print(\"host_idx_1:\", host_idx_1, \"- host_idx_2:\", host_idx_2)\n",
    "     # load the training samples\n",
    "    host_group1_list, host_group2_list = [], []\n",
    "    train_samples,train_phenotypes = [],[]\n",
    "    for idx in host_idx_1:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        host_group1_list.append(x)\n",
    "\n",
    "    for idx in host_idx_2:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        host_group2_list.append(x)\n",
    "    # finally prepare training and vallidation data\n",
    "    cut = int(1 * len(group1_list))\n",
    "    train_samples = host_group1_list[:cut] + host_group2_list[:cut]\n",
    "    train_phenotypes = [0] * len(host_group1_list[:cut]) + [1] * len(host_group2_list[:cut])\n",
    "    print(train_phenotypes)\n",
    "    #balance the generation per party\n",
    "    if(len(train_phenotypes)<ceil(14/nhosts)):\n",
    "        #increase nsubset gradually to balance the local distribution \n",
    "        generate_data(train_samples, train_phenotypes, folder_path, ncell=200, nsubset=500,per_sample=False, verbose=0,generate_valid_set=False)\n",
    "    else:\n",
    "        generate_data(train_samples, train_phenotypes, folder_path, ncell=200, nsubset=500,per_sample=False, verbose=0,generate_valid_set=False)\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is to reproduce the CellCnn accuracies with the current generation of data (local, centralized etc) to reproduce the classification metrics with original architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1988\n",
      "200\n",
      "37\n",
      "Epoch 1/20\n",
      "10/10 - 1s - loss: 0.5150 - accuracy: 0.8023\n",
      "Epoch 2/20\n",
      "10/10 - 0s - loss: 0.0852 - accuracy: 0.9960\n",
      "Epoch 3/20\n",
      "10/10 - 0s - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "10/10 - 0s - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "10/10 - 0s - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "10/10 - 0s - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "10/10 - 0s - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "10/10 - 0s - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "10/10 - 0s - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "10/10 - 0s - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "10/10 - 0s - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "10/10 - 0s - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "10/10 - 0s - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "10/10 - 0s - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "10/10 - 0s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "10/10 - 0s - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "10/10 - 0s - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "10/10 - 0s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "10/10 - 0s - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "10/10 - 0s - loss: 0.0064 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#The reproduction of original CellCnn model training without validation test and further analysis part\n",
    "#This part is used for the comparison of accuracy/precision/recall/f-score of CellCnn with secure distributed version\n",
    "#test on 200-cell multi-instances and full test set phenotype prediction\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, initializers, regularizers, optimizers, callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "#repeat cellcnn original training on full train data\n",
    "\n",
    "k = 200\n",
    "ncell = 200\n",
    "nfilter = 8\n",
    "y_tr_n = to_categorical(y_tr)\n",
    "\n",
    "def pool_top_k(x, k):\n",
    "    return tf.reduce_mean(tf.sort(x, axis=1, direction='DESCENDING')[:, :k, :], axis=1)\n",
    "def create_model (k,ncell,nfilter):\n",
    "        \n",
    "        data_input = keras.Input(shape=(ncell, 37))\n",
    "        coeff_l1=0\n",
    "        coeff_l2=1e-4\n",
    "        n_classes=2\n",
    "        # the filters\n",
    "        conv = layers.Conv1D(filters=nfilter,\n",
    "                             kernel_size=1,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer=initializers.RandomUniform(),\n",
    "                             kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                             name='conv1')(data_input)\n",
    "\n",
    "        # the cell grouping part (top-k pooling)\n",
    "        pooled = layers.Lambda(pool_top_k, output_shape=(nfilter,), arguments={'k': k})(conv)\n",
    "        output = layers.Dense(units=n_classes,\n",
    "                                  activation='softmax',\n",
    "                                  kernel_initializer=initializers.RandomUniform(),\n",
    "                                  kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                                  name='output')(pooled)\n",
    "        model = keras.Model(inputs=data_input, outputs=output)\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=0.04),\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "        return model\n",
    "model = create_model(k,ncell,nfilter)\n",
    "#generate data\n",
    "\n",
    "x_tr_n = x_tr.transpose(0,2,1)\n",
    "# Fit data to model\n",
    "print(type(x_tr_n))\n",
    "print(len(x_tr_n))\n",
    "print(len(x_tr_n[0]))\n",
    "print(len(x_tr_n[1][0]))\n",
    "\n",
    "\n",
    "history = model.fit(x_tr_n, y_tr_n,\n",
    "            batch_size=200,\n",
    "            epochs=20,\n",
    "            verbose=2,\n",
    "            validation_split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 200-cell predictions on test set with size (2000, 200, 37)\n",
      "Test loss: 0.8170528411865234, Test accuracy: 0.8500000238418579\n",
      "Accuracy: 0.85\n",
      "F-score: 0.8678414096916299\n",
      "precision: 0.7755905511811023\n",
      "recall: 0.985\n"
     ]
    }
   ],
   "source": [
    "#For 200-cell predictions on test set\n",
    "def model_pred(prob):\n",
    "    pred = []\n",
    "    for p in prob:\n",
    "        if p[0]>p[1]:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    return pred\n",
    "\n",
    "x_test_n = x_test.transpose(0,2,1)\n",
    "y_test_n = to_categorical(y_test)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "#score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "print(\"For 200-cell predictions on test set with size\",x_test_n.shape)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "y_pred = model.predict(x_test_n)\n",
    "y_pred = model_pred(y_pred)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F-score:\",f1_score(y_test, y_pred))\n",
    "print(\"precision:\",precision_score(y_test, y_pred))\n",
    "print(\"recall:\",recall_score(y_test, y_pred)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "F-score: 0.8\n",
      "precision: 0.6666666666666666\n",
      "recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "#For phenotype predictions on test set using all cells \n",
    "\n",
    "model2 = create_model(5652, 5652,8)\n",
    "weights = model.get_weights()\n",
    "model2.set_weights(weights)\n",
    "data_test_n = data_test.transpose(0,2,1)\n",
    "phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "y_pred = model2.predict(data_test)\n",
    "\n",
    "y_pred = model_pred(y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "print(\"F-score:\",f1_score(phenotypes, y_pred))\n",
    "print(\"precision:\",precision_score(phenotypes, y_pred))\n",
    "print(\"recall:\",recall_score(phenotypes, y_pred)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
