{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "['CCR2', 'CCR4', 'CCR6', 'CCR7', 'CXCR4', 'CXCR5', 'CD103', 'CD14', 'CD20', 'CD25', 'CD27', 'CD28', 'CD3', 'CD4', 'CD45RA', 'CD45RO', 'CD56', 'CD57', 'CD69', 'CD8', 'TCRgd', 'PD.1', 'GM.CSF', 'IFN.g', 'IL.10', 'IL.13', 'IL.17A', 'IL.2', 'IL.21', 'IL.22', 'IL.3', 'IL.4', 'IL.6', 'IL.9', 'TNF.a', 'gate_source', 'manual_labels', 'labels', 'cell_id', 'cd4_labels', 'cd8_labels', 'run_0', 'run_1', 'run_2', 'intersection_3_runs']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, errno, glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cellCNN_utils  \n",
    "from cellCNN_utils import loadFCS, ftrans, mkdir_p, get_items, generate_data, generate_normalized_data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "d = Path().resolve()\n",
    "sys.path.append(d)\n",
    "%pylab inline\n",
    "# define input and output directories\n",
    "FCS_DATA_PATH = os.path.join(d, 'FlowRepository')\n",
    "# select the relevant markers for further analysis\n",
    "data_fcs = loadFCS(glob.glob(FCS_DATA_PATH + '/discovery_cohort.fcs')[0], transform=None, auto_comp=False)\n",
    "print(data_fcs.channels)\n",
    "#-------------------------------SET PARAMS HERE -----------------------#\n",
    "\n",
    "cg = 'RRMS' #Control group, set to NIND or RRMS depending on experimental setting\n",
    "ncells = 100 #num cells per multi-cell input or multi-cell test data\n",
    "ncell_test = 5000 #num cells for phenotype prediction\n",
    "ntrain_all = 30000 # number of multi-cell inputs for the train data (in total)\n",
    "ntest_all = 10000 # number of multi-cell inputs for the test data\n",
    "nfilter = 8 #number of filters for the training\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "\n",
    "markers=['CCR2', 'CCR4', 'CCR6', 'CCR7', 'CXCR4', 'CXCR5', 'CD103', 'CD14', 'CD20', \n",
    "        'CD25', 'CD27', 'CD28', 'CD3', 'CD4', 'CD45RA', 'CD45RO', 'CD56', 'CD57', 'CD69', 'CD8', \n",
    "        'TCRgd', 'PD.1', 'GM.CSF', 'IFN.g', 'IL.10', 'IL.13', 'IL.17A', 'IL.2', 'IL.21', 'IL.22', 'IL.3',\n",
    "        'IL.4', 'IL.6', 'IL.9', 'TNF.a']\n",
    "len(markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gate_source is 35.index\n",
    "#gate_source is ind=1 in excell, label is ind=4\n",
    "#match gate_source from table NINDC=0, RRMS=1\n",
    "metadata= pd.read_excel(FCS_DATA_PATH+'/meta_data_discovery_cohort.xlsx')\n",
    "metadata=metadata.to_numpy()\n",
    "gate_source = metadata[:,1]\n",
    "labelsTemp = metadata[:,4]\n",
    "data = []\n",
    "sample_labels =[]\n",
    "for i in range(99):\n",
    "    cur_gs = gate_source[i]\n",
    "    cur_lab = labelsTemp[i]\n",
    "    patient_sample = []\n",
    "    if cur_lab == 'HD':\n",
    "        gs_ind = np.where(data_fcs.events[:,35]==cur_gs)\n",
    "        for j in gs_ind[0]:\n",
    "            patient_sample.append(data_fcs.events[j,0:35])\n",
    "        sample_labels.append(0)\n",
    "    elif cur_lab == cg:\n",
    "        gs_ind = np.where(data_fcs.events[:,35]==cur_gs)\n",
    "        for j in gs_ind[0]:\n",
    "            patient_sample.append(data_fcs.events[j,0:35])\n",
    "        sample_labels.append(1)\n",
    "    if len(patient_sample)>0:\n",
    "        data.append(np.asarray(patient_sample))\n",
    "sample_labels=np.asarray(sample_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we randomly split the samples in training/test sets.\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, initializers, regularizers, optimizers, callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def train_test_split(train_idx1=[], train_idx2=[], test=True):\n",
    "\n",
    "    # split the fcs files into training, validation and test set (note that secure-protocols do not use validation sets)\n",
    "    group1 = np.where(sample_labels == 0)[0]\n",
    "    group2 = np.where(sample_labels == 1)[0]\n",
    "    l1, l2 = len(group1), len(group2)\n",
    "    ntrain_per_class = 24\n",
    "    ntest_group1 = l1 - ntrain_per_class\n",
    "    ntest_group2 = l2 - ntrain_per_class\n",
    "\n",
    "    # get the sample indices\n",
    "    train_idx1 = list(np.random.choice(group1, size=ntrain_per_class, replace=False))\n",
    "    test_idx1 = [i for i in group1 if i not in train_idx1]\n",
    "    train_idx2 = list(np.random.choice(group2, size=ntrain_per_class, replace=False))\n",
    "    test_idx2 = [i for i in group2 if i not in train_idx2]\n",
    "\n",
    "    print(\"test indices\")\n",
    "    test_indices = [test_idx1,test_idx2]\n",
    "    print(test_indices) \n",
    "    print(\"train indices\")\n",
    "    print(train_idx1)\n",
    "    print(train_idx2)\n",
    "\n",
    "    train_indices = [train_idx1,train_idx2]\n",
    "\n",
    "    # load the training samples\n",
    "    group1_list, group2_list = [], []\n",
    "    for idx in train_idx1:\n",
    "        x = data[idx][:]\n",
    "        group1_list.append(x)\n",
    "\n",
    "    for idx in train_idx2:\n",
    "        x = data[idx][:]\n",
    "        group2_list.append(x)\n",
    "\n",
    "    # load the test samples\n",
    "    t_group1_list, t_group2_list = [], []\n",
    "    test_phenotypes = []\n",
    "    for idx in test_idx1:\n",
    "        x = data[idx][:]\n",
    "        t_group1_list.append(x)\n",
    "        test_phenotypes.append(0)\n",
    "\n",
    "    for idx in test_idx2:\n",
    "        x = data[idx][:]\n",
    "        t_group2_list.append(x)\n",
    "        test_phenotypes.append(1)\n",
    "\n",
    "    # finally prepare training data\n",
    "    cut = int(1 * len(group1_list))\n",
    "    train_samples = group1_list[:cut] + group2_list[:cut]\n",
    "    train_phenotypes = [0] * len(group1_list[:cut]) + [1] * len(group2_list[:cut])\n",
    "    valid_samples = group1_list[cut:] + group2_list[cut:]\n",
    "    valid_phenotypes = [0] * len(group1_list[cut:]) + [1] * len(group2_list[cut:])\n",
    "    test_samples = t_group1_list + t_group2_list\n",
    "    print(test_phenotypes)\n",
    "    return train_samples,train_phenotypes,test_samples,test_phenotypes, test_indices,train_indices\n",
    "\n",
    "#generate also the test set on full min-ncell per sample:\n",
    "def generate_for_pheno_prediction(new_samples,phenotypes,scaler):\n",
    "        ncell_per_sample = np.min([x.shape[0] for x in new_samples])\n",
    "        print(f\"Predictions based on multi-cell inputs containing {ncell_per_sample} cells.\")\n",
    "        nmark = len(new_samples[0][1])\n",
    "        # scale the new samples if we did that for the training samples\n",
    "        if scaler is not None:\n",
    "            new_samples = [scaler.transform(x) for x in new_samples]\n",
    "        print(len(new_samples))\n",
    "        print(type(new_samples))\n",
    "        new_samples = [shuffle(x)[:ncell_per_sample].reshape(1, ncell_per_sample, nmark)\n",
    "                           for x in new_samples]\n",
    "        data_test = np.vstack(new_samples)\n",
    "        return data_test,phenotypes,ncell_per_sample\n",
    "def pool_top_k(x, k):\n",
    "    return tf.reduce_mean(tf.sort(x, axis=1, direction='DESCENDING')[:, :k, :], axis=1)\n",
    "def create_model (k,ncell,nfilter):\n",
    "        data_input = keras.Input(shape=(ncell, 35))\n",
    "        coeff_l1=0\n",
    "        coeff_l2=1e-4\n",
    "        n_classes=2\n",
    "        # the filters\n",
    "        conv = layers.Conv1D(filters=nfilter,\n",
    "                             kernel_size=1,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer=initializers.RandomUniform(),\n",
    "                             kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                             name='conv1')(data_input)\n",
    "\n",
    "        # the cell grouping part (top-k pooling)\n",
    "        pooled = layers.Lambda(pool_top_k, output_shape=(nfilter,), arguments={'k': k})(conv)\n",
    "        output = layers.Dense(units=n_classes,\n",
    "                                  activation='softmax',\n",
    "                                  kernel_initializer=initializers.RandomUniform(),\n",
    "                                  kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                                  name='output')(pooled)\n",
    "        model = keras.Model(inputs=data_input, outputs=output)\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "        return model\n",
    "def model_pred(prob):\n",
    "    pred = []\n",
    "    for p in prob:\n",
    "        if p[0]>p[1]:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    return pred\n",
    "def splitForLocal(nhosts,train_indices,test_indices):\n",
    "    test_idx1 = test_indices[0]\n",
    "    test_idx2 = test_indices[1]\n",
    "\n",
    "    train_idx1 = train_indices[0]\n",
    "    train_idx2 = train_indices[1]\n",
    "\n",
    "    print(\"Test set indices:\")\n",
    "    print(test_idx1)\n",
    "    print(test_idx2)\n",
    "    print(\"Global train set indices:\")\n",
    "\n",
    "    #to take the runs on 10 different distributions (for box plot)\n",
    "\n",
    "    print(train_idx1)\n",
    "    print(train_idx2)\n",
    "\n",
    "    #distribute train indices balanced among n hosts:\n",
    "    split_idx_1 = []\n",
    "    split_idx_2 = []\n",
    "    group1_list = np.flip(np.array_split(numpy.array(train_idx1), nhosts))\n",
    "    group2_list = numpy.array_split(numpy.array(train_idx2), nhosts)\n",
    "\n",
    "    for i in range(nhosts):\n",
    "        split_idx_1.append(group1_list[i].tolist())\n",
    "        split_idx_2.append(group2_list[i].tolist())\n",
    "\n",
    "    print(\"Global train splitted among hosts - indices:\")\n",
    "    print(split_idx_1)\n",
    "    print(split_idx_2)\n",
    "\n",
    "    xtr = []\n",
    "    ytr= []\n",
    "    for i in range(nhosts):\n",
    "        print(\"\\nHost no.\", i, \":\")\n",
    "        folder_path = 'splitFlow' + str(nhosts) + '/host' + str(i) + '/'\n",
    "        host_idx_1 = split_idx_1[i]\n",
    "        host_idx_2 = split_idx_2[i]\n",
    "        print(\"host_idx_1:\", host_idx_1, \"- host_idx_2:\", host_idx_2)\n",
    "         # load the training samples\n",
    "        host_group1_list, host_group2_list = [], []\n",
    "        train_samples,train_phenotypes = [],[]\n",
    "        for idx in host_idx_1:\n",
    "            x = data[idx][:]\n",
    "            host_group1_list.append(x)\n",
    "\n",
    "        for idx in host_idx_2:\n",
    "            x = data[idx][:]\n",
    "            host_group2_list.append(x)\n",
    "\n",
    "        # finally prepare training and vallidation data\n",
    "        cut = int(1 * len(host_group1_list))\n",
    "        train_samples = host_group1_list[:cut] + host_group2_list[:cut]\n",
    "        train_phenotypes = [0] * len(host_group1_list[:cut]) + [1] * len(host_group2_list[:cut])\n",
    "        print(train_phenotypes)\n",
    "        scaler,x_tr,y_tr = generate_data(train_samples, train_phenotypes, folder_path, scale=True, ncell=ncells, \n",
    "                      nsubset=int(ntrain_all/2),per_sample=False, verbose=0,generate_valid_set=False,\n",
    "                                         saveFile=False,scaler=None,subset_selection = 'random',oneFile=None)\n",
    "        xtr.append(x_tr)\n",
    "        ytr.append(y_tr)\n",
    "    return xtr,ytr,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test indices\n",
      "[[1, 17, 21, 27, 49], [2, 5, 7, 11, 16, 33, 41]]\n",
      "train indices\n",
      "[48, 45, 54, 31, 20, 26, 58, 0, 44, 53, 50, 51, 56, 35, 18, 19, 52, 57, 36, 10, 30, 55, 15, 43]\n",
      "[34, 47, 23, 24, 13, 46, 8, 14, 39, 22, 25, 42, 32, 12, 6, 38, 37, 28, 29, 4, 9, 59, 40, 3]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "11663\n",
      "Predictions based on multi-cell inputs containing 3934 cells.\n",
      "12\n",
      "<class 'list'>\n",
      "Epoch 1/30\n",
      "469/469 - 2s - loss: 0.6923 - accuracy: 0.5182\n",
      "Epoch 2/30\n",
      "469/469 - 1s - loss: 0.6836 - accuracy: 0.5918\n",
      "Epoch 3/30\n",
      "469/469 - 2s - loss: 0.6628 - accuracy: 0.6186\n",
      "Epoch 4/30\n",
      "469/469 - 2s - loss: 0.6401 - accuracy: 0.6428\n",
      "Epoch 5/30\n",
      "469/469 - 2s - loss: 0.6207 - accuracy: 0.6565\n",
      "Epoch 6/30\n",
      "469/469 - 2s - loss: 0.6041 - accuracy: 0.6682\n",
      "Epoch 7/30\n",
      "469/469 - 2s - loss: 0.5894 - accuracy: 0.6774\n",
      "Epoch 8/30\n",
      "469/469 - 2s - loss: 0.5758 - accuracy: 0.6917\n",
      "Epoch 9/30\n",
      "469/469 - 2s - loss: 0.5629 - accuracy: 0.7081\n",
      "Epoch 10/30\n",
      "469/469 - 2s - loss: 0.5506 - accuracy: 0.7229\n",
      "Epoch 11/30\n",
      "469/469 - 2s - loss: 0.5387 - accuracy: 0.7379\n",
      "Epoch 12/30\n",
      "469/469 - 2s - loss: 0.5274 - accuracy: 0.7502\n",
      "Epoch 13/30\n",
      "469/469 - 2s - loss: 0.5166 - accuracy: 0.7613\n",
      "Epoch 14/30\n",
      "469/469 - 2s - loss: 0.5064 - accuracy: 0.7707\n",
      "Epoch 15/30\n",
      "469/469 - 2s - loss: 0.4968 - accuracy: 0.7803\n",
      "Epoch 16/30\n",
      "469/469 - 2s - loss: 0.4879 - accuracy: 0.7875\n",
      "Epoch 17/30\n",
      "469/469 - 2s - loss: 0.4797 - accuracy: 0.7947\n",
      "Epoch 18/30\n",
      "469/469 - 2s - loss: 0.4720 - accuracy: 0.7997\n",
      "Epoch 19/30\n",
      "469/469 - 2s - loss: 0.4650 - accuracy: 0.8040\n",
      "Epoch 20/30\n",
      "469/469 - 2s - loss: 0.4584 - accuracy: 0.8089\n",
      "Epoch 21/30\n",
      "469/469 - 3s - loss: 0.4523 - accuracy: 0.8111\n",
      "Epoch 22/30\n",
      "469/469 - 3s - loss: 0.4466 - accuracy: 0.8153\n",
      "Epoch 23/30\n",
      "469/469 - 3s - loss: 0.4413 - accuracy: 0.8169\n",
      "Epoch 24/30\n",
      "469/469 - 2s - loss: 0.4364 - accuracy: 0.8196\n",
      "Epoch 25/30\n",
      "469/469 - 2s - loss: 0.4318 - accuracy: 0.8220\n",
      "Epoch 26/30\n",
      "469/469 - 2s - loss: 0.4274 - accuracy: 0.8235\n",
      "Epoch 27/30\n",
      "469/469 - 2s - loss: 0.4233 - accuracy: 0.8247\n",
      "Epoch 28/30\n",
      "469/469 - 2s - loss: 0.4194 - accuracy: 0.8265\n",
      "Epoch 29/30\n",
      "469/469 - 2s - loss: 0.4157 - accuracy: 0.8278\n",
      "Epoch 30/30\n",
      "469/469 - 2s - loss: 0.4121 - accuracy: 0.8300\n",
      "For 100-cell predictions on test set with size (9998, 100, 35)\n",
      "Test loss: 0.6697097420692444, Test accuracy: 0.6559311747550964\n",
      "Accuracy: 0.6559311862372474\n",
      "F-score: 0.6525252525252526\n",
      "precision: 0.6589147286821705\n",
      "recall: 0.6462585034013606\n",
      "Accuracy: 0.5833333333333334\n",
      "F-score: 0.6153846153846153\n",
      "precision: 0.6666666666666666\n",
      "recall: 0.5714285714285714\n",
      "test indices\n",
      "[[17, 36, 53, 55, 57], [4, 14, 16, 32, 41, 42, 47]]\n",
      "train indices\n",
      "[44, 45, 56, 15, 21, 18, 30, 58, 27, 20, 50, 26, 31, 1, 35, 54, 49, 0, 43, 10, 19, 48, 51, 52]\n",
      "[24, 37, 13, 29, 6, 25, 40, 59, 34, 23, 38, 12, 8, 5, 3, 39, 9, 46, 7, 28, 11, 2, 22, 33]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "28851\n",
      "Predictions based on multi-cell inputs containing 5968 cells.\n",
      "12\n",
      "<class 'list'>\n",
      "Epoch 1/30\n",
      "469/469 - 2s - loss: 0.6919 - accuracy: 0.5024\n",
      "Epoch 2/30\n",
      "469/469 - 2s - loss: 0.6825 - accuracy: 0.5546\n",
      "Epoch 3/30\n",
      "469/469 - 2s - loss: 0.6660 - accuracy: 0.6313\n",
      "Epoch 4/30\n",
      "469/469 - 2s - loss: 0.6482 - accuracy: 0.6469\n",
      "Epoch 5/30\n",
      "469/469 - 2s - loss: 0.6309 - accuracy: 0.6487\n",
      "Epoch 6/30\n",
      "469/469 - 2s - loss: 0.6146 - accuracy: 0.6612\n",
      "Epoch 7/30\n",
      "469/469 - 2s - loss: 0.5991 - accuracy: 0.6754\n",
      "Epoch 8/30\n",
      "469/469 - 2s - loss: 0.5846 - accuracy: 0.6916\n",
      "Epoch 9/30\n",
      "469/469 - 2s - loss: 0.5709 - accuracy: 0.7063\n",
      "Epoch 10/30\n",
      "469/469 - 2s - loss: 0.5582 - accuracy: 0.7198\n",
      "Epoch 11/30\n",
      "469/469 - 2s - loss: 0.5463 - accuracy: 0.7318\n",
      "Epoch 12/30\n",
      "469/469 - 2s - loss: 0.5352 - accuracy: 0.7412\n",
      "Epoch 13/30\n",
      "469/469 - 2s - loss: 0.5249 - accuracy: 0.7495\n",
      "Epoch 14/30\n",
      "469/469 - 1s - loss: 0.5152 - accuracy: 0.7565\n",
      "Epoch 15/30\n",
      "469/469 - 1s - loss: 0.5061 - accuracy: 0.7643\n",
      "Epoch 16/30\n",
      "469/469 - 1s - loss: 0.4976 - accuracy: 0.7709\n",
      "Epoch 17/30\n",
      "469/469 - 1s - loss: 0.4898 - accuracy: 0.7763\n",
      "Epoch 18/30\n",
      "469/469 - 1s - loss: 0.4824 - accuracy: 0.7810\n",
      "Epoch 19/30\n",
      "469/469 - 1s - loss: 0.4756 - accuracy: 0.7862\n",
      "Epoch 20/30\n",
      "469/469 - 1s - loss: 0.4693 - accuracy: 0.7902\n",
      "Epoch 21/30\n",
      "469/469 - 1s - loss: 0.4634 - accuracy: 0.7939\n",
      "Epoch 22/30\n",
      "469/469 - 1s - loss: 0.4578 - accuracy: 0.7966\n",
      "Epoch 23/30\n",
      "469/469 - 2s - loss: 0.4527 - accuracy: 0.8002\n",
      "Epoch 24/30\n",
      "469/469 - 2s - loss: 0.4479 - accuracy: 0.8029\n",
      "Epoch 25/30\n",
      "469/469 - 2s - loss: 0.4433 - accuracy: 0.8051\n",
      "Epoch 26/30\n",
      "469/469 - 2s - loss: 0.4391 - accuracy: 0.8076\n",
      "Epoch 27/30\n",
      "469/469 - 2s - loss: 0.4351 - accuracy: 0.8096\n",
      "Epoch 28/30\n",
      "469/469 - 2s - loss: 0.4313 - accuracy: 0.8111\n",
      "Epoch 29/30\n",
      "469/469 - 2s - loss: 0.4278 - accuracy: 0.8132\n",
      "Epoch 30/30\n",
      "469/469 - 2s - loss: 0.4245 - accuracy: 0.8151\n",
      "For 100-cell predictions on test set with size (9998, 100, 35)\n",
      "Test loss: 0.765336811542511, Test accuracy: 0.6229245662689209\n",
      "Accuracy: 0.6229245849169834\n",
      "F-score: 0.5796164139161463\n",
      "precision: 0.6546599496221662\n",
      "recall: 0.5200080032012805\n",
      "Accuracy: 0.6666666666666666\n",
      "F-score: 0.6666666666666666\n",
      "precision: 0.8\n",
      "recall: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "accurMulti = []\n",
    "precMulti = []\n",
    "recallMulti = []\n",
    "fscoreMulti = []\n",
    "accurPheno = []\n",
    "precPheno = []\n",
    "recallPheno = []\n",
    "fscorePheno = []\n",
    "for run in range(10):\n",
    "    train_samples, train_phenotypes, test_samples, test_phenotypes, test_indices, train_indices = train_test_split()\n",
    "    # generate ntrain_all (ntrain_all/2 per phenotype) training samples for centralized test!\n",
    "\n",
    "    scaler,x_tr,y_tr = generate_data(train_samples, train_phenotypes, 'Flow/', generate_valid_set=False, \n",
    "                                                   ncell=ncells, nsubset=int(ntrain_all/2), scale=True, \n",
    "                                                   per_sample=False, verbose=0, saveFile=False,subset_selection = 'random')\n",
    "\n",
    "\n",
    "    scaler,x_test,y_test = generate_data(test_samples, test_phenotypes, 'Flow/', generate_valid_set=False, \n",
    "                                                   ncell=ncells, nsubset=5000, scale=True, \n",
    "                                                   per_sample=False, verbose=0, saveFile=False,\n",
    "                                                   subset_selection = 'random', generateAsTest=True)\n",
    "    print(len(test_samples[0]))\n",
    "    data_test,phenotypes,ncell_per_sample = generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "\n",
    "    y_tr_n = to_categorical(y_tr)\n",
    "\n",
    "\n",
    "    model = create_model(ncells,ncells,nfilter)\n",
    "    #generate data\n",
    "\n",
    "    x_tr_n = x_tr.transpose(0,2,1)\n",
    "    # Fit data to model\n",
    "    history = model.fit(x_tr_n, y_tr_n,\n",
    "                batch_size=64,\n",
    "                epochs=30,\n",
    "                verbose=2,\n",
    "                validation_split=0)\n",
    "    #For 100-cell predictions on test set\n",
    "\n",
    "    \n",
    "    #test for multi-cell\n",
    "    x_test_n = x_test.transpose(0,2,1)\n",
    "    x_test_n = x_test_n[0:ntest_all,:]\n",
    "\n",
    "    y_test_n = to_categorical(y_test)\n",
    "    y_test_n = y_test_n[0:ntest_all,:]\n",
    "    loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "    #score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "    print(\"For 100-cell predictions on test set with size\",x_test_n.shape)\n",
    "    print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "    y_pred = model.predict(x_test_n)\n",
    "    y_pred = model_pred(y_pred)\n",
    "    y_test= y_test[0:ntest_all]\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"F-score:\",f1_score(y_test, y_pred))\n",
    "    print(\"precision:\",precision_score(y_test, y_pred))\n",
    "    print(\"recall:\",recall_score(y_test, y_pred)) \n",
    "    \n",
    "    #to write to excell sheet...\n",
    "    accurMulti.append(accuracy_score(y_test, y_pred))\n",
    "    precMulti.append(precision_score(y_test, y_pred))\n",
    "    recallMulti.append(recall_score(y_test, y_pred)) \n",
    "    fscoreMulti.append(f1_score(y_test, y_pred))\n",
    "    #For phenotype predictions on test set using all cells \n",
    "\n",
    "    model2 = create_model(ncell_per_sample, ncell_per_sample,nfilter)\n",
    "    weights = model.get_weights()\n",
    "    model2.set_weights(weights)\n",
    "    data_test_n = data_test\n",
    "    phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "    y_pred = model2.predict(data_test_n)\n",
    "    y_pred = model_pred(y_pred)\n",
    "    # print(y_pred)\n",
    "    print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "    print(\"F-score:\",f1_score(phenotypes, y_pred))\n",
    "    print(\"precision:\",precision_score(phenotypes, y_pred))\n",
    "    print(\"recall:\",recall_score(phenotypes, y_pred)) \n",
    "    \n",
    "    accurPheno.append(accuracy_score(phenotypes, y_pred))\n",
    "    precPheno.append(precision_score(phenotypes, y_pred))\n",
    "    recallPheno.append(recall_score(phenotypes, y_pred)) \n",
    "    fscorePheno.append(f1_score(phenotypes, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To average LOCAL computations metrics\n",
    "nhosts = 2\n",
    "totalRun = 2\n",
    "accurMulti = np.empty([nhosts, totalRun])\n",
    "precMulti = np.empty([nhosts, totalRun])\n",
    "recallMulti = np.empty([nhosts, totalRun])\n",
    "fscoreMulti = np.empty([nhosts, totalRun])\n",
    "accurPheno = np.empty([nhosts, totalRun])\n",
    "precPheno = np.empty([nhosts, totalRun])\n",
    "recallPheno = np.empty([nhosts, totalRun])\n",
    "fscorePheno = np.empty([nhosts, totalRun])\n",
    "\n",
    "for run in range(totalRun):\n",
    "    train_samples, train_phenotypes, test_samples, test_phenotypes, test_indices, train_indices = train_test_split()\n",
    "    \n",
    "    #split between n host\n",
    "    xtr,ytr,scaler = splitForLocal(nhosts,train_indices, test_indices)\n",
    "    \n",
    "    scaler,x_test,y_test = generate_data(test_samples, test_phenotypes, 'Flow/', generate_valid_set=False, \n",
    "                                                   ncell=ncells, nsubset=5000, scale=True, \n",
    "                                                   per_sample=False, verbose=0, saveFile=False,\n",
    "                                                   subset_selection = 'random', generateAsTest=True,scaler=scaler)\n",
    "    \n",
    "    data_test,phenotypes,ncell_per_sample = generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "\n",
    "\n",
    "    for i in range(nhosts):\n",
    "        y_tr_n = to_categorical(ytr[i])\n",
    "        model = create_model(ncells,ncells,nfilter)\n",
    "        #generate data\n",
    "\n",
    "        x_tr_n = xtr[i].transpose(0,2,1)\n",
    "        # Fit data to model\n",
    "        history = model.fit(x_tr_n, y_tr_n,\n",
    "                    batch_size=64,\n",
    "                    epochs=30,\n",
    "                    verbose=2,\n",
    "                    validation_split=0)\n",
    "\n",
    "        #test for multi-cell\n",
    "        x_test_n = x_test.transpose(0,2,1)\n",
    "        x_test_n = x_test_n[0:ntest_all,:]\n",
    "\n",
    "        y_test_n = to_categorical(y_test)\n",
    "        y_test_n = y_test_n[0:ntest_all,:]\n",
    "        loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "        #score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "        print(\"For 100-cell predictions on test set with size\",x_test_n.shape)\n",
    "        print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "        y_pred = model.predict(x_test_n)\n",
    "        y_pred = model_pred(y_pred)\n",
    "        y_test= y_test[0:ntest_all]\n",
    "\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"F-score:\",f1_score(y_test, y_pred))\n",
    "        print(\"precision:\",precision_score(y_test, y_pred))\n",
    "        print(\"recall:\",recall_score(y_test, y_pred)) \n",
    "\n",
    "        #to write to excell sheet...\n",
    "        accurMulti[i,run] = accuracy_score(y_test, y_pred)\n",
    "        precMulti[i,run] = precision_score(y_test, y_pred)\n",
    "        recallMulti[i,run]= recall_score(y_test, y_pred)\n",
    "        fscoreMulti[i,run]= f1_score(y_test, y_pred)\n",
    "        #For phenotype predictions on test set using all cells \n",
    "\n",
    "        model2 = create_model(ncell_per_sample, ncell_per_sample,nfilter)\n",
    "        weights = model.get_weights()\n",
    "        model2.set_weights(weights)\n",
    "        data_test_n = data_test\n",
    "        phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "        y_pred = model2.predict(data_test_n)\n",
    "        y_pred = model_pred(y_pred)\n",
    "        # print(y_pred)\n",
    "        print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "        print(\"F-score:\",f1_score(phenotypes, y_pred))\n",
    "        print(\"precision:\",precision_score(phenotypes, y_pred))\n",
    "        print(\"recall:\",recall_score(phenotypes, y_pred)) \n",
    "        \n",
    "        accurPheno[i,run] = accuracy_score(phenotypes, y_pred)\n",
    "        precPheno[i,run] = precision_score(phenotypes, y_pred)\n",
    "        recallPheno[i,run]= recall_score(phenotypes, y_pred)\n",
    "        fscorePheno[i,run]= f1_score(phenotypes, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy\n",
    "\n",
    "with open('myfile.csv', 'w', newline='') as file:\n",
    "    mywriter = csv.writer(file, delimiter=',')\n",
    "    phen = np.sum(recallPheno[:,:],axis=0)/nhosts\n",
    "    multi = np.sum(recallMulti[:,:],axis=0)/nhosts\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(phen)))\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(multi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy\n",
    "\n",
    "with open('myfile.csv', 'w', newline='') as file:\n",
    "    mywriter = csv.writer(file, delimiter=',')\n",
    "    phenR = np.sum(recallPheno[:,:],axis=0)/nhosts\n",
    "    multiR = np.sum(recallMulti[:,:],axis=0)/nhosts\n",
    "        \n",
    "    phenP = np.sum(precPheno[:,:],axis=0)/nhosts\n",
    "    multiP = np.sum(precMulti[:,:],axis=0)/nhosts\n",
    "    \n",
    "    fscoreAvPhen = 2*phenR*phenP/(phenP+phenR)\n",
    "    fscoreAvMulti = 2*multiR*multiP/(multiP+multiR)\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(fscoreAvPhen)))\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(fscoreAvMulti)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy\n",
    "\n",
    "with open('myfile.csv', 'w', newline='') as file:\n",
    "    mywriter = csv.writer(file, delimiter=',')\n",
    "    recallPheno = np.asarray(recallPheno)\n",
    "    recallMulti = np.asarray(recallMulti)\n",
    "#     mywriter.writerows(map(lambda x: [x], np.asarray(recallPheno)))\n",
    "#     mywriter.writerows(map(lambda x: [x], np.asarray(recallMulti)))\n",
    "    fscoreAvPhen = 2*recallPheno*precPheno/(precPheno+recallPheno)\n",
    "    fscoreAvMulti = 2*recallMulti*precMulti/(precMulti+recallMulti)\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(fscoreAvPhen)))\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(fscoreAvMulti)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.5833333333333334,\n",
       " 0.6666666666666666,\n",
       " 0.6666666666666666,\n",
       " 0.5833333333333334,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accurPheno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(accurPheno[:,:],axis=0)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
