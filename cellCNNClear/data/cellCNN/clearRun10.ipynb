{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "['CCR2', 'CCR4', 'CCR6', 'CCR7', 'CXCR4', 'CXCR5', 'CD103', 'CD14', 'CD20', 'CD25', 'CD27', 'CD28', 'CD3', 'CD4', 'CD45RA', 'CD45RO', 'CD56', 'CD57', 'CD69', 'CD8', 'TCRgd', 'PD.1', 'GM.CSF', 'IFN.g', 'IL.10', 'IL.13', 'IL.17A', 'IL.2', 'IL.21', 'IL.22', 'IL.3', 'IL.4', 'IL.6', 'IL.9', 'TNF.a', 'gate_source', 'manual_labels', 'labels', 'cell_id', 'cd4_labels', 'cd8_labels', 'run_0', 'run_1', 'run_2', 'intersection_3_runs']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, errno, glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cellCNN_utils  \n",
    "from cellCNN_utils import loadFCS, ftrans, mkdir_p, get_items, generate_data, generate_normalized_data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "d = Path().resolve()\n",
    "sys.path.append(d)\n",
    "%pylab inline\n",
    "# define input and output directories\n",
    "FCS_DATA_PATH = os.path.join(d, 'FlowRepository')\n",
    "# select the relevant markers for further analysis\n",
    "data_fcs = loadFCS(glob.glob(FCS_DATA_PATH + '/discovery_cohort.fcs')[0], transform=None, auto_comp=False)\n",
    "print(data_fcs.channels)\n",
    "#-------------------------------SET PARAMS HERE -----------------------#\n",
    "\n",
    "cg = 'RRMS' #Control group, set to NIND or RRMS depending on experimental setting\n",
    "ncells = 100 #num cells per multi-cell input or multi-cell test data\n",
    "ncell_test = 5000 #num cells for phenotype prediction\n",
    "ntrain_all = 30000 # number of multi-cell inputs for the train data (in total)\n",
    "ntest_all = 10000 # number of multi-cell inputs for the test data\n",
    "nfilter = 8 #number of filters for the training\n",
    "\n",
    "#-----------------------------------------------------------------------#\n",
    "\n",
    "markers=['CCR2', 'CCR4', 'CCR6', 'CCR7', 'CXCR4', 'CXCR5', 'CD103', 'CD14', 'CD20', \n",
    "        'CD25', 'CD27', 'CD28', 'CD3', 'CD4', 'CD45RA', 'CD45RO', 'CD56', 'CD57', 'CD69', 'CD8', \n",
    "        'TCRgd', 'PD.1', 'GM.CSF', 'IFN.g', 'IL.10', 'IL.13', 'IL.17A', 'IL.2', 'IL.21', 'IL.22', 'IL.3',\n",
    "        'IL.4', 'IL.6', 'IL.9', 'TNF.a']\n",
    "len(markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gate_source is 35.index\n",
    "#gate_source is ind=1 in excell, label is ind=4\n",
    "#match gate_source from table NINDC=0, RRMS=1\n",
    "metadata= pd.read_excel(FCS_DATA_PATH+'/meta_data_discovery_cohort.xlsx')\n",
    "metadata=metadata.to_numpy()\n",
    "gate_source = metadata[:,1]\n",
    "labelsTemp = metadata[:,4]\n",
    "data = []\n",
    "sample_labels =[]\n",
    "for i in range(99):\n",
    "    cur_gs = gate_source[i]\n",
    "    cur_lab = labelsTemp[i]\n",
    "    patient_sample = []\n",
    "    if cur_lab == 'HD':\n",
    "        gs_ind = np.where(data_fcs.events[:,35]==cur_gs)\n",
    "        for j in gs_ind[0]:\n",
    "            patient_sample.append(data_fcs.events[j,0:35])\n",
    "        sample_labels.append(0)\n",
    "    elif cur_lab == cg:\n",
    "        gs_ind = np.where(data_fcs.events[:,35]==cur_gs)\n",
    "        for j in gs_ind[0]:\n",
    "            patient_sample.append(data_fcs.events[j,0:35])\n",
    "        sample_labels.append(1)\n",
    "    if len(patient_sample)>0:\n",
    "        data.append(np.asarray(patient_sample))\n",
    "sample_labels=np.asarray(sample_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Here we randomly split the samples in training/test sets.\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, initializers, regularizers, optimizers, callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def train_test_split(train_idx1=[], train_idx2=[], test=True):\n",
    "\n",
    "    # split the fcs files into training, validation and test set (note that secure-protocols do not use validation sets)\n",
    "    group1 = np.where(sample_labels == 0)[0]\n",
    "    group2 = np.where(sample_labels == 1)[0]\n",
    "    l1, l2 = len(group1), len(group2)\n",
    "    ntrain_per_class = 24\n",
    "    ntest_group1 = l1 - ntrain_per_class\n",
    "    ntest_group2 = l2 - ntrain_per_class\n",
    "\n",
    "    # get the sample indices\n",
    "    train_idx1 = list(np.random.choice(group1, size=ntrain_per_class, replace=False))\n",
    "    test_idx1 = [i for i in group1 if i not in train_idx1]\n",
    "    train_idx2 = list(np.random.choice(group2, size=ntrain_per_class, replace=False))\n",
    "    test_idx2 = [i for i in group2 if i not in train_idx2]\n",
    "\n",
    "    print(\"test indices\")\n",
    "    test_indices = [test_idx1,test_idx2]\n",
    "    print(test_indices) \n",
    "    print(\"train indices\")\n",
    "    print(train_idx1)\n",
    "    print(train_idx2)\n",
    "\n",
    "    train_indices = [train_idx1,train_idx2]\n",
    "\n",
    "    # load the training samples\n",
    "    group1_list, group2_list = [], []\n",
    "    for idx in train_idx1:\n",
    "        x = data[idx][:]\n",
    "        group1_list.append(x)\n",
    "\n",
    "    for idx in train_idx2:\n",
    "        x = data[idx][:]\n",
    "        group2_list.append(x)\n",
    "\n",
    "    # load the test samples\n",
    "    t_group1_list, t_group2_list = [], []\n",
    "    test_phenotypes = []\n",
    "    for idx in test_idx1:\n",
    "        x = data[idx][:]\n",
    "        t_group1_list.append(x)\n",
    "        test_phenotypes.append(0)\n",
    "\n",
    "    for idx in test_idx2:\n",
    "        x = data[idx][:]\n",
    "        t_group2_list.append(x)\n",
    "        test_phenotypes.append(1)\n",
    "\n",
    "    # finally prepare training data\n",
    "    cut = int(1 * len(group1_list))\n",
    "    train_samples = group1_list[:cut] + group2_list[:cut]\n",
    "    train_phenotypes = [0] * len(group1_list[:cut]) + [1] * len(group2_list[:cut])\n",
    "    valid_samples = group1_list[cut:] + group2_list[cut:]\n",
    "    valid_phenotypes = [0] * len(group1_list[cut:]) + [1] * len(group2_list[cut:])\n",
    "    test_samples = t_group1_list + t_group2_list\n",
    "    print(test_phenotypes)\n",
    "    return train_samples,train_phenotypes,test_samples,test_phenotypes, test_indices,train_indices\n",
    "\n",
    "#generate also the test set on full min-ncell per sample:\n",
    "def generate_for_pheno_prediction(new_samples,phenotypes,scaler):\n",
    "        ncell_per_sample = np.min([x.shape[0] for x in new_samples])\n",
    "        print(f\"Predictions based on multi-cell inputs containing {ncell_per_sample} cells.\")\n",
    "        nmark = len(new_samples[0][1])\n",
    "        # scale the new samples if we did that for the training samples\n",
    "        if scaler is not None:\n",
    "            new_samples = [scaler.transform(x) for x in new_samples]\n",
    "        print(len(new_samples))\n",
    "        print(type(new_samples))\n",
    "        new_samples = [shuffle(x)[:ncell_per_sample].reshape(1, ncell_per_sample, nmark)\n",
    "                           for x in new_samples]\n",
    "        data_test = np.vstack(new_samples)\n",
    "        return data_test,phenotypes,ncell_per_sample\n",
    "def pool_top_k(x, k):\n",
    "    return tf.reduce_mean(tf.sort(x, axis=1, direction='DESCENDING')[:, :k, :], axis=1)\n",
    "def create_model (k,ncell,nfilter):\n",
    "        data_input = keras.Input(shape=(ncell, 35))\n",
    "        coeff_l1=0\n",
    "        coeff_l2=1e-4\n",
    "        n_classes=2\n",
    "        # the filters\n",
    "        conv = layers.Conv1D(filters=nfilter,\n",
    "                             kernel_size=1,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer=initializers.RandomUniform(),\n",
    "                             kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                             name='conv1')(data_input)\n",
    "\n",
    "        # the cell grouping part (top-k pooling)\n",
    "        pooled = layers.Lambda(pool_top_k, output_shape=(nfilter,), arguments={'k': k})(conv)\n",
    "        output = layers.Dense(units=n_classes,\n",
    "                                  activation='softmax',\n",
    "                                  kernel_initializer=initializers.RandomUniform(),\n",
    "                                  kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                                  name='output')(pooled)\n",
    "        model = keras.Model(inputs=data_input, outputs=output)\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "        return model\n",
    "def model_pred(prob):\n",
    "    pred = []\n",
    "    for p in prob:\n",
    "        if p[0]>p[1]:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    return pred\n",
    "def splitForLocal(nhosts,train_indices,test_indices):\n",
    "    test_idx1 = test_indices[0]\n",
    "    test_idx2 = test_indices[1]\n",
    "\n",
    "    train_idx1 = train_indices[0]\n",
    "    train_idx2 = train_indices[1]\n",
    "\n",
    "    print(\"Test set indices:\")\n",
    "    print(test_idx1)\n",
    "    print(test_idx2)\n",
    "    print(\"Global train set indices:\")\n",
    "\n",
    "    #to take the runs on 10 different distributions (for box plot)\n",
    "\n",
    "    print(train_idx1)\n",
    "    print(train_idx2)\n",
    "\n",
    "    #distribute train indices balanced among n hosts:\n",
    "    split_idx_1 = []\n",
    "    split_idx_2 = []\n",
    "    group1_list = np.flip(np.array_split(numpy.array(train_idx1), nhosts))\n",
    "    group2_list = numpy.array_split(numpy.array(train_idx2), nhosts)\n",
    "\n",
    "    for i in range(nhosts):\n",
    "        split_idx_1.append(group1_list[i].tolist())\n",
    "        split_idx_2.append(group2_list[i].tolist())\n",
    "\n",
    "    print(\"Global train splitted among hosts - indices:\")\n",
    "    print(split_idx_1)\n",
    "    print(split_idx_2)\n",
    "\n",
    "    xtr = []\n",
    "    ytr= []\n",
    "    for i in range(nhosts):\n",
    "        print(\"\\nHost no.\", i, \":\")\n",
    "        folder_path = 'splitFlow' + str(nhosts) + '/host' + str(i) + '/'\n",
    "        host_idx_1 = split_idx_1[i]\n",
    "        host_idx_2 = split_idx_2[i]\n",
    "        print(\"host_idx_1:\", host_idx_1, \"- host_idx_2:\", host_idx_2)\n",
    "         # load the training samples\n",
    "        host_group1_list, host_group2_list = [], []\n",
    "        train_samples,train_phenotypes = [],[]\n",
    "        for idx in host_idx_1:\n",
    "            x = data[idx][:]\n",
    "            host_group1_list.append(x)\n",
    "\n",
    "        for idx in host_idx_2:\n",
    "            x = data[idx][:]\n",
    "            host_group2_list.append(x)\n",
    "\n",
    "        # finally prepare training and vallidation data\n",
    "        cut = int(1 * len(host_group1_list))\n",
    "        train_samples = host_group1_list[:cut] + host_group2_list[:cut]\n",
    "        train_phenotypes = [0] * len(host_group1_list[:cut]) + [1] * len(host_group2_list[:cut])\n",
    "        print(train_phenotypes)\n",
    "        scaler,x_tr,y_tr = generate_data(train_samples, train_phenotypes, folder_path, scale=True, ncell=ncells, \n",
    "                      nsubset=int(ntrain_all/2),per_sample=False, verbose=0,generate_valid_set=False,\n",
    "                                         saveFile=False,scaler=None,subset_selection = 'random',oneFile=None)\n",
    "        xtr.append(x_tr)\n",
    "        ytr.append(y_tr)\n",
    "    return xtr,ytr,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test indices\n",
      "[[0, 21, 31, 44, 57], [2, 4, 13, 24, 38, 42, 47]]\n",
      "train indices\n",
      "[53, 55, 1, 35, 48, 50, 19, 20, 36, 10, 54, 49, 51, 43, 18, 58, 52, 30, 26, 45, 56, 17, 27, 15]\n",
      "[46, 11, 3, 16, 14, 28, 8, 33, 29, 22, 9, 32, 41, 23, 39, 6, 40, 7, 37, 59, 12, 34, 25, 5]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "7172\n",
      "Predictions based on multi-cell inputs containing 3247 cells.\n",
      "12\n",
      "<class 'list'>\n",
      "Epoch 1/30\n",
      "469/469 - 2s - loss: 0.4017 - accuracy: 0.8156\n",
      "Epoch 2/30\n",
      "469/469 - 2s - loss: 0.2879 - accuracy: 0.8815\n",
      "Epoch 3/30\n",
      "469/469 - 2s - loss: 0.2497 - accuracy: 0.9032\n",
      "Epoch 4/30\n",
      "469/469 - 2s - loss: 0.2282 - accuracy: 0.9134\n",
      "Epoch 5/30\n",
      "469/469 - 2s - loss: 0.2197 - accuracy: 0.9195\n",
      "Epoch 6/30\n",
      "469/469 - 2s - loss: 0.2098 - accuracy: 0.9255\n",
      "Epoch 7/30\n",
      "469/469 - 2s - loss: 0.2057 - accuracy: 0.9273\n",
      "Epoch 8/30\n",
      "469/469 - 2s - loss: 0.1976 - accuracy: 0.9306\n",
      "Epoch 9/30\n",
      "469/469 - 3s - loss: 0.1935 - accuracy: 0.9324\n",
      "Epoch 10/30\n",
      "469/469 - 2s - loss: 0.1893 - accuracy: 0.9342\n",
      "Epoch 11/30\n",
      "469/469 - 2s - loss: 0.1870 - accuracy: 0.9359\n",
      "Epoch 12/30\n",
      "469/469 - 2s - loss: 0.1840 - accuracy: 0.9361\n",
      "Epoch 13/30\n",
      "469/469 - 2s - loss: 0.1805 - accuracy: 0.9392\n",
      "Epoch 14/30\n",
      "469/469 - 2s - loss: 0.1818 - accuracy: 0.9374\n",
      "Epoch 15/30\n",
      "469/469 - 2s - loss: 0.1806 - accuracy: 0.9382\n",
      "Epoch 16/30\n",
      "469/469 - 2s - loss: 0.1764 - accuracy: 0.9415\n",
      "Epoch 17/30\n",
      "469/469 - 2s - loss: 0.1774 - accuracy: 0.9405\n",
      "Epoch 18/30\n",
      "469/469 - 2s - loss: 0.1772 - accuracy: 0.9405\n",
      "Epoch 19/30\n",
      "469/469 - 2s - loss: 0.1768 - accuracy: 0.9405\n",
      "Epoch 20/30\n",
      "469/469 - 2s - loss: 0.1722 - accuracy: 0.9428\n",
      "Epoch 21/30\n",
      "469/469 - 2s - loss: 0.1745 - accuracy: 0.9422\n",
      "Epoch 22/30\n",
      "469/469 - 2s - loss: 0.1722 - accuracy: 0.9452\n",
      "Epoch 23/30\n",
      "469/469 - 2s - loss: 0.1698 - accuracy: 0.9450\n",
      "Epoch 24/30\n",
      "469/469 - 2s - loss: 0.1717 - accuracy: 0.9431\n",
      "Epoch 25/30\n",
      "469/469 - 2s - loss: 0.1702 - accuracy: 0.9438\n",
      "Epoch 26/30\n",
      "469/469 - 2s - loss: 0.1689 - accuracy: 0.9448\n",
      "Epoch 27/30\n",
      "469/469 - 2s - loss: 0.1686 - accuracy: 0.9454\n",
      "Epoch 28/30\n",
      "469/469 - 2s - loss: 0.1686 - accuracy: 0.9447\n",
      "Epoch 29/30\n",
      "469/469 - 2s - loss: 0.1670 - accuracy: 0.9460\n",
      "Epoch 30/30\n",
      "469/469 - 2s - loss: 0.1698 - accuracy: 0.9447\n",
      "For 100-cell predictions on test set with size (9998, 100, 35)\n",
      "Test loss: 1.7228342294692993, Test accuracy: 0.5542108416557312\n",
      "Accuracy: 0.5542108421684336\n",
      "F-score: 0.548840975807268\n",
      "precision: 0.5554189715222291\n",
      "recall: 0.5424169667867147\n",
      "Accuracy: 0.6666666666666666\n",
      "F-score: 0.7142857142857143\n",
      "precision: 0.7142857142857143\n",
      "recall: 0.7142857142857143\n",
      "test indices\n",
      "[[15, 20, 21, 27, 52], [3, 6, 7, 12, 16, 29, 39]]\n",
      "train indices\n",
      "[58, 26, 50, 35, 31, 1, 30, 10, 54, 44, 56, 19, 36, 18, 0, 48, 53, 45, 57, 51, 55, 49, 43, 17]\n",
      "[24, 46, 11, 34, 9, 28, 8, 33, 40, 4, 41, 23, 2, 22, 5, 38, 32, 42, 25, 37, 47, 13, 59, 14]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "34876\n",
      "Predictions based on multi-cell inputs containing 3411 cells.\n",
      "12\n",
      "<class 'list'>\n",
      "Epoch 1/30\n",
      "469/469 - 3s - loss: 0.4020 - accuracy: 0.8224\n",
      "Epoch 2/30\n",
      "469/469 - 2s - loss: 0.2911 - accuracy: 0.8828\n",
      "Epoch 3/30\n",
      "469/469 - 2s - loss: 0.2655 - accuracy: 0.8967\n",
      "Epoch 4/30\n",
      "469/469 - 3s - loss: 0.2505 - accuracy: 0.9041\n",
      "Epoch 5/30\n",
      "469/469 - 3s - loss: 0.2415 - accuracy: 0.9071\n",
      "Epoch 6/30\n",
      "469/469 - 3s - loss: 0.2339 - accuracy: 0.9107\n",
      "Epoch 7/30\n",
      "469/469 - 3s - loss: 0.2258 - accuracy: 0.9145\n",
      "Epoch 8/30\n",
      "469/469 - 2s - loss: 0.2213 - accuracy: 0.9167\n",
      "Epoch 9/30\n",
      "469/469 - 2s - loss: 0.2165 - accuracy: 0.9193\n",
      "Epoch 10/30\n",
      "469/469 - 2s - loss: 0.2138 - accuracy: 0.9210\n",
      "Epoch 11/30\n",
      "469/469 - 2s - loss: 0.2102 - accuracy: 0.9225\n",
      "Epoch 12/30\n",
      "469/469 - 2s - loss: 0.2061 - accuracy: 0.9248\n",
      "Epoch 13/30\n",
      "469/469 - 2s - loss: 0.2073 - accuracy: 0.9232\n",
      "Epoch 14/30\n",
      "469/469 - 2s - loss: 0.2048 - accuracy: 0.9257\n",
      "Epoch 15/30\n",
      "469/469 - 2s - loss: 0.2030 - accuracy: 0.9267\n",
      "Epoch 16/30\n",
      "469/469 - 2s - loss: 0.2021 - accuracy: 0.9283\n",
      "Epoch 17/30\n",
      "469/469 - 3s - loss: 0.2022 - accuracy: 0.9260\n",
      "Epoch 18/30\n",
      "469/469 - 3s - loss: 0.1982 - accuracy: 0.9301\n",
      "Epoch 19/30\n",
      "469/469 - 3s - loss: 0.1985 - accuracy: 0.9297\n",
      "Epoch 20/30\n",
      "469/469 - 3s - loss: 0.1974 - accuracy: 0.9300\n",
      "Epoch 21/30\n",
      "469/469 - 2s - loss: 0.1973 - accuracy: 0.9309\n",
      "Epoch 22/30\n",
      "469/469 - 2s - loss: 0.1986 - accuracy: 0.9286\n",
      "Epoch 23/30\n",
      "469/469 - 2s - loss: 0.1987 - accuracy: 0.9291\n",
      "Epoch 24/30\n",
      "469/469 - 3s - loss: 0.1976 - accuracy: 0.9284\n",
      "Epoch 25/30\n",
      "469/469 - 3s - loss: 0.1968 - accuracy: 0.9307\n",
      "Epoch 26/30\n",
      "469/469 - 3s - loss: 0.1954 - accuracy: 0.9305\n",
      "Epoch 27/30\n",
      "469/469 - 3s - loss: 0.1948 - accuracy: 0.9300\n",
      "Epoch 28/30\n",
      "469/469 - 3s - loss: 0.1933 - accuracy: 0.9333\n",
      "Epoch 29/30\n",
      "469/469 - 3s - loss: 0.1967 - accuracy: 0.9307\n",
      "Epoch 30/30\n",
      "469/469 - 3s - loss: 0.1952 - accuracy: 0.9319\n",
      "For 100-cell predictions on test set with size (9998, 100, 35)\n",
      "Test loss: 1.3240532875061035, Test accuracy: 0.5999199748039246\n",
      "Accuracy: 0.5999199839967994\n",
      "F-score: 0.6667777407530824\n",
      "precision: 0.571224664573223\n",
      "recall: 0.8007202881152461\n",
      "Accuracy: 0.6666666666666666\n",
      "F-score: 0.75\n",
      "precision: 0.6666666666666666\n",
      "recall: 0.8571428571428571\n",
      "test indices\n",
      "[[26, 49, 53, 54, 56], [2, 6, 9, 34, 42, 47, 59]]\n",
      "train indices\n",
      "[58, 10, 30, 21, 57, 45, 19, 31, 48, 36, 0, 35, 27, 52, 18, 20, 17, 50, 1, 51, 15, 44, 43, 55]\n",
      "[32, 14, 5, 7, 28, 37, 24, 3, 46, 40, 39, 25, 8, 29, 12, 41, 33, 38, 4, 23, 13, 11, 16, 22]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "9735\n",
      "Predictions based on multi-cell inputs containing 9735 cells.\n",
      "12\n",
      "<class 'list'>\n",
      "Epoch 1/30\n",
      "469/469 - 4s - loss: 0.4231 - accuracy: 0.8085\n",
      "Epoch 2/30\n",
      "469/469 - 3s - loss: 0.3051 - accuracy: 0.8755\n",
      "Epoch 3/30\n",
      "469/469 - 3s - loss: 0.2645 - accuracy: 0.8969\n",
      "Epoch 4/30\n",
      "469/469 - 3s - loss: 0.2413 - accuracy: 0.9104\n",
      "Epoch 5/30\n",
      "469/469 - 3s - loss: 0.2299 - accuracy: 0.9162\n",
      "Epoch 6/30\n",
      "469/469 - 2s - loss: 0.2232 - accuracy: 0.9188\n",
      "Epoch 7/30\n",
      "469/469 - 3s - loss: 0.2166 - accuracy: 0.9231\n",
      "Epoch 8/30\n",
      "469/469 - 2s - loss: 0.2105 - accuracy: 0.9260\n",
      "Epoch 9/30\n",
      "469/469 - 2s - loss: 0.2097 - accuracy: 0.9261\n",
      "Epoch 10/30\n",
      "469/469 - 2s - loss: 0.2031 - accuracy: 0.9307\n",
      "Epoch 11/30\n",
      "469/469 - 2s - loss: 0.2029 - accuracy: 0.9306\n",
      "Epoch 12/30\n",
      "469/469 - 2s - loss: 0.1993 - accuracy: 0.9332\n",
      "Epoch 13/30\n",
      "469/469 - 2s - loss: 0.1982 - accuracy: 0.9326\n",
      "Epoch 14/30\n",
      "469/469 - 2s - loss: 0.1955 - accuracy: 0.9337\n",
      "Epoch 15/30\n",
      "469/469 - 2s - loss: 0.1955 - accuracy: 0.9345\n",
      "Epoch 16/30\n",
      "469/469 - 2s - loss: 0.1966 - accuracy: 0.9339\n",
      "Epoch 17/30\n",
      "469/469 - 2s - loss: 0.1910 - accuracy: 0.9365\n",
      "Epoch 18/30\n",
      "469/469 - 2s - loss: 0.1933 - accuracy: 0.9353\n",
      "Epoch 19/30\n",
      "469/469 - 2s - loss: 0.1905 - accuracy: 0.9372\n",
      "Epoch 20/30\n",
      "469/469 - 2s - loss: 0.1902 - accuracy: 0.9377\n",
      "Epoch 21/30\n",
      "469/469 - 2s - loss: 0.1876 - accuracy: 0.9379\n",
      "Epoch 22/30\n",
      "469/469 - 2s - loss: 0.1887 - accuracy: 0.9385\n",
      "Epoch 23/30\n",
      "469/469 - 2s - loss: 0.1884 - accuracy: 0.9391\n",
      "Epoch 24/30\n",
      "469/469 - 2s - loss: 0.1887 - accuracy: 0.9376\n",
      "Epoch 25/30\n",
      "469/469 - 2s - loss: 0.1845 - accuracy: 0.9396\n",
      "Epoch 26/30\n",
      "469/469 - 2s - loss: 0.1859 - accuracy: 0.9389\n",
      "Epoch 27/30\n",
      "469/469 - 2s - loss: 0.1870 - accuracy: 0.9395\n",
      "Epoch 28/30\n",
      "469/469 - 2s - loss: 0.1848 - accuracy: 0.9393\n",
      "Epoch 29/30\n",
      "469/469 - 2s - loss: 0.1855 - accuracy: 0.9399\n",
      "Epoch 30/30\n",
      "469/469 - 2s - loss: 0.1844 - accuracy: 0.9394\n",
      "For 100-cell predictions on test set with size (9998, 100, 35)\n",
      "Test loss: 1.0445971488952637, Test accuracy: 0.6271254420280457\n",
      "Accuracy: 0.627125425085017\n",
      "F-score: 0.5656024236774645\n",
      "precision: 0.6771763392857143\n",
      "recall: 0.48559423769507803\n",
      "Accuracy: 0.5833333333333334\n",
      "F-score: 0.5454545454545454\n",
      "precision: 0.75\n",
      "recall: 0.42857142857142855\n",
      "test indices\n",
      "[[10, 30, 36, 45, 50], [5, 9, 12, 13, 39, 41, 42]]\n",
      "train indices\n",
      "[58, 53, 19, 0, 44, 55, 51, 43, 27, 56, 26, 35, 57, 20, 49, 18, 15, 52, 48, 1, 21, 54, 17, 31]\n",
      "[37, 59, 3, 23, 33, 7, 46, 47, 6, 14, 25, 38, 32, 11, 24, 28, 16, 2, 40, 22, 34, 8, 4, 29]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "33574\n",
      "Predictions based on multi-cell inputs containing 9077 cells.\n",
      "12\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "469/469 - 3s - loss: 0.4385 - accuracy: 0.7911\n",
      "Epoch 2/30\n",
      "469/469 - 2s - loss: 0.3243 - accuracy: 0.8628\n",
      "Epoch 3/30\n",
      "469/469 - 2s - loss: 0.2915 - accuracy: 0.8813\n",
      "Epoch 4/30\n",
      "469/469 - 2s - loss: 0.2695 - accuracy: 0.8920\n",
      "Epoch 5/30\n",
      "469/469 - 2s - loss: 0.2597 - accuracy: 0.8996\n",
      "Epoch 6/30\n",
      "469/469 - 2s - loss: 0.2519 - accuracy: 0.9035\n",
      "Epoch 7/30\n",
      "469/469 - 2s - loss: 0.2473 - accuracy: 0.9063\n",
      "Epoch 8/30\n",
      "469/469 - 2s - loss: 0.2417 - accuracy: 0.9091\n",
      "Epoch 9/30\n",
      "469/469 - 2s - loss: 0.2365 - accuracy: 0.9119\n",
      "Epoch 10/30\n",
      "469/469 - 2s - loss: 0.2288 - accuracy: 0.9176\n",
      "Epoch 11/30\n",
      "469/469 - 2s - loss: 0.2250 - accuracy: 0.9185\n",
      "Epoch 12/30\n",
      "469/469 - 2s - loss: 0.2229 - accuracy: 0.9188\n",
      "Epoch 13/30\n",
      "469/469 - 2s - loss: 0.2211 - accuracy: 0.9211\n",
      "Epoch 14/30\n",
      "469/469 - 2s - loss: 0.2203 - accuracy: 0.9204\n",
      "Epoch 15/30\n",
      "469/469 - 2s - loss: 0.2200 - accuracy: 0.9201\n",
      "Epoch 16/30\n",
      "469/469 - 2s - loss: 0.2167 - accuracy: 0.9231\n",
      "Epoch 17/30\n",
      "469/469 - 2s - loss: 0.2158 - accuracy: 0.9238\n",
      "Epoch 18/30\n",
      "469/469 - 3s - loss: 0.2148 - accuracy: 0.9245\n",
      "Epoch 19/30\n",
      "469/469 - 3s - loss: 0.2152 - accuracy: 0.9235\n",
      "Epoch 20/30\n",
      "469/469 - 2s - loss: 0.2130 - accuracy: 0.9254\n",
      "Epoch 21/30\n",
      "469/469 - 2s - loss: 0.2119 - accuracy: 0.9246\n",
      "Epoch 22/30\n",
      "469/469 - 2s - loss: 0.2130 - accuracy: 0.9240\n",
      "Epoch 23/30\n",
      "469/469 - 2s - loss: 0.2099 - accuracy: 0.9264\n",
      "Epoch 24/30\n",
      "469/469 - 3s - loss: 0.2103 - accuracy: 0.9262\n",
      "Epoch 25/30\n",
      "469/469 - 3s - loss: 0.2121 - accuracy: 0.9255\n",
      "Epoch 26/30\n",
      "469/469 - 3s - loss: 0.2093 - accuracy: 0.9269\n",
      "Epoch 27/30\n",
      "469/469 - 2s - loss: 0.2101 - accuracy: 0.9273\n",
      "Epoch 28/30\n",
      "469/469 - 2s - loss: 0.2079 - accuracy: 0.9270\n",
      "Epoch 29/30\n",
      "469/469 - 3s - loss: 0.2073 - accuracy: 0.9284\n",
      "Epoch 30/30\n",
      "469/469 - 3s - loss: 0.2087 - accuracy: 0.9275\n",
      "For 100-cell predictions on test set with size (9998, 100, 35)\n",
      "Test loss: 0.999501645565033, Test accuracy: 0.6247249245643616\n",
      "Accuracy: 0.6247249449889978\n",
      "F-score: 0.6312893081761007\n",
      "precision: 0.6203167246040943\n",
      "recall: 0.64265706282513\n",
      "Accuracy: 0.6666666666666666\n",
      "F-score: 0.7142857142857143\n",
      "precision: 0.7142857142857143\n",
      "recall: 0.7142857142857143\n",
      "test indices\n",
      "[[18, 21, 26, 27, 35], [6, 8, 13, 14, 23, 25, 46]]\n",
      "train indices\n",
      "[49, 10, 1, 58, 56, 55, 51, 53, 31, 50, 30, 57, 54, 52, 17, 19, 48, 43, 0, 44, 36, 20, 15, 45]\n",
      "[2, 22, 11, 4, 9, 41, 3, 59, 33, 16, 7, 28, 12, 32, 5, 47, 24, 40, 34, 38, 37, 42, 39, 29]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "33390\n",
      "Predictions based on multi-cell inputs containing 9735 cells.\n",
      "12\n",
      "<class 'list'>\n",
      "Epoch 1/30\n",
      "469/469 - 2s - loss: 0.4276 - accuracy: 0.7983\n",
      "Epoch 2/30\n",
      "469/469 - 2s - loss: 0.3275 - accuracy: 0.8637\n",
      "Epoch 3/30\n",
      "469/469 - 2s - loss: 0.2963 - accuracy: 0.8790\n",
      "Epoch 4/30\n",
      "469/469 - 2s - loss: 0.2778 - accuracy: 0.8889\n",
      "Epoch 5/30\n",
      "469/469 - 2s - loss: 0.2659 - accuracy: 0.8935\n",
      "Epoch 6/30\n",
      "469/469 - 2s - loss: 0.2558 - accuracy: 0.9001\n",
      "Epoch 7/30\n",
      "469/469 - 2s - loss: 0.2499 - accuracy: 0.9019\n",
      "Epoch 8/30\n",
      "469/469 - 2s - loss: 0.2446 - accuracy: 0.9050\n",
      "Epoch 9/30\n",
      "469/469 - 2s - loss: 0.2365 - accuracy: 0.9104\n",
      "Epoch 10/30\n",
      "469/469 - 2s - loss: 0.2316 - accuracy: 0.9152\n",
      "Epoch 11/30\n",
      "469/469 - 2s - loss: 0.2273 - accuracy: 0.9159\n",
      "Epoch 12/30\n",
      "469/469 - 2s - loss: 0.2263 - accuracy: 0.9156\n",
      "Epoch 13/30\n",
      "469/469 - 2s - loss: 0.2191 - accuracy: 0.9197\n",
      "Epoch 14/30\n",
      "469/469 - 2s - loss: 0.2172 - accuracy: 0.9213\n",
      "Epoch 15/30\n",
      "469/469 - 2s - loss: 0.2162 - accuracy: 0.9194\n",
      "Epoch 16/30\n",
      "469/469 - 2s - loss: 0.2156 - accuracy: 0.9222\n",
      "Epoch 17/30\n",
      "469/469 - 2s - loss: 0.2124 - accuracy: 0.9234\n",
      "Epoch 18/30\n",
      "469/469 - 2s - loss: 0.2089 - accuracy: 0.9258\n",
      "Epoch 19/30\n",
      "469/469 - 2s - loss: 0.2104 - accuracy: 0.9244\n",
      "Epoch 20/30\n",
      "469/469 - 2s - loss: 0.2120 - accuracy: 0.9243\n",
      "Epoch 21/30\n",
      "469/469 - 2s - loss: 0.2083 - accuracy: 0.9254\n",
      "Epoch 22/30\n",
      "469/469 - 2s - loss: 0.2088 - accuracy: 0.9256\n",
      "Epoch 23/30\n",
      "469/469 - 2s - loss: 0.2060 - accuracy: 0.9273\n",
      "Epoch 24/30\n",
      "469/469 - 2s - loss: 0.2071 - accuracy: 0.9273\n",
      "Epoch 25/30\n",
      "469/469 - 2s - loss: 0.2050 - accuracy: 0.9281\n",
      "Epoch 26/30\n",
      "469/469 - 2s - loss: 0.2064 - accuracy: 0.9272\n",
      "Epoch 27/30\n",
      "469/469 - 2s - loss: 0.2044 - accuracy: 0.9283\n",
      "Epoch 28/30\n",
      "469/469 - 2s - loss: 0.2041 - accuracy: 0.9284\n",
      "Epoch 29/30\n",
      "469/469 - 2s - loss: 0.2043 - accuracy: 0.9277\n",
      "Epoch 30/30\n",
      "469/469 - 2s - loss: 0.2045 - accuracy: 0.9277\n",
      "For 100-cell predictions on test set with size (9998, 100, 35)\n",
      "Test loss: 1.0129516124725342, Test accuracy: 0.6330265998840332\n",
      "Accuracy: 0.6330266053210643\n",
      "F-score: 0.6734312416555407\n",
      "precision: 0.6065416065416065\n",
      "recall: 0.7569027611044418\n",
      "Accuracy: 0.6666666666666666\n",
      "F-score: 0.75\n",
      "precision: 0.6666666666666666\n",
      "recall: 0.8571428571428571\n",
      "test indices\n",
      "[[10, 30, 35, 52, 56], [2, 8, 22, 28, 32, 38, 40]]\n",
      "train indices\n",
      "[15, 36, 20, 43, 58, 51, 1, 50, 44, 18, 0, 49, 19, 17, 55, 54, 27, 31, 48, 45, 53, 21, 57, 26]\n",
      "[41, 3, 11, 13, 33, 5, 24, 7, 29, 37, 16, 46, 12, 47, 4, 34, 14, 6, 9, 25, 39, 23, 42, 59]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "33574\n",
      "Predictions based on multi-cell inputs containing 5968 cells.\n",
      "12\n",
      "<class 'list'>\n",
      "Epoch 1/30\n",
      "469/469 - 2s - loss: 0.4028 - accuracy: 0.8215\n",
      "Epoch 2/30\n",
      "469/469 - 2s - loss: 0.2940 - accuracy: 0.8823\n",
      "Epoch 3/30\n",
      "469/469 - 2s - loss: 0.2635 - accuracy: 0.8981\n",
      "Epoch 4/30\n",
      "469/469 - 2s - loss: 0.2408 - accuracy: 0.9105\n",
      "Epoch 5/30\n",
      "469/469 - 2s - loss: 0.2291 - accuracy: 0.9166\n",
      "Epoch 6/30\n",
      "469/469 - 2s - loss: 0.2201 - accuracy: 0.9204\n",
      "Epoch 7/30\n",
      "469/469 - 2s - loss: 0.2143 - accuracy: 0.9228\n",
      "Epoch 8/30\n",
      "469/469 - 2s - loss: 0.2081 - accuracy: 0.9255\n",
      "Epoch 9/30\n",
      "469/469 - 2s - loss: 0.2056 - accuracy: 0.9282\n",
      "Epoch 10/30\n",
      "469/469 - 2s - loss: 0.2005 - accuracy: 0.9294\n",
      "Epoch 11/30\n",
      "469/469 - 2s - loss: 0.2003 - accuracy: 0.9293\n",
      "Epoch 12/30\n",
      "469/469 - 2s - loss: 0.1970 - accuracy: 0.9317\n",
      "Epoch 13/30\n",
      "469/469 - 2s - loss: 0.1966 - accuracy: 0.9328\n",
      "Epoch 14/30\n",
      "469/469 - 2s - loss: 0.1960 - accuracy: 0.9343\n",
      "Epoch 15/30\n",
      "469/469 - 2s - loss: 0.1941 - accuracy: 0.9342\n",
      "Epoch 16/30\n",
      "469/469 - 3s - loss: 0.1933 - accuracy: 0.9337\n",
      "Epoch 17/30\n",
      "469/469 - 3s - loss: 0.1921 - accuracy: 0.9351\n",
      "Epoch 18/30\n",
      "469/469 - 2s - loss: 0.1908 - accuracy: 0.9351\n",
      "Epoch 19/30\n",
      "469/469 - 4s - loss: 0.1904 - accuracy: 0.9346\n",
      "Epoch 20/30\n",
      "469/469 - 3s - loss: 0.1876 - accuracy: 0.9381\n",
      "Epoch 21/30\n",
      "469/469 - 3s - loss: 0.1891 - accuracy: 0.9366\n",
      "Epoch 22/30\n",
      "469/469 - 3s - loss: 0.1875 - accuracy: 0.9376\n",
      "Epoch 23/30\n",
      "469/469 - 3s - loss: 0.1865 - accuracy: 0.9387\n",
      "Epoch 24/30\n",
      "469/469 - 3s - loss: 0.1877 - accuracy: 0.9378\n",
      "Epoch 25/30\n",
      "469/469 - 2s - loss: 0.1884 - accuracy: 0.9372\n",
      "Epoch 26/30\n",
      "469/469 - 3s - loss: 0.1858 - accuracy: 0.9383\n",
      "Epoch 27/30\n",
      "469/469 - 2s - loss: 0.1844 - accuracy: 0.9386\n",
      "Epoch 28/30\n",
      "469/469 - 3s - loss: 0.1874 - accuracy: 0.9381\n",
      "Epoch 29/30\n",
      "469/469 - 3s - loss: 0.1854 - accuracy: 0.9381\n",
      "Epoch 30/30\n",
      "469/469 - 3s - loss: 0.1825 - accuracy: 0.9398\n",
      "For 100-cell predictions on test set with size (9998, 100, 35)\n",
      "Test loss: 1.928684949874878, Test accuracy: 0.4823964834213257\n",
      "Accuracy: 0.48239647929585916\n",
      "F-score: 0.58396977248975\n",
      "precision: 0.4881064373068136\n",
      "recall: 0.7266906762705082\n",
      "Accuracy: 0.5833333333333334\n",
      "F-score: 0.7058823529411764\n",
      "precision: 0.6\n",
      "recall: 0.8571428571428571\n",
      "test indices\n",
      "[[1, 48, 49, 52, 54], [5, 9, 14, 24, 34, 37, 38]]\n",
      "train indices\n",
      "[27, 45, 44, 36, 57, 19, 0, 30, 18, 20, 55, 21, 15, 53, 35, 50, 10, 51, 17, 56, 31, 26, 58, 43]\n",
      "[39, 8, 33, 46, 4, 41, 2, 32, 23, 59, 28, 16, 11, 3, 47, 29, 40, 25, 42, 13, 6, 22, 12, 7]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "11663\n",
      "Predictions based on multi-cell inputs containing 11663 cells.\n",
      "12\n",
      "<class 'list'>\n",
      "Epoch 1/30\n",
      "469/469 - 3s - loss: 0.4475 - accuracy: 0.7921\n",
      "Epoch 2/30\n",
      "469/469 - 2s - loss: 0.3402 - accuracy: 0.8567\n",
      "Epoch 3/30\n",
      "469/469 - 2s - loss: 0.3033 - accuracy: 0.8776\n",
      "Epoch 4/30\n",
      "469/469 - 2s - loss: 0.2812 - accuracy: 0.8883\n",
      "Epoch 5/30\n",
      "469/469 - 2s - loss: 0.2649 - accuracy: 0.8966\n",
      "Epoch 6/30\n",
      "469/469 - 2s - loss: 0.2527 - accuracy: 0.9032\n",
      "Epoch 7/30\n",
      "469/469 - 3s - loss: 0.2477 - accuracy: 0.9055\n",
      "Epoch 8/30\n",
      "469/469 - 3s - loss: 0.2395 - accuracy: 0.9113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "469/469 - 3s - loss: 0.2360 - accuracy: 0.9129\n",
      "Epoch 10/30\n",
      "469/469 - 3s - loss: 0.2333 - accuracy: 0.9133\n",
      "Epoch 11/30\n",
      "469/469 - 3s - loss: 0.2303 - accuracy: 0.9162\n",
      "Epoch 12/30\n",
      "469/469 - 3s - loss: 0.2322 - accuracy: 0.9150\n",
      "Epoch 13/30\n",
      "469/469 - 2s - loss: 0.2241 - accuracy: 0.9185\n",
      "Epoch 14/30\n",
      "469/469 - 2s - loss: 0.2242 - accuracy: 0.9198\n",
      "Epoch 15/30\n",
      "469/469 - 2s - loss: 0.2234 - accuracy: 0.9197\n",
      "Epoch 16/30\n",
      "469/469 - 2s - loss: 0.2218 - accuracy: 0.9207\n",
      "Epoch 17/30\n",
      "469/469 - 2s - loss: 0.2201 - accuracy: 0.9217\n",
      "Epoch 18/30\n",
      "469/469 - 2s - loss: 0.2201 - accuracy: 0.9206\n",
      "Epoch 19/30\n",
      "469/469 - 2s - loss: 0.2179 - accuracy: 0.9225\n",
      "Epoch 20/30\n",
      "469/469 - 2s - loss: 0.2160 - accuracy: 0.9228\n",
      "Epoch 21/30\n",
      "469/469 - 2s - loss: 0.2162 - accuracy: 0.9237\n",
      "Epoch 22/30\n",
      "469/469 - 2s - loss: 0.2152 - accuracy: 0.9237\n",
      "Epoch 23/30\n",
      "469/469 - 2s - loss: 0.2151 - accuracy: 0.9237\n",
      "Epoch 24/30\n",
      "469/469 - 2s - loss: 0.2118 - accuracy: 0.9254\n",
      "Epoch 25/30\n",
      "469/469 - 2s - loss: 0.2120 - accuracy: 0.9254\n",
      "Epoch 26/30\n",
      "469/469 - 2s - loss: 0.2131 - accuracy: 0.9236\n",
      "Epoch 27/30\n",
      "469/469 - 2s - loss: 0.2105 - accuracy: 0.9263\n",
      "Epoch 28/30\n",
      "469/469 - 2s - loss: 0.2124 - accuracy: 0.9265\n",
      "Epoch 29/30\n",
      "469/469 - 2s - loss: 0.2109 - accuracy: 0.9263\n",
      "Epoch 30/30\n",
      "469/469 - 2s - loss: 0.2112 - accuracy: 0.9262\n",
      "For 100-cell predictions on test set with size (9998, 100, 35)\n",
      "Test loss: 0.6569966673851013, Test accuracy: 0.7282456755638123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "accurMulti = []\n",
    "precMulti = []\n",
    "recallMulti = []\n",
    "fscoreMulti = []\n",
    "accurPheno = []\n",
    "precPheno = []\n",
    "recallPheno = []\n",
    "fscorePheno = []\n",
    "for run in range(10):\n",
    "    train_samples, train_phenotypes, test_samples, test_phenotypes, test_indices, train_indices = train_test_split()\n",
    "    # generate ntrain_all (ntrain_all/2 per phenotype) training samples for centralized test!\n",
    "\n",
    "    scaler,x_tr,y_tr = generate_data(train_samples, train_phenotypes, 'Flow/', generate_valid_set=False, \n",
    "                                                   ncell=ncells, nsubset=int(ntrain_all/2), scale=True, \n",
    "                                                   per_sample=False, verbose=0, saveFile=False,subset_selection = 'random')\n",
    "\n",
    "\n",
    "    scaler,x_test,y_test = generate_data(test_samples, test_phenotypes, 'Flow/', generate_valid_set=False, \n",
    "                                                   ncell=ncells, nsubset=5000, scale=True, \n",
    "                                                   per_sample=False, verbose=0, saveFile=False,\n",
    "                                                   subset_selection = 'random', generateAsTest=True)\n",
    "    print(len(test_samples[0]))\n",
    "    data_test,phenotypes,ncell_per_sample = generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "\n",
    "    y_tr_n = to_categorical(y_tr)\n",
    "\n",
    "\n",
    "    model = create_model(ncells,ncells,nfilter)\n",
    "    #generate data\n",
    "\n",
    "    x_tr_n = x_tr.transpose(0,2,1)\n",
    "    # Fit data to model\n",
    "    history = model.fit(x_tr_n, y_tr_n,\n",
    "                batch_size=64,\n",
    "                epochs=30,\n",
    "                verbose=2,\n",
    "                validation_split=0)\n",
    "    #For 100-cell predictions on test set\n",
    "\n",
    "    \n",
    "    #test for multi-cell\n",
    "    x_test_n = x_test.transpose(0,2,1)\n",
    "    x_test_n = x_test_n[0:ntest_all,:]\n",
    "\n",
    "    y_test_n = to_categorical(y_test)\n",
    "    y_test_n = y_test_n[0:ntest_all,:]\n",
    "    loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "    #score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "    print(\"For 100-cell predictions on test set with size\",x_test_n.shape)\n",
    "    print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "    y_pred = model.predict(x_test_n)\n",
    "    y_pred = model_pred(y_pred)\n",
    "    y_test= y_test[0:ntest_all]\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"F-score:\",f1_score(y_test, y_pred))\n",
    "    print(\"precision:\",precision_score(y_test, y_pred))\n",
    "    print(\"recall:\",recall_score(y_test, y_pred)) \n",
    "    \n",
    "    #to write to excell sheet...\n",
    "    accurMulti.append(accuracy_score(y_test, y_pred))\n",
    "    precMulti.append(precision_score(y_test, y_pred))\n",
    "    recallMulti.append(recall_score(y_test, y_pred)) \n",
    "    fscoreMulti.append(f1_score(y_test, y_pred))\n",
    "    #For phenotype predictions on test set using all cells \n",
    "\n",
    "    model2 = create_model(ncell_per_sample, ncell_per_sample,nfilter)\n",
    "    weights = model.get_weights()\n",
    "    model2.set_weights(weights)\n",
    "    data_test_n = data_test\n",
    "    phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "    y_pred = model2.predict(data_test_n)\n",
    "    y_pred = model_pred(y_pred)\n",
    "    # print(y_pred)\n",
    "    print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "    print(\"F-score:\",f1_score(phenotypes, y_pred))\n",
    "    print(\"precision:\",precision_score(phenotypes, y_pred))\n",
    "    print(\"recall:\",recall_score(phenotypes, y_pred)) \n",
    "    \n",
    "    accurPheno.append(accuracy_score(phenotypes, y_pred))\n",
    "    precPheno.append(precision_score(phenotypes, y_pred))\n",
    "    recallPheno.append(recall_score(phenotypes, y_pred)) \n",
    "    fscorePheno.append(f1_score(phenotypes, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To average LOCAL computations metrics\n",
    "nhosts = 2\n",
    "totalRun = 2\n",
    "accurMulti = np.empty([nhosts, totalRun])\n",
    "precMulti = np.empty([nhosts, totalRun])\n",
    "recallMulti = np.empty([nhosts, totalRun])\n",
    "fscoreMulti = np.empty([nhosts, totalRun])\n",
    "accurPheno = np.empty([nhosts, totalRun])\n",
    "precPheno = np.empty([nhosts, totalRun])\n",
    "recallPheno = np.empty([nhosts, totalRun])\n",
    "fscorePheno = np.empty([nhosts, totalRun])\n",
    "\n",
    "for run in range(totalRun):\n",
    "    train_samples, train_phenotypes, test_samples, test_phenotypes, test_indices, train_indices = train_test_split()\n",
    "    \n",
    "    #split between n host\n",
    "    xtr,ytr,scaler = splitForLocal(nhosts,train_indices, test_indices)\n",
    "    \n",
    "    scaler,x_test,y_test = generate_data(test_samples, test_phenotypes, 'Flow/', generate_valid_set=False, \n",
    "                                                   ncell=ncells, nsubset=5000, scale=True, \n",
    "                                                   per_sample=False, verbose=0, saveFile=False,\n",
    "                                                   subset_selection = 'random', generateAsTest=True,scaler=scaler)\n",
    "    \n",
    "    data_test,phenotypes,ncell_per_sample = generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "\n",
    "\n",
    "    for i in range(nhosts):\n",
    "        y_tr_n = to_categorical(ytr[i])\n",
    "        model = create_model(ncells,ncells,nfilter)\n",
    "        #generate data\n",
    "\n",
    "        x_tr_n = xtr[i].transpose(0,2,1)\n",
    "        # Fit data to model\n",
    "        history = model.fit(x_tr_n, y_tr_n,\n",
    "                    batch_size=64,\n",
    "                    epochs=30,\n",
    "                    verbose=2,\n",
    "                    validation_split=0)\n",
    "\n",
    "        #test for multi-cell\n",
    "        x_test_n = x_test.transpose(0,2,1)\n",
    "        x_test_n = x_test_n[0:ntest_all,:]\n",
    "\n",
    "        y_test_n = to_categorical(y_test)\n",
    "        y_test_n = y_test_n[0:ntest_all,:]\n",
    "        loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "        #score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "        print(\"For 100-cell predictions on test set with size\",x_test_n.shape)\n",
    "        print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "        y_pred = model.predict(x_test_n)\n",
    "        y_pred = model_pred(y_pred)\n",
    "        y_test= y_test[0:ntest_all]\n",
    "\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"F-score:\",f1_score(y_test, y_pred))\n",
    "        print(\"precision:\",precision_score(y_test, y_pred))\n",
    "        print(\"recall:\",recall_score(y_test, y_pred)) \n",
    "\n",
    "        #to write to excell sheet...\n",
    "        accurMulti[i,run] = accuracy_score(y_test, y_pred)\n",
    "        precMulti[i,run] = precision_score(y_test, y_pred)\n",
    "        recallMulti[i,run]= recall_score(y_test, y_pred)\n",
    "        fscoreMulti[i,run]= f1_score(y_test, y_pred)\n",
    "        #For phenotype predictions on test set using all cells \n",
    "\n",
    "        model2 = create_model(ncell_per_sample, ncell_per_sample,nfilter)\n",
    "        weights = model.get_weights()\n",
    "        model2.set_weights(weights)\n",
    "        data_test_n = data_test\n",
    "        phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "        y_pred = model2.predict(data_test_n)\n",
    "        y_pred = model_pred(y_pred)\n",
    "        # print(y_pred)\n",
    "        print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "        print(\"F-score:\",f1_score(phenotypes, y_pred))\n",
    "        print(\"precision:\",precision_score(phenotypes, y_pred))\n",
    "        print(\"recall:\",recall_score(phenotypes, y_pred)) \n",
    "        \n",
    "        accurPheno[i,run] = accuracy_score(phenotypes, y_pred)\n",
    "        precPheno[i,run] = precision_score(phenotypes, y_pred)\n",
    "        recallPheno[i,run]= recall_score(phenotypes, y_pred)\n",
    "        fscorePheno[i,run]= f1_score(phenotypes, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy\n",
    "\n",
    "with open('myfile.csv', 'w', newline='') as file:\n",
    "    mywriter = csv.writer(file, delimiter=',')\n",
    "    phen = np.sum(recallPheno[:,:],axis=0)/nhosts\n",
    "    multi = np.sum(recallMulti[:,:],axis=0)/nhosts\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(phen)))\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(multi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy\n",
    "\n",
    "with open('myfile.csv', 'w', newline='') as file:\n",
    "    mywriter = csv.writer(file, delimiter=',')\n",
    "    phenR = np.sum(recallPheno[:,:],axis=0)/nhosts\n",
    "    multiR = np.sum(recallMulti[:,:],axis=0)/nhosts\n",
    "        \n",
    "    phenP = np.sum(precPheno[:,:],axis=0)/nhosts\n",
    "    multiP = np.sum(precMulti[:,:],axis=0)/nhosts\n",
    "    \n",
    "    fscoreAvPhen = 2*phenR*phenP/(phenP+phenR)\n",
    "    fscoreAvMulti = 2*multiR*multiP/(multiP+multiR)\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(fscoreAvPhen)))\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(fscoreAvMulti)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy\n",
    "\n",
    "with open('myfile.csv', 'w', newline='') as file:\n",
    "    mywriter = csv.writer(file, delimiter=',')\n",
    "    phen = np.sum(recallPheno[:,:],axis=0)\n",
    "    multi = np.sum(recallMulti[:,:],axis=0)\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(phen)))\n",
    "    mywriter.writerows(map(lambda x: [x], np.asarray(multi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurPheno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(accurPheno[:,:],axis=0)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
