{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CellCnn [1] data generation\n",
    "\n",
    "source: https://github.com/eiriniar/CellCnn\n",
    "\n",
    "\"\"\" Copyright 2016-2017 ETH Zurich, Eirini Arvaniti and Manfred Claassen.\n",
    "\n",
    "This module contains data preprocessing/distribution functions.\n",
    "\n",
    "\"\"\"\n",
    "The code is slightly changed depending on original implementation to make it compatible with decentralized settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we preprocess and distribute a mass Acute Myeloid Leukaemia (AML) dataset[2] for 3-class classification problem for healthy, cytogenetically normal (CN), and core-binding factor translocation (CBF). For each cell, the dataset includes mass cytometry measurements of 16 markers for each cell. \n",
    "\n",
    "\n",
    "The dataset comprises mass cytometry measurements of 16 markers, as in original cellCnn[1] analysis, we use the AML samples on the AML samples with at least 10% CD34+ blast cells with availability of additional cytogenetic information. \n",
    "\n",
    "To run this example, \n",
    "\n",
    "    - download the [AML cell dataset] at https://imsb.ethz.ch/research/claassen/Software/cellcnn.html, under ALL dataset zip folder\n",
    "    - uncompress and place it in the data/cellCNN/ folder\n",
    "\n",
    "Data distribution: We fix the test set for all experimental settings, the training dataset is then generated by distribution different donors for each institution depending on number of institutions.\n",
    "\n",
    "[1] E. Arvaniti and M. Claassen. Sensitive detection of rare disease-associated cell subsets via representation learning.Nat Commun, 8:1–10, 2017\n",
    "[2] . Levine, E. Simonds, S. Bendall, K. Davis, E.-A. Amir, M. Tadmor, O. Litvin, H. Fienberg, A. Jager, E. Zunder, R. Finck, A. Gedman,I. Radtke, J. Downing, D. Pe’er, and G. Nolan. Data-driven phenotypic dissection of aml reveals progenitor-like cells that correlatewith prognosis.Cell, 162, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os, sys, errno, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cellCNN_utils  \n",
    "import sklearn.utils as sku\n",
    "from cellCNN_utils import loadFCS, ftrans, mkdir_p, get_items, generate_data, generate_normalized_data\n",
    "from pathlib import Path\n",
    "%pylab inline\n",
    "\n",
    "rand_seed = 12345\n",
    "np.random.seed(rand_seed)\n",
    "stim = 'AML'\n",
    "\n",
    "d = Path().resolve()\n",
    "sys.path.append(d)\n",
    "\n",
    "# WDIR = os.path.join(cellCnn.__path__[0], 'examples')\n",
    "OUTDIR = os.path.join(d, 'output_%s' % stim)\n",
    "mkdir_p(OUTDIR)\n",
    "\n",
    "LOOKUP_PATH = os.path.join(d, 'AML.pkl')\n",
    "\n",
    "with open(LOOKUP_PATH, 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin'\n",
    "    lookup = u.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patients SJ10, SJ12, SJ13 were characterized as CN\n",
    "#patients SJ1, SJ2, SJ3, SJ4, SJ5 presented CBF\n",
    "labels = ['CD19', 'CD11b', 'CD34', 'CD45', 'CD123', 'CD33', 'CD47', 'CD7', 'CD15', 'CD44', 'CD38', 'CD3', 'CD117', 'HLA-DR', 'CD64', 'CD41']\n",
    "trainCN, trainCBF, trainHealthy = [], [],[]\n",
    "testCN, testCBF, testHealthy = [], [],[]\n",
    "for key, val in lookup.items():\n",
    "    if \"SJ\" not in key: \n",
    "        if key == \"healthy_BM\":\n",
    "            trainHealthy.append(val[0][1])\n",
    "            trainHealthy.append(val[1][1])\n",
    "            trainHealthy.append(val[2][1])\n",
    "            testHealthy.append(val[3][1])\n",
    "            testHealthy.append(val[4][1])\n",
    "\n",
    "    if key == 'SJ10' or key == 'SJ12':\n",
    "        trainCN.append(val)\n",
    "    if key == 'SJ13':\n",
    "        testCN.append(val)\n",
    "    if key == 'SJ1' or key == 'SJ2': \n",
    "        trainCBF.append(val)\n",
    "    if key == 'SJ3' or key == 'SJ4' or key == 'SJ5':\n",
    "        testCBF.append(val)\n",
    "\n",
    "test_samples = testCN+ testCBF+ testHealthy\n",
    "test_phenotypes = [1,2,2,2,0,0 ] \n",
    "\n",
    "train_phenotypes = [0, 1 ,2]  #healthy, cn, cbf\n",
    "\n",
    "x_trainHealthy = sku.shuffle(np.vstack(trainHealthy))\n",
    "x_trainCN = sku.shuffle(np.vstack(trainCN))\n",
    "x_trainCBF = sku.shuffle(np.vstack(trainCBF))\n",
    "\n",
    "train_samples = [x_trainHealthy,x_trainCN, x_trainCBF]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate original data (with transform)\n",
    "In the following, \n",
    "- We generate training data with $ncell=200$ cells per sample and $nsubset=500$ samples per class\n",
    "- We generate the test data for $ncell=200$ cells per sample from test indices, called X_test\n",
    "- We generate another test set 'per-individual' in test indices using maximum number of cells to use for phenotype prediction, called X_test_all\n",
    "\n",
    "Processed data is placed under originalAML/ folder\n",
    "\n",
    "The script prints the max number of cells for the current example (i.e., 12440 for this dataset) which then will be used as a parameter in the golang protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "Predictions based on multi-cell inputs containing 12440 cells.\n",
      "(6, 12440, 16)\n",
      "(1498, 16, 200)\n",
      "(1498,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "scaler,x_tr,y_tr,x_test,y_test = generate_data(train_samples, train_phenotypes, 'originalAML/', valid_samples=test_samples, valid_phenotypes=test_phenotypes, ncell=200, nsubset=500, verbose=0)\n",
    "\n",
    "#generate also the test set on full ncell per sample:\n",
    "def generate_for_pheno_prediction(new_samples,phenotypes,scaler):\n",
    "        ncell_per_sample = np.min([x.shape[0] for x in new_samples])\n",
    "        print(f\"Predictions based on multi-cell inputs containing {ncell_per_sample} cells.\")\n",
    "        nmark = len(new_samples[0][1])\n",
    "        # z-transform the new samples if we did that for the training samples\n",
    "        if scaler is not None:\n",
    "            new_samples = [scaler.transform(x) for x in new_samples]\n",
    "        new_samples = [shuffle(x)[:ncell_per_sample].reshape(1, ncell_per_sample, nmark)\n",
    "                           for x in new_samples]\n",
    "        data_test = np.vstack(new_samples)\n",
    "        mkdir_p('originalAML/X_test_all/')\n",
    "        for i in range(len(data_test)):\n",
    "            np.savetxt('originalAML/' + 'X_test_all/' + str(i) +'.txt', (transpose(data_test[i])))\n",
    "        np.savetxt('originalAML/' + 'y_test_all.txt', phenotypes)\n",
    "        return data_test,phenotypes\n",
    "\n",
    "data_test,phenotypes=generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "print(shape(data_test))\n",
    "print(shape(x_test))\n",
    "print(shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate  data split between $nhosts$ parties\n",
    "In the following, \n",
    "- We generate training data with $ncell=200$ cells per sample and $nsubset=700$ samples per class, per party\n",
    "- Example below distributes the train indices per donor for 2 parties\n",
    "\n",
    "Processed data is placed under splitAML/host_i for party-i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global train splitted among hosts - indices:\n",
      "[[2], [1], [0]]\n",
      "[[0], [1], []]\n",
      "[[0], [1], []]\n",
      "\n",
      "Host no. 0 :\n",
      "scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 1 :\n",
      "scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 2 :\n",
      "scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(12345)\n",
    "# Here we randomly split the samples in training/test sets.\n",
    "nhosts= 2\n",
    "cofactor = 5\n",
    "\n",
    "#distribute train indices balanced among n hosts:\n",
    "numH = [0,1,2] #training set healthy sample indices\n",
    "numCN = [0,1] #training set CN indices\n",
    "numCBF = [0,1] #training set CBF indices\n",
    "group1_list = np.flip(np.array_split(numpy.array(numH), nhosts))\n",
    "group2_list = numpy.array_split(numpy.array(numCN), nhosts)\n",
    "group3_list = numpy.array_split(numpy.array(numCBF), nhosts)\n",
    "\n",
    "split_idx_1 = []\n",
    "split_idx_2 = []\n",
    "split_idx_3 = []\n",
    "for i in range(nhosts):\n",
    "    split_idx_1.append(group1_list[i].tolist())\n",
    "    split_idx_2.append(group2_list[i].tolist())\n",
    "    split_idx_3.append(group3_list[i].tolist())\n",
    "\n",
    "\n",
    "print(\"Global train splitted among hosts - indices:\")\n",
    "print(split_idx_1)\n",
    "print(split_idx_2)\n",
    "print(split_idx_3)\n",
    "#make sure each client gets at least one patient, comment in the following part to shuffle otherwise\n",
    "# random.shuffle(split_idx_1)\n",
    "# random.shuffle(split_idx_2)\n",
    "# random.shuffle(split_idx_3)\n",
    "# print(split_idx_1)\n",
    "# print(split_idx_2)\n",
    "# print(split_idx_3)\n",
    "for i in range(nhosts):\n",
    "    print(\"\\nHost no.\", i, \":\")\n",
    "    folder_path = 'splitAML/host' + str(i) + '/'\n",
    "    trainHealthyidx = split_idx_1[i]\n",
    "    trainCNTempidx = split_idx_2[i]\n",
    "    trainCBFTempidx = split_idx_3[i]\n",
    "    trainHealthyTemp,trainCNTemp,trainCBFTemp = [],[],[]\n",
    "    for idx in trainHealthyidx:\n",
    "        trainHealthyTemp.append(trainHealthy[idx])\n",
    "    for idx in trainCNTempidx:\n",
    "        trainCNTemp.append(trainCN[idx])\n",
    "    for idx in trainCBFTempidx:\n",
    "        trainCBFTemp.append(trainCBF[idx])\n",
    "    train_phenotypes = []\n",
    "    train_samples =[]\n",
    "     # load the training samples\n",
    "    if len(trainHealthyTemp) != 0:\n",
    "        x_trainHealthy = sku.shuffle(np.vstack(trainHealthyTemp))\n",
    "        train_phenotypes.append(0)\n",
    "    if len(trainCNTemp) != 0:\n",
    "        x_trainCN = sku.shuffle(np.vstack(trainCNTemp))\n",
    "        train_phenotypes.append(1)\n",
    "    if len(trainCBFTemp) != 0:\n",
    "        x_trainCBF = sku.shuffle(np.vstack(trainCBFTemp))\n",
    "        train_phenotypes.append(2)\n",
    "    train_samples = [x_trainHealthy] + [x_trainCN]+ [x_trainCBF]\n",
    "    generate_data(train_samples, train_phenotypes, folder_path, ncell=200, nsubset=700, verbose=0,generate_valid_set=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
