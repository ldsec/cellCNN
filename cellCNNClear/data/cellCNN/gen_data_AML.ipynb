{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CellCnn [1] data generation\n",
    "\n",
    "source: https://github.com/eiriniar/CellCnn\n",
    "\n",
    "\"\"\" Copyright 2016-2017 ETH Zurich, Eirini Arvaniti and Manfred Claassen.\n",
    "\n",
    "This module contains data preprocessing/distribution functions.\n",
    "\n",
    "\"\"\"\n",
    "The code is slightly changed depending on original implementation to make it compatible with decentralized settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we preprocess and distribute a mass Acute Myeloid Leukaemia (AML) dataset[2] for 3-class classification problem for healthy, cytogenetically normal (CN), and core-binding factor translocation (CBF). For each cell, the dataset includes mass cytometry measurements of 16 markers for each cell. \n",
    "\n",
    "\n",
    "The dataset comprises mass cytometry measurements of 16 markers, as in original cellCnn[1] analysis, we use the AML samples on the AML samples with at least 10% CD34+ blast cells with availability of additional cytogenetic information. \n",
    "\n",
    "To run this example, \n",
    "\n",
    "    - download the [AML cell dataset] at https://imsb.ethz.ch/research/claassen/Software/cellcnn.html, under ALL dataset zip folder\n",
    "    - uncompress and place it in the data/cellCNN/ folder\n",
    "\n",
    "Data distribution: We fix the test set for all experimental settings, the training dataset is then generated by distribution different donors for each institution depending on number of institutions.\n",
    "\n",
    "[1] E. Arvaniti and M. Claassen. Sensitive detection of rare disease-associated cell subsets via representation learning.Nat Commun, 8:1–10, 2017\n",
    "[2] . Levine, E. Simonds, S. Bendall, K. Davis, E.-A. Amir, M. Tadmor, O. Litvin, H. Fienberg, A. Jager, E. Zunder, R. Finck, A. Gedman,I. Radtke, J. Downing, D. Pe’er, and G. Nolan. Data-driven phenotypic dissection of aml reveals progenitor-like cells that correlatewith prognosis.Cell, 162, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os, sys, errno, glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cellCNN_utils  \n",
    "import sklearn.utils as sku\n",
    "from cellCNN_utils import loadFCS, ftrans, mkdir_p, get_items, generate_data, generate_normalized_data\n",
    "from pathlib import Path\n",
    "%pylab inline\n",
    "\n",
    "rand_seed = 12345\n",
    "np.random.seed(rand_seed)\n",
    "stim = 'AML'\n",
    "\n",
    "d = Path().resolve()\n",
    "sys.path.append(d)\n",
    "\n",
    "# WDIR = os.path.join(cellCnn.__path__[0], 'examples')\n",
    "OUTDIR = os.path.join(d, 'output_%s' % stim)\n",
    "mkdir_p(OUTDIR)\n",
    "\n",
    "LOOKUP_PATH = os.path.join(d, 'AML.pkl')\n",
    "\n",
    "with open(LOOKUP_PATH, 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin'\n",
    "    lookup = u.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ntrain_all = 3000\n",
    "\n",
    "#patients SJ10, SJ12, SJ13 were characterized as CN\n",
    "#patients SJ1, SJ2, SJ3, SJ4, SJ5 presented CBF\n",
    "labels = ['CD19', 'CD11b', 'CD34', 'CD45', 'CD123', 'CD33', 'CD47', 'CD7', 'CD15', 'CD44', 'CD38', 'CD3', 'CD117', 'HLA-DR', 'CD64', 'CD41']\n",
    "trainCN, trainCBF, trainHealthy = [], [],[]\n",
    "testCN, testCBF, testHealthy = [], [],[]\n",
    "for key, val in lookup.items():\n",
    "    if \"SJ\" not in key: \n",
    "        if key == \"healthy_BM\":\n",
    "            trainHealthy.append(val[0][1])\n",
    "            trainHealthy.append(val[1][1])\n",
    "            trainHealthy.append(val[2][1])\n",
    "            testHealthy.append(val[3][1])\n",
    "            testHealthy.append(val[4][1])\n",
    "\n",
    "    if key == 'SJ10' or key == 'SJ12':\n",
    "        trainCN.append(val)\n",
    "    if key == 'SJ13':\n",
    "        testCN.append(val)\n",
    "    if key == 'SJ1' or key == 'SJ2': \n",
    "        trainCBF.append(val)\n",
    "    if key == 'SJ3' or key == 'SJ4' or key == 'SJ5':\n",
    "        testCBF.append(val)\n",
    "\n",
    "test_samples = testCN+ testCBF+ testHealthy\n",
    "test_phenotypes = [1,2,2,2,0,0] \n",
    "\n",
    "train_phenotypes = [0, 1 ,2]  #healthy, cn, cbf\n",
    "\n",
    "x_trainHealthy = sku.shuffle(np.vstack(trainHealthy))\n",
    "x_trainCN = sku.shuffle(np.vstack(trainCN))\n",
    "x_trainCBF = sku.shuffle(np.vstack(trainCBF))\n",
    "\n",
    "train_samples = [x_trainHealthy,x_trainCN, x_trainCBF]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate original data (with transform)\n",
    "In the following, \n",
    "- We generate training data with $ncell=200$ cells per sample and $nsubset=500$ samples per class\n",
    "- We generate the test data for $ncell=200$ cells per sample from test indices, called X_test\n",
    "- We generate another test set 'per-individual' in test indices using maximum number of cells to use for phenotype prediction, called X_test_all\n",
    "\n",
    "Processed data is placed under originalAML/ folder\n",
    "\n",
    "The script prints the max number of cells for the current example (i.e., 12440 for this dataset) which then will be used as a parameter in the golang protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "Predictions based on multi-cell inputs containing 12440 cells.\n",
      "(6, 12440, 16)\n",
      "(1498, 16, 200)\n",
      "(1498,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "scaler,x_tr,y_tr,x_test,y_test = generate_data(train_samples, train_phenotypes, 'originalAML/', \n",
    "                                               valid_samples=test_samples, valid_phenotypes=test_phenotypes, ncell=200, nsubset=500, verbose=0, saveFile=True)\n",
    "\n",
    "#generate also the test set on full ncell per sample:\n",
    "def generate_for_pheno_prediction(new_samples,phenotypes,scaler):\n",
    "        ncell_per_sample = np.min([x.shape[0] for x in new_samples])\n",
    "        print(f\"Predictions based on multi-cell inputs containing {ncell_per_sample} cells.\")\n",
    "        nmark = len(new_samples[0][1])\n",
    "        # z-transform the new samples if we did that for the training samples\n",
    "        if scaler is not None:\n",
    "            new_samples = [scaler.transform(x) for x in new_samples]\n",
    "        new_samples = [shuffle(x)[:ncell_per_sample].reshape(1, ncell_per_sample, nmark)\n",
    "                           for x in new_samples]\n",
    "        data_test = np.vstack(new_samples)\n",
    "        mkdir_p('originalAML/X_test_all/')\n",
    "        for i in range(len(data_test)):\n",
    "            np.savetxt('originalAML/' + 'X_test_all/' + str(i) +'.txt', (transpose(data_test[i])))\n",
    "        np.savetxt('originalAML/' + 'y_test_all.txt', phenotypes)\n",
    "        return data_test,phenotypes\n",
    "\n",
    "data_test,phenotypes=generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "print(shape(data_test))\n",
    "print(shape(x_test))\n",
    "print(shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate  data split between $nhosts$ parties\n",
    "In the following, \n",
    "- We generate training data with $ncell=200$ cells per sample and $nsubset=700$ samples per class, per party\n",
    "- Example below distributes the train indices per donor for 2 parties\n",
    "\n",
    "Processed data is placed under splitAML/host_i for party-i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global train splitted among hosts - indices:\n",
      "[[0], [1], [2]]\n",
      "[[0], [1], []]\n",
      "[[1], [], [0]]\n",
      "\n",
      "Host no. 0 :\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 1 :\n",
      "[1]\n",
      "[1]\n",
      "[]\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# set random seed for reproducible results\n",
    "# np.random.seed(1234)\n",
    "# Here we randomly split the samples in training/test sets.\n",
    "nhosts= 2\n",
    "\n",
    "\n",
    "#distribute train indices balanced among n hosts:\n",
    "numH = [0,1,2] #training set healthy sample indices\n",
    "numCN = [0,1] #training set CN indices\n",
    "numCBF = [0,1] #training set CBF indices\n",
    "\n",
    "group1_list = np.flip(np.array_split(numpy.array(numH), nhosts))\n",
    "group2_list = numpy.array_split(numpy.array(numCN), nhosts)\n",
    "group3_list = numpy.array_split(numpy.array(numCBF), nhosts)\n",
    "\n",
    "split_idx_1 = []\n",
    "split_idx_2 = []\n",
    "split_idx_3 = []\n",
    "for i in range(nhosts):\n",
    "    split_idx_1.append(group1_list[i].tolist())\n",
    "    split_idx_2.append(group2_list[i].tolist())\n",
    "    split_idx_3.append(group3_list[i].tolist())\n",
    "\n",
    "print(\"Global train splitted among hosts - indices:\")\n",
    "\n",
    "#make sure each client gets at least one patient, comment in the following part to shuffle otherwise\n",
    "random.shuffle(split_idx_1)\n",
    "random.shuffle(split_idx_2)\n",
    "random.shuffle(split_idx_3)\n",
    "\n",
    "#OR manually distribute indices\n",
    "split_idx_1 = [[0], [1], [2]]\n",
    "split_idx_2 = [[0], [1], []]\n",
    "split_idx_3 = [[1], [], [0]]\n",
    "\n",
    "print(split_idx_1)\n",
    "print(split_idx_2)\n",
    "print(split_idx_3)\n",
    "for i in range(nhosts):\n",
    "    print(\"\\nHost no.\", i, \":\")\n",
    "    folder_path = 'splitAML/host' + str(i) + '/'\n",
    "    trainHealthyidx = split_idx_1[i]\n",
    "    trainCNTempidx = split_idx_2[i]\n",
    "    trainCBFTempidx = split_idx_3[i]\n",
    "    print(trainHealthyidx)\n",
    "    print(trainCNTempidx)\n",
    "    print(trainCBFTempidx)\n",
    "    trainHealthyTemp,trainCNTemp,trainCBFTemp = [],[],[]\n",
    "    for idx in trainHealthyidx:\n",
    "        trainHealthyTemp.append(trainHealthy[idx])\n",
    "    for idx in trainCNTempidx:\n",
    "        trainCNTemp.append(trainCN[idx])\n",
    "    for idx in trainCBFTempidx:\n",
    "        trainCBFTemp.append(trainCBF[idx])\n",
    "    train_phenotypes = []\n",
    "    train_samples =[]\n",
    "\n",
    "     # load the training samples\n",
    "    if len(trainHealthyTemp) != 0:\n",
    "        x_trainHealthy = sku.shuffle(np.vstack(trainHealthyTemp))\n",
    "        train_phenotypes.append(0)\n",
    "    if len(trainCNTemp) != 0:\n",
    "        x_trainCN = sku.shuffle(np.vstack(trainCNTemp))\n",
    "        train_phenotypes.append(1)\n",
    "    if len(trainCBFTemp) != 0:\n",
    "        x_trainCBF = sku.shuffle(np.vstack(trainCBFTemp))\n",
    "        train_phenotypes.append(2)\n",
    "    train_samples = [x_trainHealthy] + [x_trainCN]+ [x_trainCBF]\n",
    "    generate_data(train_samples, train_phenotypes, folder_path, ncell=200, nsubset=ntrain_all/(3*nhosts), verbose=0,generate_valid_set=False,per_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1500\n",
      "200\n",
      "16\n",
      "Epoch 1/20\n",
      "8/8 - 1s - loss: 1.0444 - accuracy: 0.8940\n",
      "Epoch 2/20\n",
      "8/8 - 0s - loss: 0.7756 - accuracy: 0.9867\n",
      "Epoch 3/20\n",
      "8/8 - 0s - loss: 0.4694 - accuracy: 0.9133\n",
      "Epoch 4/20\n",
      "8/8 - 0s - loss: 0.2887 - accuracy: 0.9987\n",
      "Epoch 5/20\n",
      "8/8 - 0s - loss: 0.1654 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "8/8 - 0s - loss: 0.0933 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "8/8 - 0s - loss: 0.0546 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "8/8 - 0s - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "8/8 - 0s - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "8/8 - 0s - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "8/8 - 0s - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "8/8 - 0s - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "8/8 - 0s - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 - 0s - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 - 0s - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 - 0s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 - 0s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 - 0s - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 - 0s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 - 0s - loss: 0.0064 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#The reproduction of original CellCnn model training without validation test and further analysis part\n",
    "#This part is used for the comparison of accuracy/precision/recall/f-score of CellCnn with secure distributed version\n",
    "#test on 200-cell multi-instances and full test set phenotype prediction\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, initializers, regularizers, optimizers, callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "#repeat cellcnn original training on full train data\n",
    "\n",
    "k = 200\n",
    "ncell = 200\n",
    "nfilter = 8\n",
    "y_tr_n = to_categorical(y_tr)\n",
    "\n",
    "def pool_top_k(x, k):\n",
    "    return tf.reduce_mean(tf.sort(x, axis=1, direction='DESCENDING')[:, :k, :], axis=1)\n",
    "def create_model (k,ncell,nfilter):\n",
    "        \n",
    "        data_input = keras.Input(shape=(ncell, 16))\n",
    "        coeff_l1=0\n",
    "        coeff_l2=1e-4\n",
    "        n_classes=3\n",
    "        # the filters\n",
    "        conv = layers.Conv1D(filters=nfilter,\n",
    "                             kernel_size=1,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer=initializers.RandomUniform(),\n",
    "                             kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                             name='conv1')(data_input)\n",
    "\n",
    "        # the cell grouping part (top-k pooling)\n",
    "        pooled = layers.Lambda(pool_top_k, output_shape=(nfilter,), arguments={'k': k})(conv)\n",
    "        output = layers.Dense(units=n_classes,\n",
    "                                  activation='softmax',\n",
    "                                  kernel_initializer=initializers.RandomUniform(),\n",
    "                                  kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                                  name='output')(pooled)\n",
    "        model = keras.Model(inputs=data_input, outputs=output)\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "model = create_model(k,ncell,nfilter)\n",
    "#generate data\n",
    "\n",
    "x_tr_n = x_tr.transpose(0,2,1)\n",
    "# Fit data to model\n",
    "print(type(x_tr_n))\n",
    "print(len(x_tr_n))\n",
    "print(len(x_tr_n[0]))\n",
    "print(len(x_tr_n[1][0]))\n",
    "\n",
    "\n",
    "history = model.fit(x_tr_n, y_tr_n,\n",
    "            batch_size=200,\n",
    "            epochs=20,\n",
    "            verbose=2,\n",
    "            validation_split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 200-cell predictions on test set with size (1498, 200, 16)\n",
      "Test loss: 0.025039520114660263, Test accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "F-score: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "#For 200-cell predictions on test set\n",
    "def model_pred(prob):\n",
    "    pred = []\n",
    "    for p in prob:\n",
    "        max_value = max(p)\n",
    "        max_index =  np.where(p == max_value)\n",
    "        pred.append(max_index)\n",
    "    pred = np.array(pred)\n",
    "    pred=pred.reshape(len(prob))\n",
    "    return pred\n",
    "\n",
    "x_test_n = x_test.transpose(0,2,1)\n",
    "y_test_n = to_categorical(y_test)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "#score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "print(\"For 200-cell predictions on test set with size\",x_test_n.shape)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "y_pred = model.predict(x_test_n)\n",
    "y_pred = model_pred(y_pred)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F-score:\",f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"precision:\",precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"recall:\",recall_score(y_test, y_pred, average=\"macro\")) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "F-score: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "#For phenotype predictions on test set using all cells \n",
    "\n",
    "model2 = create_model(12440, 12440,8)\n",
    "weights = model.get_weights()\n",
    "model2.set_weights(weights)\n",
    "data_test_n = data_test.transpose(0,2,1)\n",
    "phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "y_pred = model2.predict(data_test)\n",
    "\n",
    "y_pred = model_pred(y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "print(\"F-score:\",f1_score(phenotypes, y_pred, average=\"macro\"))\n",
    "print(\"precision:\",precision_score(phenotypes, y_pred, average=\"macro\"))\n",
    "print(\"recall:\",recall_score(phenotypes, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global train splitted among hosts - indices:\n",
      "\n",
      "Host no. 0 :\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 1 :\n",
      "new scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 2 :\n",
      "new scale\n",
      "Generating multi-cell inputs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sav/GolandProjects/cellCNN/cellCNNClear/data/cellCNN/cellCNN_utils.py:644: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  nsubset_list.append(nsubset / np.sum(train_phenotypes == pheno))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-d1e8a472d1b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotalRun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m#split between n host\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocalSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnhosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     scaler,x_test,y_test = generate_data(test_samples, test_phenotypes, 'Flow/', generate_valid_set=False, \n\u001b[1;32m     79\u001b[0m                                                    \u001b[0mncell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-d1e8a472d1b5>\u001b[0m in \u001b[0;36mlocalSplit\u001b[0;34m(nhosts)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtrain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_trainHealthy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_trainCN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_trainCBF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         generate_data(train_samples, train_phenotypes, folder_path, ncell=200, nsubset=ntrain_all/(3*nhosts),\n\u001b[0;32m---> 56\u001b[0;31m                       verbose=0,generate_valid_set=False,per_sample=False)\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mxtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mytr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GolandProjects/cellCNN/cellCNNClear/data/cellCNN/cellCNN_utils.py\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m(train_samples, train_phenotypes, outdir, valid_samples, valid_phenotypes, generate_valid_set, scale, quant_normed, nrun, regression, ncell, nsubset, per_sample, subset_selection, maxpool_percentages, nfilter_choice, learning_rate, coeff_l1, coeff_l2, dropout, dropout_p, max_epochs, patience, dendrogram_cutoff, accur_thres, verbose, saveFile, scaler, oneFile, generateAsTest)\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mnsubset_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsubset\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_phenotypes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpheno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             X_tr, y_tr = generate_subsets(X_train, train_phenotypes, id_train,\n\u001b[0;32m--> 646\u001b[0;31m                                           nsubset_list, ncell, per_sample)\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgenerate_valid_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GolandProjects/cellCNN/cellCNNClear/data/cellCNN/cellCNN_utils.py\u001b[0m in \u001b[0;36mgenerate_subsets\u001b[0;34m(X, pheno_map, sample_id, nsubsets, ncell, per_sample, k_init)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnsubsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpheno_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m             \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper_sample_subsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;31m# mix them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# set random seed for reproducible results\n",
    "# np.random.seed(1234)\n",
    "# Here we randomly split the samples in training/test sets.\n",
    "def localSplit(nhosts):\n",
    "    #distribute train indices balanced among n hosts:\n",
    "    numH = [0,1,2] #training set healthy sample indices\n",
    "    numCN = [0,1] #training set CN indices\n",
    "    numCBF = [0,1] #training set CBF indices\n",
    "    group1_list = np.flip(np.array_split(numpy.array(numH), nhosts))\n",
    "    group2_list = numpy.array_split(numpy.array(numCN), nhosts)\n",
    "    group3_list = numpy.array_split(numpy.array(numCBF), nhosts)\n",
    "\n",
    "    split_idx_1 = []\n",
    "    split_idx_2 = []\n",
    "    split_idx_3 = []\n",
    "    for i in range(nhosts):\n",
    "        split_idx_1.append(group1_list[i].tolist())\n",
    "        split_idx_2.append(group2_list[i].tolist())\n",
    "        split_idx_3.append(group3_list[i].tolist())\n",
    "\n",
    "    print(\"Global train splitted among hosts - indices:\")\n",
    "\n",
    "    #or manually distribute indices\n",
    "    split_idx_1 = [[0], [1], [2]]\n",
    "    split_idx_2 = [[0], [1], []]\n",
    "    split_idx_3 = [[1], [], [0]]\n",
    "    xtr=[]\n",
    "    ytr=[]\n",
    "    for i in range(nhosts):\n",
    "        print(\"\\nHost no.\", i, \":\")\n",
    "        folder_path = 'splitAML/host' + str(i) + '/'\n",
    "        trainHealthyidx = split_idx_1[i]\n",
    "        trainCNTempidx = split_idx_2[i]\n",
    "        trainCBFTempidx = split_idx_3[i]\n",
    "        trainHealthyTemp,trainCNTemp,trainCBFTemp = [],[],[]\n",
    "        for idx in trainHealthyidx:\n",
    "            trainHealthyTemp.append(trainHealthy[idx])\n",
    "        for idx in trainCNTempidx:\n",
    "            trainCNTemp.append(trainCN[idx])\n",
    "        for idx in trainCBFTempidx:\n",
    "            trainCBFTemp.append(trainCBF[idx])\n",
    "        train_phenotypes = []\n",
    "        train_samples =[]\n",
    "         # load the training samples\n",
    "        if len(trainHealthyTemp) != 0:\n",
    "            x_trainHealthy = sku.shuffle(np.vstack(trainHealthyTemp))\n",
    "            train_phenotypes.append(0)\n",
    "        if len(trainCNTemp) != 0:\n",
    "            x_trainCN = sku.shuffle(np.vstack(trainCNTemp))\n",
    "            train_phenotypes.append(1)\n",
    "        if len(trainCBFTemp) != 0:\n",
    "            x_trainCBF = sku.shuffle(np.vstack(trainCBFTemp))\n",
    "            train_phenotypes.append(2)\n",
    "        train_samples = [x_trainHealthy] + [x_trainCN]+ [x_trainCBF]\n",
    "        generate_data(train_samples, train_phenotypes, folder_path, ncell=200, nsubset=ntrain_all/(3*nhosts),\n",
    "                      verbose=0,generate_valid_set=False,per_sample=False)\n",
    "        xtr.append(x_tr)\n",
    "        ytr.append(y_tr)\n",
    "    return xtr,ytr,scaler\n",
    "\n",
    "#To average LOCAL computations metrics\n",
    "nhosts = 3\n",
    "totalRun = 10\n",
    "ncells=200\n",
    "k=200\n",
    "accurMulti = np.empty([nhosts, totalRun])\n",
    "precMulti = np.empty([nhosts, totalRun])\n",
    "recallMulti = np.empty([nhosts, totalRun])\n",
    "fscoreMulti = np.empty([nhosts, totalRun])\n",
    "accurPheno = np.empty([nhosts, totalRun])\n",
    "precPheno = np.empty([nhosts, totalRun])\n",
    "recallPheno = np.empty([nhosts, totalRun])\n",
    "fscorePheno = np.empty([nhosts, totalRun])\n",
    "\n",
    "for run in range(totalRun):\n",
    "    #split between n host\n",
    "    xtr,ytr,scaler = localSplit(nhosts)\n",
    "    scaler,x_test,y_test = generate_data(test_samples, test_phenotypes, 'Flow/', generate_valid_set=False, \n",
    "                                                   ncell=200, nsubset=500, scale=True, \n",
    "                                                   per_sample=False, verbose=0, saveFile=False,\n",
    "                                                   subset_selection = 'random', generateAsTest=True,scaler=scaler)\n",
    "    \n",
    "    data_test,phenotypes = generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "\n",
    "\n",
    "    for i in range(nhosts):\n",
    "        y_tr_n = to_categorical(ytr[i])\n",
    "        model = create_model(ncells,ncells,nfilter)\n",
    "        #generate data\n",
    "\n",
    "        x_tr_n = xtr[i].transpose(0,2,1)\n",
    "        # Fit data to model\n",
    "        history = model.fit(x_tr_n, y_tr_n,\n",
    "                    batch_size=64,\n",
    "                    epochs=30,\n",
    "                    verbose=2,\n",
    "                    validation_split=0)\n",
    "\n",
    "        #test for multi-cell\n",
    "        x_test_n = x_test.transpose(0,2,1)\n",
    "#         x_test_n = x_test_n[0:ntest_all,:]\n",
    "\n",
    "        y_test_n = to_categorical(y_test)\n",
    "#         y_test_n = y_test_n[0:ntest_all,:]\n",
    "        loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "        #score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "        print(\"For 100-cell predictions on test set with size\",x_test_n.shape)\n",
    "        print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "        y_pred = model.predict(x_test_n)\n",
    "        y_pred = model_pred(y_pred)\n",
    "#         y_test= y_test[0:ntest_all]\n",
    "\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"F-score:\",f1_score(y_test, y_pred, average=\"macro\"))\n",
    "        print(\"precision:\",precision_score(y_test, y_pred, average=\"macro\"))\n",
    "        print(\"recall:\",recall_score(y_test, y_pred, average=\"macro\") )\n",
    "\n",
    "        #to write to excell sheet...\n",
    "        accurMulti[i,run] = accuracy_score(y_test, y_pred)\n",
    "        precMulti[i,run] = precision_score(y_test, y_pred, average=\"macro\")\n",
    "        recallMulti[i,run]= recall_score(y_test, y_pred, average=\"macro\")\n",
    "        fscoreMulti[i,run]= f1_score(y_test, y_pred, average=\"macro\")\n",
    "        #For phenotype predictions on test set using all cells \n",
    "\n",
    "        model2 = create_model(12440, 12440,nfilter)\n",
    "        weights = model.get_weights()\n",
    "        model2.set_weights(weights)\n",
    "        data_test_n = data_test\n",
    "        phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "        y_pred = model2.predict(data_test_n)\n",
    "        y_pred = model_pred(y_pred)\n",
    "        # print(y_pred)\n",
    "        print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "        print(\"F-score:\",f1_score(phenotypes, y_pred, average=\"macro\"))\n",
    "        print(\"precision:\",precision_score(phenotypes, y_pred, average=\"macro\"))\n",
    "        print(\"recall:\",recall_score(phenotypes, y_pred, average=\"macro\") )\n",
    "        \n",
    "        accurPheno[i,run] = accuracy_score(phenotypes, y_pred)\n",
    "        precPheno[i,run] = precision_score(phenotypes, y_pred, average=\"macro\")\n",
    "        recallPheno[i,run]= recall_score(phenotypes, y_pred, average=\"macro\")\n",
    "        fscorePheno[i,run]= f1_score(phenotypes, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
