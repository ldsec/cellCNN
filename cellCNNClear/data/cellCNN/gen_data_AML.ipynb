{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CellCnn [1] data generation\n",
    "\n",
    "source: https://github.com/eiriniar/CellCnn\n",
    "\n",
    "\"\"\" Copyright 2016-2017 ETH Zurich, Eirini Arvaniti and Manfred Claassen.\n",
    "\n",
    "This module contains data preprocessing/distribution functions.\n",
    "\n",
    "\"\"\"\n",
    "The code is slightly changed depending on original implementation to make it compatible with decentralized settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we preprocess and distribute a mass Acute Myeloid Leukaemia (AML) dataset[2] for 3-class classification problem for healthy, cytogenetically normal (CN), and core-binding factor translocation (CBF). For each cell, the dataset includes mass cytometry measurements of 16 markers for each cell. \n",
    "\n",
    "\n",
    "The dataset comprises mass cytometry measurements of 16 markers, as in original cellCnn[1] analysis, we use the AML samples on the AML samples with at least 10% CD34+ blast cells with availability of additional cytogenetic information. \n",
    "\n",
    "To run this example, \n",
    "\n",
    "    - download the [AML cell dataset] at https://imsb.ethz.ch/research/claassen/Software/cellcnn.html, under ALL dataset zip folder\n",
    "    - uncompress and place it in the data/cellCNN/ folder\n",
    "\n",
    "Data distribution: We fix the test set for all experimental settings, the training dataset is then generated by distribution different donors for each institution depending on number of institutions.\n",
    "\n",
    "[1] E. Arvaniti and M. Claassen. Sensitive detection of rare disease-associated cell subsets via representation learning.Nat Commun, 8:1–10, 2017\n",
    "[2] . Levine, E. Simonds, S. Bendall, K. Davis, E.-A. Amir, M. Tadmor, O. Litvin, H. Fienberg, A. Jager, E. Zunder, R. Finck, A. Gedman,I. Radtke, J. Downing, D. Pe’er, and G. Nolan. Data-driven phenotypic dissection of aml reveals progenitor-like cells that correlatewith prognosis.Cell, 162, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, errno, glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cellCNN_utils  \n",
    "import sklearn.utils as sku\n",
    "from cellCNN_utils import loadFCS, ftrans, mkdir_p, get_items, generate_data, generate_normalized_data\n",
    "from pathlib import Path\n",
    "%pylab inline\n",
    "\n",
    "rand_seed = 12345\n",
    "np.random.seed(rand_seed)\n",
    "stim = 'AML'\n",
    "\n",
    "d = Path().resolve()\n",
    "sys.path.append(d)\n",
    "\n",
    "# WDIR = os.path.join(cellCnn.__path__[0], 'examples')\n",
    "OUTDIR = os.path.join(d, 'output_%s' % stim)\n",
    "mkdir_p(OUTDIR)\n",
    "\n",
    "LOOKUP_PATH = os.path.join(d, 'AML.pkl')\n",
    "\n",
    "with open(LOOKUP_PATH, 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin'\n",
    "    lookup = u.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patients SJ10, SJ12, SJ13 were characterized as CN\n",
    "#patients SJ1, SJ2, SJ3, SJ4, SJ5 presented CBF\n",
    "labels = ['CD19', 'CD11b', 'CD34', 'CD45', 'CD123', 'CD33', 'CD47', 'CD7', 'CD15', 'CD44', 'CD38', 'CD3', 'CD117', 'HLA-DR', 'CD64', 'CD41']\n",
    "trainCN, trainCBF, trainHealthy = [], [],[]\n",
    "testCN, testCBF, testHealthy = [], [],[]\n",
    "for key, val in lookup.items():\n",
    "    if \"SJ\" not in key: \n",
    "        if key == \"healthy_BM\":\n",
    "            trainHealthy.append(val[0][1])\n",
    "            trainHealthy.append(val[1][1])\n",
    "            trainHealthy.append(val[2][1])\n",
    "            testHealthy.append(val[3][1])\n",
    "            testHealthy.append(val[4][1])\n",
    "\n",
    "    if key == 'SJ10' or key == 'SJ12':\n",
    "        trainCN.append(val)\n",
    "    if key == 'SJ13':\n",
    "        testCN.append(val)\n",
    "    if key == 'SJ1' or key == 'SJ2': \n",
    "        trainCBF.append(val)\n",
    "    if key == 'SJ3' or key == 'SJ4' or key == 'SJ5':\n",
    "        testCBF.append(val)\n",
    "\n",
    "test_samples = testCN+ testCBF+ testHealthy\n",
    "test_phenotypes = [1,2,2,2,0,0 ] \n",
    "\n",
    "train_phenotypes = [0, 1 ,2]  #healthy, cn, cbf\n",
    "\n",
    "x_trainHealthy = sku.shuffle(np.vstack(trainHealthy))\n",
    "x_trainCN = sku.shuffle(np.vstack(trainCN))\n",
    "x_trainCBF = sku.shuffle(np.vstack(trainCBF))\n",
    "\n",
    "train_samples = [x_trainHealthy,x_trainCN, x_trainCBF]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate original data (with transform)\n",
    "In the following, \n",
    "- We generate training data with $ncell=200$ cells per sample and $nsubset=500$ samples per class\n",
    "- We generate the test data for $ncell=200$ cells per sample from test indices, called X_test\n",
    "- We generate another test set 'per-individual' in test indices using maximum number of cells to use for phenotype prediction, called X_test_all\n",
    "\n",
    "Processed data is placed under originalAML/ folder\n",
    "\n",
    "The script prints the max number of cells for the current example (i.e., 12440 for this dataset) which then will be used as a parameter in the golang protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "scaler,x_tr,y_tr,x_test,y_test = generate_data(train_samples, train_phenotypes, 'originalAML/', valid_samples=test_samples, valid_phenotypes=test_phenotypes, ncell=200, nsubset=500, verbose=0, saveFile=True)\n",
    "\n",
    "#generate also the test set on full ncell per sample:\n",
    "def generate_for_pheno_prediction(new_samples,phenotypes,scaler):\n",
    "        ncell_per_sample = np.min([x.shape[0] for x in new_samples])\n",
    "        print(f\"Predictions based on multi-cell inputs containing {ncell_per_sample} cells.\")\n",
    "        nmark = len(new_samples[0][1])\n",
    "        # z-transform the new samples if we did that for the training samples\n",
    "        if scaler is not None:\n",
    "            new_samples = [scaler.transform(x) for x in new_samples]\n",
    "        new_samples = [shuffle(x)[:ncell_per_sample].reshape(1, ncell_per_sample, nmark)\n",
    "                           for x in new_samples]\n",
    "        data_test = np.vstack(new_samples)\n",
    "        mkdir_p('originalAML/X_test_all/')\n",
    "        for i in range(len(data_test)):\n",
    "            np.savetxt('originalAML/' + 'X_test_all/' + str(i) +'.txt', (transpose(data_test[i])))\n",
    "        np.savetxt('originalAML/' + 'y_test_all.txt', phenotypes)\n",
    "        return data_test,phenotypes\n",
    "\n",
    "data_test,phenotypes=generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "print(shape(data_test))\n",
    "print(shape(x_test))\n",
    "print(shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate  data split between $nhosts$ parties\n",
    "In the following, \n",
    "- We generate training data with $ncell=200$ cells per sample and $nsubset=700$ samples per class, per party\n",
    "- Example below distributes the train indices per donor for 2 parties\n",
    "\n",
    "Processed data is placed under splitAML/host_i for party-i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproducible results\n",
    "np.random.seed(1234)\n",
    "# Here we randomly split the samples in training/test sets.\n",
    "nhosts= 3\n",
    "\n",
    "\n",
    "#distribute train indices balanced among n hosts:\n",
    "numH = [0,1,2] #training set healthy sample indices\n",
    "numCN = [0,1] #training set CN indices\n",
    "numCBF = [0,1] #training set CBF indices\n",
    "group1_list = np.flip(np.array_split(numpy.array(numH), nhosts))\n",
    "group2_list = numpy.array_split(numpy.array(numCN), nhosts)\n",
    "group3_list = numpy.array_split(numpy.array(numCBF), nhosts)\n",
    "\n",
    "split_idx_1 = []\n",
    "split_idx_2 = []\n",
    "split_idx_3 = []\n",
    "for i in range(nhosts):\n",
    "    split_idx_1.append(group1_list[i].tolist())\n",
    "    split_idx_2.append(group2_list[i].tolist())\n",
    "    split_idx_3.append(group3_list[i].tolist())\n",
    "\n",
    "\n",
    "print(\"Global train splitted among hosts - indices:\")\n",
    "\n",
    "#make sure each client gets at least one patient, comment in the following part to shuffle otherwise\n",
    "# random.shuffle(split_idx_1)\n",
    "# random.shuffle(split_idx_2)\n",
    "# random.shuffle(split_idx_3)\n",
    "\n",
    "#or manually distribute indices\n",
    "split_idx_1 = [[0], [1], [2]]\n",
    "split_idx_2 = [[0], [1], []]\n",
    "split_idx_3 = [[1], [], [0]]\n",
    "print(split_idx_1)\n",
    "print(split_idx_2)\n",
    "print(split_idx_3)\n",
    "for i in range(nhosts):\n",
    "    print(\"\\nHost no.\", i, \":\")\n",
    "    folder_path = 'splitAML/host' + str(i) + '/'\n",
    "    trainHealthyidx = split_idx_1[i]\n",
    "    trainCNTempidx = split_idx_2[i]\n",
    "    trainCBFTempidx = split_idx_3[i]\n",
    "    trainHealthyTemp,trainCNTemp,trainCBFTemp = [],[],[]\n",
    "    for idx in trainHealthyidx:\n",
    "        trainHealthyTemp.append(trainHealthy[idx])\n",
    "    for idx in trainCNTempidx:\n",
    "        trainCNTemp.append(trainCN[idx])\n",
    "    for idx in trainCBFTempidx:\n",
    "        trainCBFTemp.append(trainCBF[idx])\n",
    "    train_phenotypes = []\n",
    "    train_samples =[]\n",
    "     # load the training samples\n",
    "    if len(trainHealthyTemp) != 0:\n",
    "        x_trainHealthy = sku.shuffle(np.vstack(trainHealthyTemp))\n",
    "        train_phenotypes.append(0)\n",
    "    if len(trainCNTemp) != 0:\n",
    "        x_trainCN = sku.shuffle(np.vstack(trainCNTemp))\n",
    "        train_phenotypes.append(1)\n",
    "    if len(trainCBFTemp) != 0:\n",
    "        x_trainCBF = sku.shuffle(np.vstack(trainCBFTemp))\n",
    "        train_phenotypes.append(2)\n",
    "    train_samples = [x_trainHealthy] + [x_trainCN]+ [x_trainCBF]\n",
    "    generate_data(train_samples, train_phenotypes, folder_path, ncell=200, nsubset=160, verbose=0,generate_valid_set=False,per_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The reproduction of original CellCnn model training without validation test and further analysis part\n",
    "#This part is used for the comparison of accuracy/precision/recall/f-score of CellCnn with secure distributed version\n",
    "#test on 200-cell multi-instances and full test set phenotype prediction\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, initializers, regularizers, optimizers, callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "#repeat cellcnn original training on full train data\n",
    "\n",
    "k = 200\n",
    "ncell = 200\n",
    "nfilter = 8\n",
    "y_tr_n = to_categorical(y_tr)\n",
    "\n",
    "def pool_top_k(x, k):\n",
    "    return tf.reduce_mean(tf.sort(x, axis=1, direction='DESCENDING')[:, :k, :], axis=1)\n",
    "def create_model (k,ncell,nfilter):\n",
    "        \n",
    "        data_input = keras.Input(shape=(ncell, 16))\n",
    "        coeff_l1=0\n",
    "        coeff_l2=1e-4\n",
    "        n_classes=3\n",
    "        # the filters\n",
    "        conv = layers.Conv1D(filters=nfilter,\n",
    "                             kernel_size=1,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer=initializers.RandomUniform(),\n",
    "                             kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                             name='conv1')(data_input)\n",
    "\n",
    "        # the cell grouping part (top-k pooling)\n",
    "        pooled = layers.Lambda(pool_top_k, output_shape=(nfilter,), arguments={'k': k})(conv)\n",
    "        output = layers.Dense(units=n_classes,\n",
    "                                  activation='softmax',\n",
    "                                  kernel_initializer=initializers.RandomUniform(),\n",
    "                                  kernel_regularizer=regularizers.l1_l2(l1=coeff_l1, l2=coeff_l2),\n",
    "                                  name='output')(pooled)\n",
    "        model = keras.Model(inputs=data_input, outputs=output)\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "        return model\n",
    "model = create_model(k,ncell,nfilter)\n",
    "#generate data\n",
    "\n",
    "x_tr_n = x_tr.transpose(0,2,1)\n",
    "# Fit data to model\n",
    "print(type(x_tr_n))\n",
    "print(len(x_tr_n))\n",
    "print(len(x_tr_n[0]))\n",
    "print(len(x_tr_n[1][0]))\n",
    "\n",
    "\n",
    "history = model.fit(x_tr_n, y_tr_n,\n",
    "            batch_size=200,\n",
    "            epochs=20,\n",
    "            verbose=2,\n",
    "            validation_split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 200-cell predictions on test set\n",
    "def model_pred(prob):\n",
    "    pred = []\n",
    "    for p in prob:\n",
    "        max_value = max(p)\n",
    "        max_index =  np.where(p == max_value)\n",
    "        pred.append(max_index)\n",
    "    pred = np.array(pred)\n",
    "    pred=pred.reshape(len(prob))\n",
    "    return pred\n",
    "\n",
    "x_test_n = x_test.transpose(0,2,1)\n",
    "y_test_n = to_categorical(y_test)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "#score = model.evaluate(x_test_n, y_test_n, verbose=0)\n",
    "print(\"For 200-cell predictions on test set with size\",x_test_n.shape)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "y_pred = model.predict(x_test_n)\n",
    "y_pred = model_pred(y_pred)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F-score:\",f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"precision:\",precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"recall:\",recall_score(y_test, y_pred, average=\"macro\")) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For phenotype predictions on test set using all cells \n",
    "\n",
    "model2 = create_model(12440, 12440,8)\n",
    "weights = model.get_weights()\n",
    "model2.set_weights(weights)\n",
    "data_test_n = data_test.transpose(0,2,1)\n",
    "phenotypes_n = to_categorical(phenotypes)\n",
    "\n",
    "y_pred = model2.predict(data_test)\n",
    "\n",
    "y_pred = model_pred(y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print(\"Accuracy:\", accuracy_score(phenotypes, y_pred))\n",
    "print(\"F-score:\",f1_score(phenotypes, y_pred, average=\"macro\"))\n",
    "print(\"precision:\",precision_score(phenotypes, y_pred, average=\"macro\"))\n",
    "print(\"recall:\",recall_score(phenotypes, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
