{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellCNN data generation\n",
    "\n",
    "source: https://github.com/eiriniar/CellCnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use CellCnn to analyze a mass cytometry dataset acquired to characterize human natural killer (NK) cell diversity and associate NK cell subsets with genetic and environmental factors, namely prior Cytomegalovirus (CMV) infection [1]. This dataset comprises mass cytometry measurements of 36 markers, including 28 NK cell receptors, for PBMC samples of 20 donors with varying serology for CMV. \n",
    "\n",
    "We will train CellCnn to identify CMV seropositivity-associated cell populations within the **manually gated NK cell compartment**. To run this example, please download the [NK cell dataset](http://www.imsb.ethz.ch/research/claassen/Software/cellcnn.html) and place the decompressed folder in the cellCnn/examples directory.\n",
    "\n",
    "[1] Horowitz, A. et al. Genetic and environmental determinants of human NK cell diversity revealed by mass cytometry. Sci. Transl. Med. 5 (2013).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install FlowIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os, sys, errno, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cellCNN_utils  \n",
    "from cellCNN_utils import loadFCS, ftrans, mkdir_p, get_items, generate_data, generate_normalized_data\n",
    "sys.path.append('/Users/sav/Desktop/POSEIDON/CellCnn-python3')\n",
    "import cellCnn\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input and output directories\n",
    "WDIR = os.path.join(cellCnn.__path__[0], 'examples')\n",
    "FCS_DATA_PATH = os.path.join(WDIR, 'NK_cell_dataset', 'gated_NK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'Cell_length', 'CD3', 'Dead', '(La139)Dd', 'CD27', 'CD19', 'CD4', 'CD8', 'CD57', '2DL1-S1', 'TRAIL', '2DL2-L3-S2', 'CD16', 'CD10', '3DL1-S1', 'CD117', '2DS4', 'ILT2-CD85j', 'NKp46', 'NKG2D', 'NKG2C', '2B4', 'CD33', 'CD11b', 'NKp30', 'CD122', '3DL1', 'NKp44', 'CD127', '2DL1', 'CD94', 'CD34', 'CCR7', '2DL3', 'NKG2A', 'HLA-DR', '2DL4', 'CD56', '2DL5', 'CD25', 'DNA1', 'DNA2']\n",
      "(43,)\n"
     ]
    }
   ],
   "source": [
    "# look at the measured markers\n",
    "data_fcs = loadFCS(glob.glob(FCS_DATA_PATH + '/*.fcs')[0], transform=None, auto_comp=False)\n",
    "print(data_fcs.channels)\n",
    "print(shape(data_fcs.channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant markers for further analysis\n",
    "markers = ['CD3', 'CD27', 'CD19', 'CD4', 'CD8', 'CD57', '2DL1-S1', 'TRAIL', '2DL2-L3-S2',\n",
    "           'CD16', 'CD10', '3DL1-S1', 'CD117', '2DS4', 'ILT2-CD85j', 'NKp46', 'NKG2D',\n",
    "           'NKG2C', '2B4', 'CD33', 'CD11b', 'NKp30', 'CD122', '3DL1', 'NKp44', 'CD127', '2DL1',\n",
    "           'CD94', 'CD34', 'CCR7', '2DL3', 'NKG2A', 'HLA-DR', '2DL4', 'CD56', '2DL5', 'CD25']\n",
    "marker_idx = [data_fcs.channels.index(label) for label in markers]\n",
    "nmark = len(markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a_001_NK.fcs' 'a_002_NK.fcs' 'a_003_NK.fcs' 'a_004_NK.fcs'\n",
      " 'a_005_NK.fcs' 'a_006_NK.fcs' 'a_007_NK.fcs' 'a_009_NK.fcs'\n",
      " 'a_010_NK.fcs' 'a_011_NK.fcs' 'a_012_NK.fcs' 'a_1a_NK.fcs' 'a_2a_NK.fcs'\n",
      " 'a_2b_NK.fcs' 'a_3a_NK.fcs' 'a_3b_NK.fcs' 'a_4a_NK.fcs' 'a_4b_NK.fcs'\n",
      " 'a_5a_NK.fcs' 'a_5b_NK.fcs']\n",
      "[1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# load the sample names and corresponding labels (0: CMV-, 1: CMV+), here from a CSV file\n",
    "# prior CMV infection status is obtained from the original study (Horowitz et al. 2013)\n",
    "csv_file = 'NK_fcs_samples_with_labels.csv'\n",
    "fcs_info = np.array(pd.read_csv(csv_file, sep=','))\n",
    "sample_ids = fcs_info[:, 0]\n",
    "sample_labels = fcs_info[:, 1].astype(int)\n",
    "print(sample_ids)\n",
    "print((sample_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we randomly split the samples in training/test sets.\n",
    "\n",
    "def train_test_split(train_idx1=[], train_idx2=[], test=True):\n",
    "    # set random seed for reproducible results\n",
    "    np.random.seed(12345)\n",
    "\n",
    "    # cofactor for arcsinh transformation\n",
    "    cofactor = 5\n",
    "    \n",
    "    # split the fcs files into training, validation and test set\n",
    "    group1 = np.where(sample_labels == 0)[0]\n",
    "    group2 = np.where(sample_labels == 1)[0]\n",
    "    l1, l2 = len(group1), len(group2)\n",
    "    ntrain_per_class = 7\n",
    "    ntest_group1 = l1 - ntrain_per_class\n",
    "    ntest_group2 = l2 - ntrain_per_class\n",
    "\n",
    "    # get the sample indices\n",
    "    train_idx1 = list(np.random.choice(group1, size=ntrain_per_class, replace=False))\n",
    "    test_idx1 = [i for i in group1 if i not in train_idx1]\n",
    "    train_idx2 = list(np.random.choice(group2, size=ntrain_per_class, replace=False))\n",
    "    test_idx2 = [i for i in group2 if i not in train_idx2]\n",
    "    \n",
    "    print(test_idx1)\n",
    "    print(test_idx2)\n",
    "    print(train_idx1)\n",
    "    print(train_idx2)\n",
    "    \n",
    "    # load the training samples\n",
    "    group1_list, group2_list = [], []\n",
    "    for idx in train_idx1:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        group1_list.append(x)\n",
    "\n",
    "    for idx in train_idx2:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        group2_list.append(x)\n",
    "\n",
    "    # load the test samples\n",
    "    t_group1_list, t_group2_list = [], []\n",
    "    test_phenotypes = []\n",
    "    for idx in test_idx1:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        t_group1_list.append(x)\n",
    "        test_phenotypes.append(0)\n",
    "\n",
    "    for idx in test_idx2:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        t_group2_list.append(x)\n",
    "        test_phenotypes.append(1)\n",
    "\n",
    "    # finally prepare training data\n",
    "    cut = int(1 * len(group1_list))\n",
    "    train_samples = group1_list[:cut] + group2_list[:cut]\n",
    "    train_phenotypes = [0] * len(group1_list[:cut]) + [1] * len(group2_list[:cut])\n",
    "    valid_samples = group1_list[cut:] + group2_list[cut:]\n",
    "    valid_phenotypes = [0] * len(group1_list[cut:]) + [1] * len(group2_list[cut:])\n",
    "    test_samples = t_group1_list + t_group2_list\n",
    "\n",
    "    return train_samples,train_phenotypes,test_samples,test_phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate original data (not normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 8, 9]\n",
      "[1, 6]\n",
      "[12, 2, 7, 11, 15, 14, 13]\n",
      "[16, 0, 4, 10, 18, 19, 17]\n",
      "scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "Predictions based on multi-cell inputs containing 5652 cells.\n",
      "not none\n",
      "(6, 5652, 37)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train_samples, train_phenotypes, test_samples, test_phenotypes = train_test_split()\n",
    "scaler = generate_data(train_samples, train_phenotypes, 'original/', valid_samples=test_samples, valid_phenotypes=test_phenotypes, ncell=200, nsubset=1000, verbose=0)\n",
    "\n",
    "#generate also the test set on full ncell per sample:\n",
    "def generate_for_pheno_prediction(new_samples,phenotypes,scaler):\n",
    "        ncell_per_sample = np.min([x.shape[0] for x in new_samples])\n",
    "        print(f\"Predictions based on multi-cell inputs containing {ncell_per_sample} cells.\")\n",
    "        nmark = len(new_samples[0][1])\n",
    "        # z-transform the new samples if we did that for the training samples\n",
    "        if scaler is not None:\n",
    "            new_samples = [scaler.transform(x) for x in new_samples]\n",
    "        new_samples = [shuffle(x)[:ncell_per_sample].reshape(1, ncell_per_sample, nmark)\n",
    "                           for x in new_samples]\n",
    "        data_test = np.vstack(new_samples)\n",
    "        mkdir_p('original/X_test_all/')\n",
    "        for i in range(len(data_test)):\n",
    "            np.savetxt('original/' + 'X_test_all/' + str(i) +'.txt', (transpose(data_test[i])))\n",
    "        np.savetxt('original/' + 'y_test_all.txt', phenotypes)\n",
    "        return data_test\n",
    "\n",
    "data_test=generate_for_pheno_prediction(test_samples,test_phenotypes,scaler)\n",
    "print(shape(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "train_samples, train_phenotypes, test_samples, test_phenotypes = train_test_split()\n",
    "generate_normalized_data(train_samples, train_phenotypes, 'normalized/', valid_samples=test_samples, valid_phenotypes=test_phenotypes, ncell=200, nsubset=1000)\n",
    "\n",
    "#generate also the test set on full ncell per sample:\n",
    "def generate_for_pheno_prediction(new_samples,phenotypes):\n",
    "        ncell_per_sample = np.min([x.shape[0] for x in new_samples])\n",
    "        print(f\"Predictions based on multi-cell inputs containing {ncell_per_sample} cells.\")\n",
    "        nmark = len(new_samples[0][1])\n",
    "\n",
    "        new_samples = [shuffle(x)[:ncell_per_sample].reshape(1, ncell_per_sample, nmark)\n",
    "                           for x in new_samples]\n",
    "        data_test = np.vstack(new_samples)\n",
    "        mkdir_p('normalized/X_test_all/')\n",
    "        for i in range(len(data_test)):\n",
    "            np.savetxt('normalized/' + 'X_test_all/' + str(i) +'.txt', normalize(transpose(data_test[i])))\n",
    "        np.savetxt('normalized/' + 'y_test_all.txt', phenotypes)\n",
    "        return data_test\n",
    "\n",
    "data_test=generate_for_pheno_prediction(test_samples,test_phenotypes)\n",
    "print(shape(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate  data split between 5 parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set indices:\n",
      "[3, 7, 11, 14]\n",
      "[6, 19]\n",
      "Global train set indices:\n",
      "[13, 15, 8, 9, 12, 5, 2]\n",
      "[17, 10, 0, 18, 4, 16, 1]\n",
      "Global train splitted among hosts - indices:\n",
      "[[2], [5], [12], [8, 9], [13, 15]]\n",
      "[[17, 10], [0, 18], [4], [16], [1]]\n",
      "\n",
      "Host no. 0 :\n",
      "host_idx_1: [2] - host_idx_2: [17, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1]\n",
      "scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 1 :\n",
      "host_idx_1: [5] - host_idx_2: [0, 18]\n",
      "[0, 1, 1]\n",
      "scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 2 :\n",
      "host_idx_1: [12] - host_idx_2: [4]\n",
      "[0, 1]\n",
      "scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 3 :\n",
      "host_idx_1: [8, 9] - host_idx_2: [16]\n",
      "[0, 0, 1]\n",
      "scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 4 :\n",
      "host_idx_1: [13, 15] - host_idx_2: [1]\n",
      "[0, 0, 1]\n",
      "scale\n",
      "Generating multi-cell inputs...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Here we randomly split the samples in training/test sets.\n",
    "nhosts=5\n",
    "cofactor = 5\n",
    "\n",
    "# first seperate the test set\n",
    "group1 = np.where(sample_labels == 0)[0]\n",
    "group2 = np.where(sample_labels == 1)[0]\n",
    "l1, l2 = len(group1), len(group2)\n",
    "ntrain_per_class = 7\n",
    "ntest_group1 = l1 - ntrain_per_class\n",
    "ntest_group2 = l2 - ntrain_per_class\n",
    "\n",
    "# get the sample indices\n",
    "train_idx1 = list(np.random.choice(group1, size=ntrain_per_class, replace=False))\n",
    "test_idx1 = [i for i in group1 if i not in train_idx1]\n",
    "train_idx2 = list(np.random.choice(group2, size=ntrain_per_class, replace=False))\n",
    "test_idx2 = [i for i in group2 if i not in train_idx2]\n",
    "\n",
    "print(\"Test set indices:\")\n",
    "print(test_idx1)\n",
    "print(test_idx2)\n",
    "print(\"Global train set indices:\")\n",
    "print(train_idx1)\n",
    "print(train_idx2)\n",
    "\n",
    "#distribute train indices balanced among n hosts:\n",
    "split_idx_1 = []\n",
    "split_idx_2 = []\n",
    "group1_list = np.flip(np.array_split(numpy.array(train_idx1), nhosts))\n",
    "group2_list = numpy.array_split(numpy.array(train_idx2), nhosts)\n",
    "\n",
    "for i in range(nhosts):\n",
    "    split_idx_1.append(group1_list[i].tolist())\n",
    "    split_idx_2.append(group2_list[i].tolist())\n",
    "\n",
    "print(\"Global train splitted among hosts - indices:\")\n",
    "print(split_idx_1)\n",
    "print(split_idx_2)\n",
    "\n",
    "\n",
    "for i in range(nhosts):\n",
    "    print(\"\\nHost no.\", i, \":\")\n",
    "    folder_path = 'split/host' + str(i) + '/'\n",
    "    host_idx_1 = split_idx_1[i]\n",
    "    host_idx_2 = split_idx_2[i]\n",
    "    print(\"host_idx_1:\", host_idx_1, \"- host_idx_2:\", host_idx_2)\n",
    "     # load the training samples\n",
    "    host_group1_list, host_group2_list = [], []\n",
    "    train_samples,train_phenotypes = [],[]\n",
    "    for idx in host_idx_1:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        host_group1_list.append(x)\n",
    "\n",
    "    for idx in host_idx_2:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        host_group2_list.append(x)\n",
    "    # finally prepare training and vallidation data\n",
    "    cut = int(1 * len(group1_list))\n",
    "    train_samples = host_group1_list[:cut] + host_group2_list[:cut]\n",
    "    train_phenotypes = [0] * len(host_group1_list[:cut]) + [1] * len(host_group2_list[:cut])\n",
    "    print(train_phenotypes)\n",
    "    generate_data(train_samples, train_phenotypes, folder_path, ncell=200, nsubset=200, verbose=0,generate_valid_set=False)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
