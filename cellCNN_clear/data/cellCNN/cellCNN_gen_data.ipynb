{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellCNN data generation\n",
    "\n",
    "source: https://github.com/eiriniar/CellCnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use CellCnn to analyze a mass cytometry dataset acquired to characterize human natural killer (NK) cell diversity and associate NK cell subsets with genetic and environmental factors, namely prior Cytomegalovirus (CMV) infection [1]. This dataset comprises mass cytometry measurements of 36 markers, including 28 NK cell receptors, for PBMC samples of 20 donors with varying serology for CMV. \n",
    "\n",
    "We will train CellCnn to identify CMV seropositivity-associated cell populations within the **manually gated NK cell compartment**. To run this example, please download the [NK cell dataset](http://www.imsb.ethz.ch/research/claassen/Software/cellcnn.html) and place the decompressed folder in the cellCnn/examples directory.\n",
    "\n",
    "[1] Horowitz, A. et al. Genetic and environmental determinants of human NK cell diversity revealed by mass cytometry. Sci. Transl. Med. 5 (2013).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install FlowIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os, sys, errno, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cellCNN_utils  \n",
    "from cellCNN_utils import loadFCS, ftrans, mkdir_p, get_items, generate_data, generate_normalized_data\n",
    "\n",
    "%pylab inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gated_NK\n"
     ]
    }
   ],
   "source": [
    "# define input and output directories\n",
    "WDIR = ''\n",
    "FCS_DATA_PATH = os.path.join(WDIR, 'gated_NK')\n",
    "\n",
    "# define output directory\n",
    "OUTDIR = os.path.join(WDIR, 'output_NK')\n",
    "mkdir_p(OUTDIR)\n",
    "print(FCS_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'Cell_length', 'CD3', 'Dead', '(La139)Dd', 'CD27', 'CD19', 'CD4', 'CD8', 'CD57', '2DL1-S1', 'TRAIL', '2DL2-L3-S2', 'CD16', 'CD10', '3DL1-S1', 'CD117', '2DS4', 'ILT2-CD85j', 'NKp46', 'NKG2D', 'NKG2C', '2B4', 'CD33', 'CD11b', 'NKp30', 'CD122', '3DL1', 'NKp44', 'CD127', '2DL1', 'CD94', 'CD34', 'CCR7', '2DL3', 'NKG2A', 'HLA-DR', '2DL4', 'CD56', '2DL5', 'CD25', 'DNA1', 'DNA2']\n",
      "(43,)\n"
     ]
    }
   ],
   "source": [
    "# look at the measured markers\n",
    "data_fcs = loadFCS(glob.glob(FCS_DATA_PATH + '/*.fcs')[0], transform=None, auto_comp=False)\n",
    "print(data_fcs.channels)\n",
    "print(shape(data_fcs.channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the relevant markers for further analysis\n",
    "markers = ['CD3', 'CD27', 'CD19', 'CD4', 'CD8', 'CD57', '2DL1-S1', 'TRAIL', '2DL2-L3-S2',\n",
    "           'CD16', 'CD10', '3DL1-S1', 'CD117', '2DS4', 'ILT2-CD85j', 'NKp46', 'NKG2D',\n",
    "           'NKG2C', '2B4', 'CD33', 'CD11b', 'NKp30', 'CD122', '3DL1', 'NKp44', 'CD127', '2DL1',\n",
    "           'CD94', 'CD34', 'CCR7', '2DL3', 'NKG2A', 'HLA-DR', '2DL4', 'CD56', '2DL5', 'CD25']\n",
    "marker_idx = [data_fcs.channels.index(label) for label in markers]\n",
    "nmark = len(markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a_001_NK.fcs' 'a_002_NK.fcs' 'a_003_NK.fcs' 'a_004_NK.fcs'\n",
      " 'a_005_NK.fcs' 'a_006_NK.fcs' 'a_007_NK.fcs' 'a_009_NK.fcs'\n",
      " 'a_010_NK.fcs' 'a_011_NK.fcs' 'a_012_NK.fcs' 'a_1a_NK.fcs' 'a_2a_NK.fcs'\n",
      " 'a_2b_NK.fcs' 'a_3a_NK.fcs' 'a_3b_NK.fcs' 'a_4a_NK.fcs' 'a_4b_NK.fcs'\n",
      " 'a_5a_NK.fcs' 'a_5b_NK.fcs']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# load the sample names and corresponding labels (0: CMV-, 1: CMV+), here from a CSV file\n",
    "# prior CMV infection status is obtained from the original study (Horowitz et al. 2013)\n",
    "csv_file = 'NK_fcs_samples_with_labels.csv'\n",
    "fcs_info = np.array(pd.read_csv(csv_file, sep=','))\n",
    "sample_ids = fcs_info[:, 0]\n",
    "sample_labels = fcs_info[:, 1].astype(int)\n",
    "print(sample_ids)\n",
    "print(len(sample_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we randomly split the samples in training/validation/test sets.\n",
    "\n",
    "def train_valid_split(train_idx1=[], train_idx2=[], valid=True):\n",
    "    # set random seed for reproducible results\n",
    "    np.random.seed(12345)\n",
    "\n",
    "    # cofactor for arcsinh transformation\n",
    "    cofactor = 5\n",
    "\n",
    "    # split the fcs files into training, validation and test set\n",
    "    group1 = np.where(sample_labels == 0)[0]\n",
    "    group2 = np.where(sample_labels == 1)[0]\n",
    "    l1, l2 = len(group1), len(group2)\n",
    "\n",
    "    # get the sample indices\n",
    "    if len(train_idx1) == 0:\n",
    "        train_idx1 = list(range(len(group1)))\n",
    "    if len(train_idx2) == 0:\n",
    "        train_idx2 = list(range(len(group2)))\n",
    "\n",
    "    # load the training samples\n",
    "    group1_list, group2_list = [], []\n",
    "    for idx in train_idx1:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        group1_list.append(x)\n",
    "\n",
    "    for idx in train_idx2:\n",
    "        fname = os.path.join(FCS_DATA_PATH, sample_ids[idx])\n",
    "        x_full = np.asarray(loadFCS(fname, transform=None, auto_comp=False))\n",
    "        x = ftrans(x_full[:,marker_idx], cofactor)\n",
    "        group2_list.append(x)\n",
    "\n",
    "    # finally prepare training and validation data\n",
    "    if valid:\n",
    "        cut = int(.2 * len(group1_list))\n",
    "    else:\n",
    "        cut = 0\n",
    "    train_samples = group1_list[cut:] + group2_list[cut:]\n",
    "    train_phenotypes = [0] * len(group1_list[cut:]) + [1] * len(group2_list[cut:])\n",
    "    valid_samples = group1_list[:cut] + group2_list[:cut]\n",
    "    valid_phenotypes = [0] * len(group1_list[:cut]) + [1] * len(group2_list[:cut])\n",
    "\n",
    "    return train_samples, train_phenotypes, valid_samples, valid_phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate original data (not normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating multi-cell inputs...\n",
      "2000 ; 37 ; 200\n",
      "2000\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train_samples, train_phenotypes, valid_samples, valid_phenotypes = train_valid_split()\n",
    "generate_data(train_samples, train_phenotypes, 'original/', valid_samples=valid_samples, valid_phenotypes=valid_phenotypes, ncell=200, nsubset=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating multi-cell inputs...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train_samples, train_phenotypes, valid_samples, valid_phenotypes = train_valid_split()\n",
    "generate_normalized_data(train_samples, train_phenotypes, 'normalized/', valid_samples=valid_samples, valid_phenotypes=valid_phenotypes, ncell=200, nsubset=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate normalized data split between 5 parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 9\n",
      "[[14, 15], [12, 13], [9, 11], [7, 8], [2, 3, 5]]\n",
      "[[0, 1], [4, 6], [10, 16], [17, 18], [19]]\n"
     ]
    }
   ],
   "source": [
    "nhosts = 5\n",
    "\n",
    "group1 = np.where(sample_labels == 0)[0]\n",
    "group2 = np.where(sample_labels == 1)[0]\n",
    "print(len(group1), len(group2))\n",
    "\n",
    "split_idx_1 = []\n",
    "split_idx_2 = []\n",
    "group1_list = np.flip(np.array_split(numpy.array(group1), nhosts))\n",
    "group2_list = numpy.array_split(numpy.array(group2), nhosts)\n",
    "\n",
    "for i in range(nhosts):\n",
    "    split_idx_1.append(group1_list[i].tolist())\n",
    "    split_idx_2.append(group2_list[i].tolist())\n",
    "\n",
    "print(split_idx_1)\n",
    "print(split_idx_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Host no. 0 :\n",
      "host_idx_1: [14, 15] - host_idx_2: [0, 1]\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 1 :\n",
      "host_idx_1: [12, 13] - host_idx_2: [4, 6]\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 2 :\n",
      "host_idx_1: [9, 11] - host_idx_2: [10, 16]\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 3 :\n",
      "host_idx_1: [7, 8] - host_idx_2: [17, 18]\n",
      "Generating multi-cell inputs...\n",
      "Done.\n",
      "\n",
      "Host no. 4 :\n",
      "host_idx_1: [2, 3, 5] - host_idx_2: [19]\n",
      "Generating multi-cell inputs...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for i in range(nhosts):\n",
    "    print(\"\\nHost no.\", i, \":\")\n",
    "    folder_path = 'split-normalized/host' + str(i) + '/'\n",
    "    host_idx_1 = split_idx_1[i]\n",
    "    host_idx_2 = split_idx_2[i]\n",
    "    print(\"host_idx_1:\", host_idx_1, \"- host_idx_2:\", host_idx_2)\n",
    "    train_samples, train_phenotypes, _, _ = train_valid_split(host_idx_1, host_idx_2)\n",
    "    generate_normalized_data(train_samples, train_phenotypes, folder_path, ncell=200, nsubset=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
